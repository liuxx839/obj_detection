import streamlit as st
from ultralytics import YOLO
import cv2
import numpy as np
from PIL import Image
import io
import os
import pickle
import uuid
import time
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import torch
from torchvision import models, transforms
from groq import Groq
from openai import OpenAI
import base64
from io import BytesIO
from mtcnn import MTCNN # Use MTCNN for detection
from deepface import DeepFace # Use DeepFace for face analysis and representation
# Removed: import face_recognition
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from imutils import build_montages
import shutil
from datetime import datetime
import hashlib
import random
import tempfile


# è®¾ç½®é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="ç‰©ä½“æ£€æµ‹ä¸ç›¸ä¼¼æœç´¢å·¥å…·", layout="wide")

# åˆå§‹åŒ–session_stateä»¥è·Ÿè¸ªå›¾ç‰‡å¤„ç†çŠ¶æ€
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'image' not in st.session_state:
    st.session_state.image = None
if 'last_upload_id' not in st.session_state:
    st.session_state.last_upload_id = None
if 'detection_results' not in st.session_state:
    st.session_state.detection_results = None
if 'processed_img' not in st.session_state:
    st.session_state.processed_img = None
if 'detected_faces' not in st.session_state:
    st.session_state.detected_faces = []
if 'similar_images' not in st.session_state:
    st.session_state.similar_images = []
if 'similar_faces_results' not in st.session_state:
    st.session_state.similar_faces_results = []
if 'advanced_face_search_done' not in st.session_state:
    st.session_state.advanced_face_search_done = False
if 'face_best_matches' not in st.session_state:
    st.session_state.face_best_matches = []
if 'current_tolerance' not in st.session_state:
    # Default tolerance for DeepFace (cosine distance). Lower is stricter.
    st.session_state.current_tolerance = 0.4 # Adjusted default for cosine distance
if 'similar_texts' not in st.session_state:
    st.session_state.similar_texts = []
if 'batch_mode' not in st.session_state:
    st.session_state.batch_mode = False
if 'batch_images' not in st.session_state:
    st.session_state.batch_images = []  # å­˜å‚¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡
if 'batch_faces' not in st.session_state:
    st.session_state.batch_faces = []  # å­˜å‚¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡ä¸­æ£€æµ‹åˆ°çš„äººè„¸
if 'batch_processed_images' not in st.session_state:
    st.session_state.batch_processed_images = []  # å­˜å‚¨æ‰¹é‡å¤„ç†åçš„å›¾ç‰‡
if 'batch_image_vectors' not in st.session_state:
    st.session_state.batch_image_vectors = {}  # å­˜å‚¨æ‰¹é‡å›¾ç‰‡çš„ç‰¹å¾å‘é‡
if 'batch_face_vectors' not in st.session_state:
    st.session_state.batch_face_vectors = {}  # å­˜å‚¨æ‰¹é‡äººè„¸çš„ç‰¹å¾å‘é‡
if 'batch_similarity_results' not in st.session_state:
    st.session_state.batch_similarity_results = []  # å­˜å‚¨æ‰¹é‡å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦ç»“æœ
if 'batch_face_similarity_results' not in st.session_state:
    st.session_state.batch_face_similarity_results = []  # å­˜å‚¨æ‰¹é‡äººè„¸é—´çš„ç›¸ä¼¼åº¦ç»“æœ

# æ ‡é¢˜
st.title("ğŸ“· é«˜ç²¾åº¦ç‰©ä½“æ£€æµ‹ä¸ç›¸ä¼¼æœç´¢å·¥å…·")
st.write("ä¸Šä¼ å›¾ç‰‡ï¼Œæ£€æµ‹äººã€é…’æ¯ç­‰ç‰©ä½“ï¼Œå¹¶åœ¨å†å²å›¾ç‰‡ä¸­å¯»æ‰¾ç›¸ä¼¼å›¾ç‰‡å’Œäººè„¸")

# åˆ›å»ºå­˜å‚¨ç›®å½•
def create_directories():
    dirs = ["data", "data/images", "data/faces", "data/vectors"]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)
    return Path("data")

data_dir = create_directories()

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")

    # æ¨¡å‹é€‰æ‹©
    model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["YOLOv11x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)",
         "YOLOv11l (é«˜ç²¾åº¦)",
         "YOLOv11m (ä¸­ç­‰ç²¾åº¦)",
         "YOLOv11s (è¾ƒå¿«)",
         "YOLOv11n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)",
         "YOLOv8x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)",
         "YOLOv8l (é«˜ç²¾åº¦)",
         "YOLOv8m (ä¸­ç­‰ç²¾åº¦)",
         "YOLOv8s (è¾ƒå¿«)",
         "YOLOv8n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)"],
        index=4  # é»˜è®¤é€‰æ‹©YOLOv11n (changed from YOLOv11x for speed)
    )

    # æ¨¡å‹æ˜ å°„
    model_mapping = {
        "YOLOv11x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)": "yolo11x.pt",
        "YOLOv11l (é«˜ç²¾åº¦)": "yolo11l.pt",
        "YOLOv11m (ä¸­ç­‰ç²¾åº¦)": "yolo11m.pt",
        "YOLOv11s (è¾ƒå¿«)": "yolo11s.pt",
        "YOLOv11n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)": "yolo11n.pt",
        "YOLOv8x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)": "yolov8x.pt",
        "YOLOv8l (é«˜ç²¾åº¦)": "yolov8l.pt",
        "YOLOv8m (ä¸­ç­‰ç²¾åº¦)": "yolov8m.pt",
        "YOLOv8s (è¾ƒå¿«)": "yolov8s.pt",
        "YOLOv8n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)": "yolov8n.pt"
    }

    confidence = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.5, 0.1)

    # é€‰æ‹©è¦æ£€æµ‹çš„ç±»åˆ«
    st.subheader("é€‰æ‹©è¦æ£€æµ‹çš„ç±»åˆ«")
    detect_person = st.checkbox("äºº", value=True)
    detect_cup = st.checkbox("æ¯å­/é…’æ¯", value=True)
    detect_bottle = st.checkbox("ç“¶å­", value=True)
    detect_all = st.checkbox("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“", value=True)

    # äººè„¸æ£€æµ‹é€‰é¡¹
    st.subheader("äººè„¸æ£€æµ‹")
    detect_faces = st.checkbox("å¯ç”¨äººè„¸æ£€æµ‹", value=True)
    face_confidence = st.slider("äººè„¸æ£€æµ‹ç½®ä¿¡åº¦", 0.1, 1.0, 0.8, 0.1) # MTCNN confidence

    # ç›¸ä¼¼æœç´¢é€‰é¡¹
    st.subheader("ç›¸ä¼¼æœç´¢")
    enable_similarity_search = st.checkbox("å¯ç”¨ç›¸ä¼¼å›¾ç‰‡æœç´¢", value=True)
    enable_face_similarity = st.checkbox("å¯ç”¨äººè„¸ç›¸ä¼¼æœç´¢", value=True)
    similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.5, 0.05) # Cosine Similarity threshold
    top_k = st.slider("è¿”å›ç›¸ä¼¼ç»“æœæ•°é‡", 1, 10, 3)

    # è·å–é¢œè‰²è®¾ç½®
    st.subheader("æ ‡è®°é¢œè‰²")
    box_color = st.color_picker("è¾¹æ¡†é¢œè‰²", "#FF0000")
    face_box_color = st.color_picker("äººè„¸è¾¹æ¡†é¢œè‰²", "#00FF00")

    # æ·»åŠ åŠ å¼ºäººè„¸æ£€ç´¢é€‰é¡¹
    st.subheader("åŠ å¼ºäººè„¸æ£€ç´¢")
    enable_advanced_face_search = st.checkbox("å¯ç”¨åŠ å¼ºäººè„¸æ£€ç´¢", value=True)
    if enable_advanced_face_search:
        # Adjusted tolerance for DeepFace/Cosine Distance
        face_cluster_tolerance = st.slider("äººè„¸åŒ¹é…ä¸¥æ ¼åº¦ (è·ç¦»)", 0.1, 1.0, 0.4, 0.05, help="æ•°å€¼è¶Šä½åŒ¹é…è¶Šä¸¥æ ¼ (ä½¿ç”¨Cosineè·ç¦»)")
        min_cluster_size = st.slider("æœ€å°èšç±»æ•°é‡", 1, 10, 3, 1, help="å½¢æˆèšç±»æ‰€éœ€çš„æœ€å°å›¾ç‰‡æ•°é‡")

    # åœ¨ä¾§è¾¹æ æ·»åŠ æ‰¹é‡æ¨¡å¼é€‰é¡¹
    st.subheader("æ‰¹é‡å¤„ç†")
    batch_mode = st.checkbox("å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼", value=False)
    if batch_mode:
        st.session_state.batch_mode = True
        st.info("æ‰¹é‡æ¨¡å¼å·²å¯ç”¨ï¼Œæ‚¨å¯ä»¥ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œå¤„ç†")
        if st.button("æ¸…é™¤æ‰¹é‡å¤„ç†æ•°æ®"):
            st.session_state.batch_images = []
            st.session_state.batch_faces = []
            st.session_state.batch_processed_images = []
            st.session_state.batch_image_vectors = {}
            st.session_state.batch_face_vectors = {}
            st.session_state.batch_similarity_results = []
            st.session_state.batch_face_similarity_results = []
            st.success("æ‰¹é‡å¤„ç†æ•°æ®å·²æ¸…é™¤")
    else:
        st.session_state.batch_mode = False

# åŠ è½½YOLOæ¨¡å‹
@st.cache_resource
def load_model(model_name):
    # Check if model file exists, download if not (YOLO handles this internally)
    try:
        model = YOLO(model_name)
        # Perform a dummy prediction to ensure model is loaded/downloaded
        # model.predict(np.zeros((640, 640, 3)), verbose=False)
        return model
    except Exception as e:
        st.error(f"åŠ è½½YOLOæ¨¡å‹ {model_name} å¤±è´¥: {e}")
        st.stop()


# åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹ (MTCNN)
@st.cache_resource
def load_face_detector():
    try:
        detector = MTCNN()
        # Optional: Perform a dummy detection to ensure model is loaded
        # detector.detect_faces(np.zeros((100, 100, 3)))
        return detector
    except Exception as e:
        st.error(f"åŠ è½½MTCNNäººè„¸æ£€æµ‹æ¨¡å‹å¤±è´¥: {e}")
        st.stop()

# åŠ è½½ç‰¹å¾æå–æ¨¡å‹ (ResNet50 for general images, DeepFace for faces)
@st.cache_resource
def load_feature_extractor():
    # General image feature extractor (ResNet50)
    model = models.resnet50(weights='DEFAULT')
    model = torch.nn.Sequential(*(list(model.children())[:-1]))
    model.eval()
    # Face feature extractor is handled by DeepFace directly
    return model

# å›¾åƒé¢„å¤„ç†è½¬æ¢ (For ResNet50)
@st.cache_resource
def get_transform():
    return transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

# æå–å›¾åƒç‰¹å¾ (Using ResNet50)
def extract_image_features(image, model, transform):
    try:
        img_t = transform(image.convert("RGB")).unsqueeze(0) # Ensure RGB
        with torch.no_grad():
            features = model(img_t)
        return features.squeeze().cpu().numpy()
    except Exception as e:
        st.warning(f"æå–å›¾åƒç‰¹å¾å¤±è´¥: {e}")
        return None

# æå–äººè„¸ç‰¹å¾ (Using DeepFace)
# Input can be PIL Image or numpy array
def extract_face_features(face_image_or_array, model_name="ArcFace"):
    try:
        # DeepFace.represent returns a list of dictionaries
        # [{'embedding': [..], 'facial_area': {'x':.., 'y':.., 'w':.., 'h':..}, 'face_confidence': ..}]
        result = DeepFace.represent(
            img_path=face_image_or_array,
            model_name=model_name,
            enforce_detection=False # Assume input is already a face crop
        )
        if result and isinstance(result, list) and 'embedding' in result[0]:
            return np.array(result[0]['embedding'])
        else:
            st.warning("DeepFaceæœªèƒ½æå–äººè„¸ç‰¹å¾")
            return None
    except ValueError as ve: # Handle case where DeepFace might not find a face even if cropped
         st.warning(f"DeepFaceæ— æ³•å¤„ç†æ­¤äººè„¸å›¾åƒ: {ve}")
         return None
    except Exception as e:
        st.warning(f"æå–äººè„¸ç‰¹å¾å¤±è´¥ (DeepFace): {e}")
        return None


# åˆå§‹åŒ–å‘é‡åº“
def initialize_vector_db():
    vector_file = data_dir / "vectors" / "image_vectors.pkl"
    face_vector_file = data_dir / "vectors" / "face_vectors.pkl"

    # å›¾åƒå‘é‡æ•°æ®åº“
    image_db = {}
    if os.path.exists(vector_file):
        try:
            with open(vector_file, 'rb') as f:
                image_db = pickle.load(f)
            if not isinstance(image_db, dict): # Basic validation
                st.warning("å›¾åƒå‘é‡æ•°æ®åº“æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
                image_db = {}
        except (pickle.UnpicklingError, EOFError, TypeError) as e:
             st.warning(f"è¯»å–å›¾åƒå‘é‡æ•°æ®åº“å¤±è´¥ ({e})ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
             image_db = {}


    # äººè„¸å‘é‡æ•°æ®åº“
    face_db = {}
    if os.path.exists(face_vector_file):
        try:
            with open(face_vector_file, 'rb') as f:
                face_db = pickle.load(f)
            if not isinstance(face_db, dict): # Basic validation
                st.warning("äººè„¸å‘é‡æ•°æ®åº“æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
                face_db = {}
        except (pickle.UnpicklingError, EOFError, TypeError) as e:
            st.warning(f"è¯»å–äººè„¸å‘é‡æ•°æ®åº“å¤±è´¥ ({e})ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
            face_db = {}

    return image_db, face_db

# ä¿å­˜å‘é‡æ•°æ®åº“
def save_vector_db(image_db, face_db):
    vector_file = data_dir / "vectors" / "image_vectors.pkl"
    face_vector_file = data_dir / "vectors" / "face_vectors.pkl"

    try:
        with open(vector_file, 'wb') as f:
            pickle.dump(image_db, f)
    except Exception as e:
        st.error(f"ä¿å­˜å›¾åƒå‘é‡æ•°æ®åº“å¤±è´¥: {e}")

    try:
        with open(face_vector_file, 'wb') as f:
            pickle.dump(face_db, f)
    except Exception as e:
        st.error(f"ä¿å­˜äººè„¸å‘é‡æ•°æ®åº“å¤±è´¥: {e}")


# æ›´æ–°å‘é‡æ•°æ®åº“
def update_vector_db(image, faces, feature_extractor, transform):
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    image_db, face_db = initialize_vector_db()

    # ç”Ÿæˆå”¯ä¸€ID
    image_id = str(uuid.uuid4())
    timestamp = int(time.time())

    # ä¿å­˜å›¾åƒ
    image_filename = f"{image_id}.jpg"
    image_path = data_dir / "images" / image_filename
    try:
        img_to_save = image.convert('RGB') # Ensure RGB before saving
        img_to_save.save(image_path, format="JPEG", quality=85) # Specify format and quality
    except Exception as e:
        st.error(f"ä¿å­˜åŸå§‹å›¾ç‰‡å¤±è´¥: {e}")
        return None, []

    # æå–å›¾åƒç‰¹å¾ (ResNet50)
    image_features = extract_image_features(image, feature_extractor, transform)
    if image_features is None:
        st.warning(f"æœªèƒ½ä¸ºå›¾ç‰‡ {image_id} æå–ç‰¹å¾ï¼Œè·³è¿‡å›¾åƒç›¸ä¼¼æ€§ã€‚")
    else:
        # æ›´æ–°å›¾åƒå‘é‡æ•°æ®åº“
        image_db[image_id] = {
            'vector': image_features,
            'path': str(image_path),
            'timestamp': timestamp
        }

    # å¤„ç†äººè„¸ (Using DeepFace for features)
    face_ids = []
    for i, face_array in enumerate(faces): # faces are numpy arrays from MTCNN/detect_face
        face_id = f"{image_id}_face_{i}"
        face_filename = f"{face_id}.jpg"
        face_path = data_dir / "faces" / face_filename

        try:
            face_pil = Image.fromarray(face_array).convert('RGB') # Ensure RGB
            face_pil.save(face_path, format="JPEG", quality=85)
        except Exception as e:
            st.error(f"ä¿å­˜äººè„¸å›¾ç‰‡ {face_id} å¤±è´¥: {e}")
            continue # Skip this face

        # æå–äººè„¸ç‰¹å¾ (DeepFace)
        face_features = extract_face_features(face_array) # Pass numpy array

        if face_features is not None:
            # æ›´æ–°äººè„¸å‘é‡æ•°æ®åº“
            face_db[face_id] = {
                'vector': face_features,
                'path': str(face_path),
                'image_id': image_id,
                'timestamp': timestamp
            }
            face_ids.append(face_id)
        else:
            st.warning(f"æœªèƒ½ä¸ºäººè„¸ {face_id} æå–ç‰¹å¾ã€‚")

    # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
    save_vector_db(image_db, face_db)

    return image_id, face_ids


# æ‰§è¡Œç›¸ä¼¼æœç´¢ (Cosine Similarity)
def search_similar_images(query_vector, image_db, top_k=3, threshold=0.6):
    if not image_db or query_vector is None:
        return []

    results = []
    query_vector_norm = np.linalg.norm(query_vector)
    if query_vector_norm == 0: return [] # Avoid division by zero

    # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
    for image_id, data in image_db.items():
        db_vector = data.get('vector')
        if db_vector is None or not isinstance(db_vector, np.ndarray): continue

        db_vector_norm = np.linalg.norm(db_vector)
        if db_vector_norm == 0: continue # Avoid division by zero

        # Cosine Similarity
        similarity = np.dot(query_vector, db_vector) / (query_vector_norm * db_vector_norm)
        similarity = float(similarity) # Ensure float type

        if similarity >= threshold:
            results.append({
                'id': image_id,
                'path': data['path'],
                'similarity': similarity,
                'timestamp': data.get('timestamp', 0)
            })

    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    results.sort(key=lambda x: x['similarity'], reverse=True)

    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

# æ‰§è¡Œäººè„¸ç›¸ä¼¼æœç´¢ (Cosine Similarity using DeepFace embeddings)
def search_similar_faces(query_vectors, face_db, top_k=3, threshold=0.6):
    if not face_db or not query_vectors:
        return []

    all_results = []

    # å¯¹æ¯ä¸ªæŸ¥è¯¢äººè„¸è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
    for i, query_vector in enumerate(query_vectors):
        if query_vector is None: continue # Skip if query vector extraction failed

        results = []
        query_vector_norm = np.linalg.norm(query_vector)
        if query_vector_norm == 0: continue

        # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸å‘é‡çš„ç›¸ä¼¼åº¦
        for face_id, data in face_db.items():
            db_vector = data.get('vector')
            if db_vector is None or not isinstance(db_vector, np.ndarray): continue

            db_vector_norm = np.linalg.norm(db_vector)
            if db_vector_norm == 0: continue

            # Cosine Similarity
            similarity = np.dot(query_vector, db_vector) / (query_vector_norm * db_vector_norm)
            similarity = float(similarity)

            if similarity >= threshold:
                results.append({
                    'id': face_id,
                    'path': data['path'],
                    'image_id': data['image_id'],
                    'similarity': similarity,
                    'timestamp': data.get('timestamp', 0)
                })

        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)

        # ä¸ºå½“å‰æŸ¥è¯¢äººè„¸ä¿å­˜å‰Kä¸ªç»“æœ
        all_results.append({
            'query_face_index': i,
            'matches': results[:top_k]
        })

    return all_results

# å¤„ç†é¢„æµ‹ç»“æœå¹¶ç»˜åˆ¶è¾¹æ¡†
def process_prediction(image, results, selected_classes, conf_threshold):
    # è½¬æ¢ä¸ºOpenCVæ ¼å¼ç”¨äºç»˜å›¾
    img = np.array(image.convert("RGB")) # Ensure RGB
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)

    # COCOæ•°æ®é›†ç±»åˆ« (assuming results[0].names exists)
    class_names = results[0].names if hasattr(results[0], 'names') else {} # Handle if names not available

    # å¦‚æœé€‰æ‹©äº†"æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“"ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«
    if "æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“" in selected_classes:
        selected_class_ids = list(class_names.keys()) if class_names else []
    else:
        # åˆ›å»ºç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„ (COCO IDs)
        class_mapping = {
            "äºº": 0,  # person
            "æ¯å­/é…’æ¯": 41,  # cup (also covers wine glass often)
            "ç“¶å­": 39,  # bottle
        }
        # è·å–é€‰ä¸­ç±»åˆ«çš„ID
        selected_class_ids = [class_mapping[cls] for cls in selected_classes if cls in class_mapping]

    # è§£æåå…­è¿›åˆ¶é¢œè‰²ä¸ºBGR
    try:
        hex_color = box_color.lstrip('#')
        box_bgr = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))  # è½¬æ¢ä¸ºBGR
    except ValueError:
        box_bgr = (0, 0, 255) # Default to red if color code is invalid

    # å¦‚æœæœ‰æ£€æµ‹ç»“æœ
    if hasattr(results[0], 'boxes') and results[0].boxes is not None:
        boxes = results[0].boxes

        for box in boxes:
            # è·å–ç±»åˆ«IDå’Œç½®ä¿¡åº¦
            cls_id = int(box.cls.item())
            conf = box.conf.item()

            # å¦‚æœç±»åˆ«åœ¨é€‰ä¸­çš„ç±»åˆ«ä¸­ä¸”ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼
            if cls_id in selected_class_ids and conf >= conf_threshold:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)

                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(img, (x1, y1), (x2, y2), box_bgr, 2)

                # æ·»åŠ æ ‡ç­¾
                label = f"{class_names.get(cls_id, f'ID:{cls_id}')} {conf:.2f}" # Use get for safety
                text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                # Ensure label background doesn't go out of bounds
                label_bg_y1 = max(y1 - text_size[1] - 5, 0)
                label_bg_y2 = y1
                cv2.rectangle(img, (x1, label_bg_y1), (x1 + text_size[0], label_bg_y2), box_bgr, -1)
                cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

    # è½¬æ¢å›RGBç”¨äºStreamlitæ˜¾ç¤º
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img

# äººè„¸æ£€æµ‹å‡½æ•° (Using MTCNN)
def detect_face(image, face_detector, conf_threshold):
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶ç¡®ä¿RGBæ ¼å¼
    img_array = np.array(image.convert("RGB")) # Ensure RGB
    img_rgb = img_array.copy()

    # ä½¿ç”¨MTCNNæ£€æµ‹äººè„¸
    # MTCNN returns list of dicts: {'box': [x, y, width, height], 'confidence': score, 'keypoints': {...}}
    try:
        faces_detected = face_detector.detect_faces(img_rgb)
    except Exception as e:
        st.error(f"MTCNNäººè„¸æ£€æµ‹å¤±è´¥: {e}")
        faces_detected = []

    # ç­›é€‰ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„äººè„¸
    filtered_faces = [face for face in faces_detected if face['confidence'] >= conf_threshold]

    # è§£æäººè„¸è¾¹æ¡†é¢œè‰²
    try:
        hex_color = face_box_color.lstrip('#')
        face_bgr = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))  # è½¬æ¢ä¸ºBGR
    except ValueError:
        face_bgr = (0, 255, 0) # Default to green if color code is invalid

    # æå–çš„äººè„¸å›¾åƒåˆ—è¡¨ (as numpy arrays)
    face_image_arrays = []

    # åœ¨åŸå›¾ä¸Šæ ‡è®°äººè„¸
    img_cv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR) # Work with BGR for drawing
    for face in filtered_faces:
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x, y, w, h = face['box']
        # Ensure coordinates are valid
        x1, y1 = max(0, x), max(0, y)
        x2, y2 = min(img_cv.shape[1], x + w), min(img_cv.shape[0], y + h)

        # ç»˜åˆ¶äººè„¸è¾¹æ¡†
        cv2.rectangle(img_cv, (x1, y1), (x2, y2), face_bgr, 2)

        # æ·»åŠ æ ‡ç­¾å’Œç½®ä¿¡åº¦
        label = f"Face: {face['confidence']:.2f}"
        text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
        label_bg_y1 = max(y1 - text_size[1] - 5, 0)
        cv2.rectangle(img_cv, (x1, label_bg_y1), (x1 + text_size[0], y1), face_bgr, -1)
        cv2.putText(img_cv, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)

        # å¯é€‰ï¼šç»˜åˆ¶å…³é”®ç‚¹ (if needed)
        # keypoints = face['keypoints']
        # for point in keypoints.values():
        #     cv2.circle(img_cv, tuple(point), 2, face_bgr, 2)

        # æå–äººè„¸åŒºåŸŸ (numpy array)
        if y2 > y1 and x2 > x1: # Ensure valid crop dimensions
            face_crop = img_rgb[y1:y2, x1:x2]
            face_image_arrays.append(face_crop)

    # è½¬æ¢å›RGB for display
    result_img_rgb = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)

    return result_img_rgb, face_image_arrays # Return marked image and list of face arrays

# æ˜¾ç¤ºç›¸ä¼¼å›¾ç‰‡ç»“æœ
def display_similar_images(similar_images):
    if not similar_images:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
        return

    st.subheader(f"ç›¸ä¼¼å›¾ç‰‡æ£€ç´¢ç»“æœ (Top {len(similar_images)})")

    # æ˜¾ç¤ºç»“æœ
    cols = st.columns(min(len(similar_images), 3))
    for i, result in enumerate(similar_images):
        with cols[i % 3]:
            try:
                image_path = Path(result['path'])
                if image_path.exists():
                    img = Image.open(image_path)
                    st.image(img, caption=f"ç›¸ä¼¼åº¦: {result['similarity']:.2f}", use_container_width=True)

                    # æ˜¾ç¤ºæ—¶é—´æˆ³ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
                    if 'timestamp' in result:
                        time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(result['timestamp']))
                        st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")
                else:
                    st.warning(f"ç›¸ä¼¼å›¾ç‰‡æ–‡ä»¶ä¸¢å¤±: {image_path.name}")
            except Exception as e:
                st.error(f"æ˜¾ç¤ºç›¸ä¼¼å›¾ç‰‡æ—¶å‡ºé”™: {e}")

# æ˜¾ç¤ºç›¸ä¼¼äººè„¸ç»“æœ
def display_similar_faces(similar_faces_results, detected_faces):
    if not similar_faces_results:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼äººè„¸")
        return

    st.subheader("äººè„¸ç›¸ä¼¼åº¦æ£€ç´¢ç»“æœ")

    # ä¸ºæ¯ä¸ªæŸ¥è¯¢äººè„¸æ˜¾ç¤ºå…¶åŒ¹é…ç»“æœ
    for result in similar_faces_results:
        query_face_idx = result['query_face_index']
        matches = result['matches']

        if matches:
            st.write(f"##### æŸ¥è¯¢äººè„¸ #{query_face_idx + 1} çš„åŒ¹é…ç»“æœ:")

            # Check if query face index is valid
            if query_face_idx < len(detected_faces):
                query_face_array = detected_faces[query_face_idx]
                query_face_pil = Image.fromarray(query_face_array)

                # åˆ›å»ºåˆ—æ¥æ˜¾ç¤ºæŸ¥è¯¢äººè„¸å’ŒåŒ¹é…ç»“æœ
                cols = st.columns(1 + min(len(matches), 3))

                # æ˜¾ç¤ºæŸ¥è¯¢äººè„¸
                with cols[0]:
                    st.image(query_face_pil, caption="æŸ¥è¯¢äººè„¸", use_container_width=True)

                # æ˜¾ç¤ºåŒ¹é…ç»“æœ
                for i, match in enumerate(matches):
                    if i < len(cols) - 1:  # ç¡®ä¿ä¸è¶…å‡ºåˆ—æ•°
                        with cols[i + 1]:
                            try:
                                face_path = Path(match['path'])
                                if face_path.exists():
                                    match_face = Image.open(face_path)
                                    st.image(match_face, caption=f"ç›¸ä¼¼åº¦: {match['similarity']:.2f}", use_container_width=True)

                                    # æ˜¾ç¤ºæ—¶é—´æˆ³ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
                                    if 'timestamp' in match:
                                        time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(match['timestamp']))
                                        st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")
                                else:
                                     st.warning(f"ç›¸ä¼¼äººè„¸æ–‡ä»¶ä¸¢å¤±: {face_path.name}")
                            except Exception as e:
                                st.error(f"æ˜¾ç¤ºç›¸ä¼¼äººè„¸æ—¶å‡ºé”™: {e}")
            else:
                st.warning(f"æŸ¥è¯¢äººè„¸ç´¢å¼• #{query_face_idx + 1} æ— æ•ˆã€‚")
        # Optionally add a message if a query face had no matches above threshold
        # else:
        #     st.write(f"æŸ¥è¯¢äººè„¸ #{query_face_idx + 1}: æœªæ‰¾åˆ°ç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼çš„åŒ¹é…ç»“æœã€‚")


# ä¿®æ”¹å›¾ç‰‡è½¬base64å‡½æ•°
def image_to_base64(image):
    # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ¨¡å¼
    if image.mode in ('RGBA', 'P'):
        image = image.convert('RGB')

    buffered = BytesIO()
    image.save(buffered, format="JPEG") # Use JPEG for potentially smaller size
    return base64.b64encode(buffered.getvalue()).decode()

# ä¿®æ”¹analyze_with_groqå‡½æ•°
def analyze_with_groq(image, api_key):
    # Use Groq only if API key is provided
    if not api_key:
        return "Groq API Key æœªè®¾ç½®"

    try:
        client_vision = Groq(api_key=api_key) # Pass key directly

        # Ensure image is PIL and RGB
        if not isinstance(image, Image.Image):
            image = Image.fromarray(image) # Try converting if it's an array
        if image.mode in ('RGBA', 'P'):
            image = image.convert('RGB')

        base64_image = image_to_base64(image)

        response = client_vision.chat.completions.create(
            model="llama3-70b-8192", # Updated model
            messages=[
                {
                    "role": "system",
                    "content": '''è¯·æŒ‰ä»¥ä¸‹è§„åˆ™åˆ†æå›¾ç‰‡å†…å®¹ï¼š
                    é¦–å…ˆåˆ¤æ–­å›¾ç‰‡ä¸»è¦å†…å®¹æ˜¯å¦ä¸ºæ–‡å­—å†…å®¹ï¼ˆå³å›¾ç‰‡ä¸­è¶…è¿‡ 50% çš„åŒºåŸŸä¸ºå¯è¯†åˆ«çš„æ–‡å­—ï¼Œå¦‚æ–‡æ¡£ã€æµ·æŠ¥ã€æ ‡è¯­ç­‰ï¼‰ï¼š
                    è‹¥æ˜¯ï¼šç›´æ¥æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰å¯è¯†åˆ«æ–‡å­—ï¼ˆæ— éœ€é¢å¤–æè¿°ï¼‰ã€‚
                    è‹¥å¦ï¼šè¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼Œéœ€åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼ˆæ— ç›¸å…³ä¿¡æ¯åˆ™æ ‡æ³¨ "æœªæåŠ" æˆ– "æ— æ³•åˆ¤æ–­"ï¼‰ï¼š
                    # ä¸»ä½“å†…å®¹ï¼šæ¸…æ™°æè¿°ç”»é¢ä¸­çš„ç‰©ä½“ã€åœºæ™¯ã€åŠ¨ä½œç­‰ç»†èŠ‚ï¼ˆå¦‚ "å®¤å†…ä¼šè®®åœºæ™¯ï¼Œ3 äººå›´åè®¨è®ºï¼Œæ¡Œä¸Šæ‘†æ”¾ç¬”è®°æœ¬ç”µè„‘å’Œæ–‡ä»¶"ï¼‰ã€‚
                    # æ–‡å­—ä¿¡æ¯ï¼šå°½å¯èƒ½æå–å›¾ç‰‡é‡Œçš„æ–‡å­—ä¿¡æ¯
                    # äººç‰©ä¿¡æ¯ï¼ˆè‹¥åŒ…å«äººï¼‰ï¼š
                    ãƒ»äººå¤´æ•°ï¼šX äººï¼ˆç²¾ç¡®è®¡æ•°ï¼‰ã€‚
                    ãƒ»æ€§åˆ«æ¯”ä¾‹ï¼šç”·æ€§ X äººï¼Œå¥³æ€§ Y äººï¼Œæ€§åˆ«ä¸æ˜ Z äººï¼ˆæ— äººç‰©åˆ™å¡« 0ï¼‰ã€‚
                    ãƒ»ç§æ—æ¯”ä¾‹ï¼šé»‘äººï¼Œç™½äººï¼Œäºšæ´²äººç­‰
                    ãƒ»å¹´é¾„æ®µï¼šæŒ‰å„¿ç«¥ï¼ˆï¼œ12 å²ï¼‰ã€é’å¹´ï¼ˆ12-35 å²ï¼‰ã€ä¸­å¹´ï¼ˆ36-60 å²ï¼‰ã€è€å¹´ï¼ˆï¼60 å²ï¼‰åˆ†ç±»ï¼Œæ ‡æ³¨å„å¹´é¾„æ®µäººæ•°ï¼ˆå¦‚ "é’å¹´ 2 äººï¼Œä¸­å¹´ 1 äºº"ï¼‰ã€‚
                    # æ‰€åœ¨åœºæ™¯ï¼šå…·ä½“æè¿°ç¯å¢ƒï¼ˆå¦‚ "åŒ»é™¢å€™è¯ŠåŒº""æˆ·å¤–å…¬å›­""å®éªŒå®¤" ç­‰ï¼‰ï¼Œéœ€åŒ…å«å®¤å†… / å®¤å¤–ã€åœ°ç‚¹ç‰¹å¾ç­‰ç»†èŠ‚ã€‚
                    # Sanofi ç›¸å…³æ€§ï¼šæ˜ç¡®è¯´æ˜æ˜¯å¦å‡ºç° Sanofi æ ‡å¿—ï¼ˆå¦‚ LOGOã€å“ç‰Œåç§°ï¼‰ã€äº§å“ï¼ˆå¦‚è¯å“åŒ…è£…ã€å®£ä¼ ææ–™ï¼‰ã€æ–‡å­—æåŠï¼ˆå¦‚ "Sanofi""èµ›è¯ºè²" å­—æ ·ï¼‰æˆ–ç›¸å…³åœºæ™¯ï¼ˆå¦‚ Sanofi æ´»åŠ¨ã€åˆä½œé¡¹ç›®ç­‰ï¼‰ï¼Œè‹¥æ— åˆ™æ ‡æ³¨ "æ— ç›¸å…³å…ƒç´ "ã€‚
                    ä½¿ç”¨è¯´æ˜ï¼š
                    è‹¥å›¾ç‰‡ä¸ºçº¯æ–‡å­—ï¼ˆå¦‚åˆåŒã€è¯´æ˜ä¹¦ï¼‰ï¼Œè¾“å‡ºä»…åŒ…å«æå–çš„æ–‡å­—å†…å®¹ã€‚
                    è‹¥å›¾ç‰‡ä¸ºéæ–‡å­—å†…å®¹ï¼ˆå¦‚ç…§ç‰‡ã€æ’ç”»ï¼‰ï¼ŒæŒ‰ä¸Šè¿°æ ¼å¼åˆ†ç‚¹è¯¦ç»†æè¿°ï¼Œäººç‰©ä¿¡æ¯éœ€ä¸¥æ ¼æŒ‰è¦æ±‚åˆ†ç±»ç»Ÿè®¡ã€‚
                    ç¡®ä¿è¯­è¨€ç®€æ´å‡†ç¡®ï¼Œé¿å…ä¸»è§‚æ¨æ–­ï¼Œä»…åŸºäºå›¾ç‰‡å¯è§å†…å®¹è¾“å‡ºã€‚
                    æ–°å¢éªŒè¯é¡¹
                    é’ˆå¯¹ä»¥ä¸‹åœºæ™¯ï¼Œæ˜ç¡®æ ‡æ³¨å·¥å…·æ˜¯å¦æ­£ç¡®è¯†åˆ«ï¼ˆéœ€åŸºäºå›¾ç‰‡å¯è§å†…å®¹åˆ¤æ–­ï¼Œè€Œéä¸»è§‚æ¨æµ‹ï¼‰ï¼š
                    # å›¾ç‰‡é£æ ¼ï¼š æ˜¯å¦ä¸ºçœŸå®ç…§ç‰‡ï¼Œè¿˜æ˜¯æŸç§é£æ ¼å›¾åƒ, æ˜¯å¦ä¸ºè§†é¢‘æˆªå›¾æˆ–è€…ç¿»æ‹ï¼Œè¿˜æ˜¯è‡ªæ‹
                    # ç©ºåœºæ™¯è¯†åˆ«ï¼šå›¾ç‰‡æ˜¯å¦ä¸ºæ— äººã€æ— æ˜æ˜¾ç‰©ä½“çš„ç©ºåœºæ™¯ï¼ˆå¦‚ç©ºç™½å¢™é¢ã€çº¯è‰²èƒŒæ™¯ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ / å¦ / æ— æ³•åˆ¤æ–­ï¼ˆè‹¥æœ‰ç‰©ä½“æˆ–äººç‰©ï¼Œæ ‡æ³¨ "éç©ºåœºæ™¯"ï¼‰ã€‚
                    # ä¼šè®®æ— å…³åœºæ™¯è¯†åˆ«ï¼šåœºæ™¯æ˜¯å¦ä¸ä¼šè®®æ— ç›´æ¥å…³è”ï¼ˆå¦‚çº¯è‡ªç„¶é£æ™¯ã€å•ç‹¬é™ç‰©ã€å¨±ä¹åœºæ‰€ç­‰ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆä¼šè®®æ— å…³ï¼‰/ å¦ï¼ˆå¯èƒ½ä¸ä¼šè®®ç›¸å…³ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # äººæ•°æ•æ‰ï¼šå·¥å…·æ˜¯å¦å‡†ç¡®ç»Ÿè®¡å›¾ç‰‡ä¸­çš„äººå¤´æ•°ï¼Ÿï¼ˆè‹¥æœ‰äººç‰©ï¼Œéœ€ä¸å®é™…è®¡æ•°ä¸€è‡´ï¼‰
                    # å®é™…äººå¤´æ•°ï¼šX äººï¼›å·¥å…·è¯†åˆ«ç»“æœï¼šX äººï¼ˆä¸€è‡´ / ä¸ä¸€è‡´ï¼‰ã€‚
                    # ä¼šè®®è®¾å¤‡è¯†åˆ«ï¼šæ˜¯å¦æ­£ç¡®è¯†åˆ«ä¼šè®®å¿…è¦è®¾å¤‡ï¼ˆå¦‚æŠ•å½±ä»ªã€ç”µè„‘ã€éº¦å…‹é£ã€ç™½æ¿ã€ä¼šè®®æ¡Œç­‰ï¼‰ï¼Ÿ
                    # è¯†åˆ«è®¾å¤‡ï¼šè‹¥æœ‰ï¼Œåˆ—å‡ºå…·ä½“è®¾å¤‡åç§°ï¼ˆå¦‚ "æŠ•å½±ä»ªã€ç¬”è®°æœ¬ç”µè„‘"ï¼‰ï¼›è‹¥æ— ï¼Œæ ‡æ³¨ "æœªè¯†åˆ«åˆ°ä¼šè®®è®¾å¤‡"ã€‚
                    # ä¼šè®®ç…§ç‰‡ç¿»æ‹è¯†åˆ«ï¼šå›¾ç‰‡æ˜¯å¦ä¸ºä¼šè®®ç…§ç‰‡çš„ç¿»æ‹ï¼ˆå¦‚å¯¹å±å¹•ã€çº¸è´¨ç…§ç‰‡çš„äºŒæ¬¡æ‹æ‘„ï¼Œå¯èƒ½å­˜åœ¨åå…‰ã€å˜å½¢ç­‰ç‰¹å¾ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆç¿»æ‹ç…§ç‰‡ï¼‰/ å¦ï¼ˆéç¿»æ‹ç…§ç‰‡ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # éæ­£å¸¸ä¼šè®®åœºæ‰€è¯†åˆ«ï¼šåœºæ™¯æ˜¯å¦ä¸ºéæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå¦‚å¨±ä¹åœºæ‰€ã€å¼€æ”¾åŒºåŸŸã€å®¶åº­ç¯å¢ƒç­‰ï¼‰ï¼Ÿ
                    # åœºæ‰€ç±»å‹ï¼šæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå¦‚ä¼šè®®å®¤ï¼‰/ éæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå…·ä½“æè¿°ï¼Œå¦‚ "KTV åŒ…å¢""å…¬å›­è‰åª"ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # å‚ä¼šäººéœ²è„¸è¯†åˆ«ï¼šæ˜¯å¦æ­£ç¡®è¯†åˆ«äººç‰©éœ²è„¸æƒ…å†µï¼ˆå…¨éƒ¨éœ²è„¸ / éƒ¨åˆ†æœªéœ²è„¸ / å…¨éƒ¨æœªéœ²è„¸ï¼‰ï¼Ÿ
                    # éœ²è„¸æƒ…å†µï¼šè‹¥æœ‰äººç‰©ï¼Œæ ‡æ³¨ "å…¨éƒ¨éœ²è„¸" æˆ– "X äººæœªéœ²è„¸"ï¼›æ— äººç‰©åˆ™æ ‡æ³¨ "æ— äººç‰©"ã€‚
                    # éåˆè§„ç‰©ä»¶è¯†åˆ«ï¼šæ˜¯å¦å‡ºç°ä¸ä¼šè®®æ— å…³çš„éåˆè§„ç‰©ä»¶ï¼ˆå¦‚é…’ç“¶ã€éº»å°†æ¡Œã€é…’æ¯ï¼Œæ¸¸æˆæœºç­‰ï¼‰ï¼Œéœ€è¦ç€é‡æ£€æŸ¥ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆå…·ä½“ç‰©ä»¶ï¼šXXXï¼‰/ å¦ï¼ˆæ— éåˆè§„ç‰©ä»¶ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    '''
                },
                {
                    "role": "user",
                    "content": [
                        # LLaMA3 Vision input format differs from some others
                        {"type": "text", "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"},
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        },
                    ]
                }
            ],
            max_tokens=4096 # Increase max tokens if needed
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Groqåˆ†æå‡ºé”™: {str(e)}"


# æ·»åŠ OpenAI/Geminiå®¡æŸ¥æ€»ç»“å‡½æ•°
def summarize_with_openai(cv_results, groq_analysis, api_key, image_dimensions=None):
    # Use Gemini only if API key is provided
    if not api_key:
        return "Gemini API Key æœªè®¾ç½®"

    try:
        # Use google.generativeai endpoint for Gemini
        client = OpenAI(api_key=api_key, base_url="https://generativelanguage.googleapis.com/v1beta") # Corrected base URL

        # å°†å›¾ç‰‡å°ºå¯¸ä¿¡æ¯æ·»åŠ åˆ°CVç»“æœä¸­
        cv_payload = cv_results.copy() # Avoid modifying original dict
        if image_dimensions:
            cv_payload["image_dimensions"] = image_dimensions

        # Ensure payload is JSON serializable (convert numpy types if any)
        def convert_types(obj):
            if isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            elif isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, dict):
                return {k: convert_types(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_types(i) for i in obj]
            return obj

        cv_payload_serializable = convert_types(cv_payload)


        response = client.chat.completions.create(
            model="models/gemini-1.5-flash-latest", # Use appropriate Gemini model
            messages=[
                {
                    "role": "user", # System role might not be directly supported, combine into user prompt
                    "parts": [
                        {
                            "text": f"""ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„è¯å‚å®¡æŸ¥å…³å‘˜ã€‚è¯·æ ¹æ®ä»¥ä¸‹CVæ£€æµ‹ç»“æœå’ŒAIåˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„åˆè§„æ€§åˆ†ææŠ¥å‘Šã€‚
                            é‡ç‚¹å…³æ³¨ï¼š
                            0. æ˜¯å¦æœ‰sanofiçš„logoï¼Œæˆ–è€…æ˜ç¡®å’Œsanofiç›¸å…³çš„å› ç´ ï¼Œå¯¹äºæ˜ç¡®çš„sanofi logoï¼Œä¸å»ºè®®ç›´æ¥åˆ¤æ–­ä¸ºè¿è§„
                            1. åœºæ™¯åˆè§„æ€§ï¼ˆæ˜¯å¦ä¸ºæ­£å¼ä¼šè®®åœºæ‰€),æ ¹æ®ä¸»ä½“å†…å®¹ä¿¡æ¯åˆ¤æ–­, å®¤å¤–åœºæ™¯ä¸è¢«å…è®¸
                            2. äººå‘˜æƒ…å†µï¼ˆäººæ•°ã€ç€è£…ã€è¡Œä¸ºæ˜¯å¦å¾—ä½“ï¼‰ï¼Œè‹¥äººæ•°ä¸º0ï¼Œä¸è¢«å…è®¸
                            3. ç‰©å“åˆè§„æ€§ï¼ˆæ˜¯å¦æœ‰è¿è§„ç‰©å“å¦‚é…’ç²¾é¥®å“ï¼‰
                            4. å›¾ç‰‡é£æ ¼æ˜¯å¦ä¸ºçœŸå®ï¼Œä»…çœŸå®ç…§ç‰‡è¢«å…è®¸ã€‚è‹¥æ˜¯æˆªå›¾æˆ–è€…ç¿»æ‹ï¼Œä»¥åŠè‡ªæ‹ï¼Œä¸è¢«å…è®¸
                            5. æ–‡å­—ä¿¡æ¯åˆè§„æ€§:æ€»ç»“AIåˆ†æç»“æœå†…çš„æå–å›¾ç‰‡å†…å®¹ä¸­çš„æ–‡å­—ä¿¡æ¯ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œåˆ¤æ–­æ˜¯å¦åˆé€‚
                            6. å›¾ç‰‡å°ºå¯¸: æ˜¯å¦å­˜åœ¨æ‹‰ä¼¸æˆ–è€…å‹ç¼©ï¼Œæ˜¯å¦ä¸ºéæ ‡å‡†ç…§ç‰‡å°ºå¯¸ï¼Œè‹¥éæ ‡å‡†å°ºå¯¸ï¼Œä»…æé†’å¯èƒ½è¢«è£å‰ªï¼Œä¸å»ºè®®ç›´æ¥åˆ¤æ–­ä¸ºè¿è§„ï¼Œå¸¸è§çš„å°ºå¯¸ä¸ºï¼š
                            1.0: "1:1 (æ­£æ–¹å½¢)",
                            1.33: "4:3 (å¸¸è§å±å¹•)",
                            1.5: "3:2 (ä¼ ç»Ÿç›¸æœº)",
                            1.78: "16:9 (å®½å±)",
                            1.85: "1.85:1 (ç”µå½±)",
                            2.35: "2.35:1 (ç”µå½±å®½é“¶å¹•)"
                            7. æ•´ä½“è¯„ä¼°ï¼ˆæ˜¯å¦å»ºè®®é€šè¿‡å®¡æ ¸ï¼‰
                            è¯·ä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºï¼Œè¡¨æ ¼éœ€è¦åŒ…å«å®¡æ ¸é¡¹ç›®ï¼Œå®¡æ ¸ç»“æœï¼Œå®¡æ ¸åŸå› ä¸‰åˆ—ï¼Œå¯¹äºè¿è§„æƒ…å†µï¼Œè¯·åŠ ç²—å­—ä½“ï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚

                            CVæ£€æµ‹ç»“æœï¼š{cv_payload_serializable}

                            AIåˆ†æç»“æœï¼š{groq_analysis}
                            """
                        }
                    ]
                }
            ]
             # Add generation_config if needed, e.g., temperature=0.5
            # generation_config={"temperature": 0.5}
        )
        # Access Gemini response correctly
        if response.choices and response.choices[0].message and response.choices[0].message.content:
             return response.choices[0].message.content
        elif response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            # Handle potential structured response from Gemini
            return "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text'))
        else:
            return "OpenAI/Geminiè¿”å›äº†ç©ºçš„æ€»ç»“"
    except Exception as e:
        # Log the full error for debugging
        st.error(f"OpenAI/Geminiæ€»ç»“å‡ºé”™: {type(e).__name__} - {str(e)}")
        # You might want to print the full traceback in your server logs
        # import traceback
        # print(traceback.format_exc())
        return f"OpenAI/Geminiæ€»ç»“å‡ºé”™: {str(e)}"

# æ·»åŠ å“ˆå¸Œè®¡ç®—å‡½æ•°
def get_image_hash(image_path):
    """
    è®¡ç®—å›¾ç‰‡çš„å“ˆå¸Œå€¼ç”¨äºå»é‡
    """
    try:
        with open(image_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] Error hashing image {image_path}: {e}")
        return None

# æ·»åŠ äººè„¸èšç±»å‡½æ•° (Using MTCNN for detection, DeepFace for encoding)
def face_clustering(input_folder, output_folder, tolerance=0.4, min_cluster_size=3):
    """
    Cluster similar faces from a collection of images using MTCNN and DeepFace.

    Parameters:
    - input_folder: Path to folder containing images
    - output_folder: Path to output the clustered images
    - tolerance: Cosine distance threshold for clustering (lower is stricter, e.g., 0.4 corresponds to ~0.6 similarity)
    - min_cluster_size: Minimum number of images needed to form a cluster
    """
    # Create output directory if it doesn't exist
    os.makedirs(output_folder, exist_ok=True)

    # Create a directory for images where no faces were detected
    no_faces_dir = os.path.join(output_folder, "no_faces_detected")
    os.makedirs(no_faces_dir, exist_ok=True)

    # Create a directory for unclustered faces
    unclustered_dir = os.path.join(output_folder, "unclustered_faces")
    os.makedirs(unclustered_dir, exist_ok=True)

    print(f"[INFO] Processing images from {input_folder}")

    # Get all image files from the input folder
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff'] # Added more extensions
    image_paths = []
    for root, _, files in os.walk(input_folder):
        for file in files:
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_paths.append(os.path.join(root, file))

    print(f"[INFO] Found {len(image_paths)} images")

    # Initialize MTCNN detector
    try:
        detector = MTCNN()
    except Exception as e:
        st.error(f"Failed to initialize MTCNN in clustering: {e}")
        return output_folder # Return early

    # Initialize lists to store data
    known_encodings = [] # Stores DeepFace embeddings
    known_image_paths = [] # Stores the path of the original image for each embedding

    # Process each image
    for (i, image_path) in enumerate(image_paths):
        print(f"[INFO] Processing image {i+1}/{len(image_paths)}")

        try:
            # Load the image using OpenCV (handles various formats)
            image = cv2.imread(image_path)
            if image is None:
                print(f"[WARNING] Could not read image {image_path}")
                continue

            # Convert BGR to RGB (MTCNN expects RGB)
            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

            # Detect faces using MTCNN
            faces = detector.detect_faces(rgb)

            # If no faces were found, copy to the no_faces directory
            if not faces:
                shutil.copy2(image_path, os.path.join(no_faces_dir, os.path.basename(image_path)))
                continue

            # Compute facial embeddings for each detected face
            for face in faces:
                x, y, w, h = face['box']
                confidence = face['confidence']
                # Add a small buffer/check confidence if needed
                # Example: if confidence < 0.9: continue

                x1, y1 = max(0, x), max(0, y)
                x2, y2 = min(rgb.shape[1], x + w), min(rgb.shape[0], y + h)

                if x2 > x1 and y2 > y1: # Ensure valid crop
                    face_crop = rgb[y1:y2, x1:x2]
                    try:
                        # Get embedding using DeepFace (e.g., ArcFace model)
                        embedding_obj = DeepFace.represent(
                            img_path=face_crop,
                            model_name="ArcFace", # Consistent model choice
                            enforce_detection=False # Already detected
                        )
                        if embedding_obj and isinstance(embedding_obj, list):
                             embedding = embedding_obj[0]['embedding']
                             known_encodings.append(embedding)
                             known_image_paths.append(image_path) # Store path associated with this face
                        else:
                            print(f"[WARNING] DeepFace failed to get embedding for a face in {image_path}")

                    except ValueError as ve: # Handle cases where represent fails on crop
                         print(f"[WARNING] DeepFace ValueError for face in {image_path}: {ve}")
                    except Exception as e:
                         print(f"[ERROR] DeepFace represent error for face in {image_path}: {e}")

        except Exception as e:
            print(f"[ERROR] Error processing {image_path}: {e}")

    print(f"[INFO] Detected and encoded {len(known_encodings)} faces in total")

    # Cluster the faces using DBSCAN with Cosine Distance
    if len(known_encodings) >= min_cluster_size: # Need enough samples to cluster
        print("[INFO] Clustering faces...")

        # Convert encodings to numpy array
        data = np.array(known_encodings)

        # Perform DBSCAN clustering using cosine distance
        # eps is the maximum distance between samples for one to be considered as in the neighborhood of the other
        clt = DBSCAN(metric="cosine", eps=tolerance, min_samples=min_cluster_size, n_jobs=-1)
        clt.fit(data)

        # Get the unique cluster labels (-1 is for noise/outliers)
        label_ids = np.unique(clt.labels_)
        num_unique_faces = len(np.where(label_ids > -1)[0])

        print(f"[INFO] Found {num_unique_faces} unique face clusters (excluding noise)")

        # Process each cluster label
        for label_id in label_ids:
            # Get indices of faces belonging to this label
            indices = np.where(clt.labels_ == label_id)[0]

            # Determine output directory based on label
            if label_id == -1:
                current_cluster_dir = unclustered_dir
                print(f"[INFO] Copying {len(indices)} unclustered faces...")
            else:
                current_cluster_dir = os.path.join(output_folder, f"person_{label_id}")
                os.makedirs(current_cluster_dir, exist_ok=True)
                print(f"[INFO] Copying {len(indices)} faces to Cluster {label_id}...")

            # Copy images containing these faces to the corresponding directory, avoiding duplicates
            processed_hashes = set()
            unique_image_count = 0
            for idx in indices:
                image_path = known_image_paths[idx]
                image_hash = get_image_hash(image_path)

                # If image hash is valid and not already copied to this cluster dir
                if image_hash and image_hash not in processed_hashes:
                    target_path = os.path.join(current_cluster_dir, os.path.basename(image_path))
                    # Check if file already exists (might happen with multiple faces from same image in same cluster)
                    if not os.path.exists(target_path):
                       shutil.copy2(image_path, target_path)
                    processed_hashes.add(image_hash)
                    unique_image_count +=1
                # elif image_hash:
                    # print(f"[DEBUG] Skipping duplicate image hash {image_hash} for cluster {label_id}")
                # else:
                    # print(f"[WARNING] Could not get hash for {image_path}, copying anyway if target doesn't exist.")
                    # target_path = os.path.join(current_cluster_dir, os.path.basename(image_path))
                    # if not os.path.exists(target_path):
                    #    shutil.copy2(image_path, target_path)

            if label_id != -1:
                 print(f"[INFO] Cluster {label_id}: Copied {unique_image_count} unique images.")
            else:
                 print(f"[INFO] Unclustered: Copied {unique_image_count} unique images.")
    else:
         print("[INFO] Not enough faces detected ({len(known_encodings)}) to perform clustering (min required: {min_cluster_size}). Check 'unclustered_faces' and 'no_faces_detected' folders.")


    print("[INFO] Face clustering completed!")
    return output_folder


# æ·»åŠ åˆ›å»ºè’™å¤ªå¥‡å›¾çš„å‡½æ•°
def create_montages(output_folder, montage_size=(150, 150), images_per_row=5):
    """
    Create montages for each cluster to visualize the results
    """
    montages_info = []

    # Get all actual cluster subdirectories
    cluster_dirs = [d for d in os.listdir(output_folder)
                    if os.path.isdir(os.path.join(output_folder, d)) and d.startswith("person_")]

    montages_dir = os.path.join(output_folder, "montages")
    os.makedirs(montages_dir, exist_ok=True)

    for cluster_dir_name in cluster_dirs:
        cluster_full_path = os.path.join(output_folder, cluster_dir_name)
        # Get all image files in this cluster
        image_paths = [os.path.join(cluster_full_path, f)
                      for f in os.listdir(cluster_full_path)
                      if os.path.isfile(os.path.join(cluster_full_path, f))]

        # Skip if no images in the cluster
        if not image_paths:
            continue

        # Deduplicate based on hash (already done during clustering copy, but good practice here too)
        unique_images_paths = []
        image_hashes = set()

        for image_path in image_paths:
            image_hash = get_image_hash(image_path)
            if image_hash and image_hash not in image_hashes:
                image_hashes.add(image_hash)
                unique_images_paths.append(image_path)
            elif not image_hash: # Handle hash failure case
                unique_images_paths.append(image_path) # Include if hash failed

        print(f"[INFO] Cluster {cluster_dir_name}: {len(image_paths)} files, {len(unique_images_paths)} unique for montage")

        # Load and resize images (using unique paths)
        images_for_montage = []
        max_images_for_montage = images_per_row * 10 # Limit images per montage for performance/size
        random.shuffle(unique_images_paths) # Show a variety if too many images

        for image_path in unique_images_paths[:max_images_for_montage]:
            try:
                image = cv2.imread(image_path)
                if image is not None:
                    image = cv2.resize(image, montage_size)
                    images_for_montage.append(image)
                else:
                    print(f"[WARNING] Failed to read image for montage: {image_path}")
            except Exception as e:
                print(f"[ERROR] Failed to load/resize image {image_path} for montage: {e}")


        # Create montage
        if images_for_montage:
            # Calculate grid size dynamically
            num_images = len(images_for_montage)
            rows = (num_images + images_per_row - 1) // images_per_row
            montage = build_montages(images_for_montage, montage_size, (images_per_row, rows))[0]

            # Save the montage
            montage_filename = f"{cluster_dir_name}_montage.jpg"
            montage_path = os.path.join(montages_dir, montage_filename)
            try:
                cv2.imwrite(montage_path, montage)
                print(f"[INFO] Created montage: {montage_path} with {len(images_for_montage)} images")
                montages_info.append({
                    "path": montage_path,
                    "cluster": cluster_dir_name,
                    "images_count": len(images_for_montage) # Count actually used in montage
                })
            except Exception as e:
                 print(f"[ERROR] Failed to save montage {montage_path}: {e}")
        else:
            print(f"[INFO] No valid images found for montage in cluster {cluster_dir_name}")


    print("[INFO] Montage creation completed!")
    return montages_info

# ä¿®æ”¹process_face_for_advanced_searchå‡½æ•°ï¼Œä½¿ç”¨DeepFaceæ¯”è¾ƒ
def process_face_for_advanced_search(detected_faces_arrays, tolerance=0.4):
    # Find the latest cluster results directory
    current_dir = Path(".") # Use current working directory where Streamlit runs
    try:
        cluster_parent_dirs = [d for d in current_dir.iterdir()
                               if d.is_dir() and d.name.startswith("clustered_faces_")]
        if not cluster_parent_dirs:
            st.warning("æœªåœ¨å½“å‰ç›®å½•ä¸‹æ‰¾åˆ°äººè„¸èšç±»æ•°æ®(clustered_faces_*)ï¼Œè¯·å…ˆè¿è¡Œç¦»çº¿èšç±»å·¥å…·ã€‚")
            return []

        # Sort by name (assuming timestamp is in name like clustered_faces_YYYYMMDD_HHMMSS)
        cluster_parent_dirs.sort(key=lambda x: x.name, reverse=True)
        latest_cluster_dir = cluster_parent_dirs[0]
    except Exception as e:
         st.error(f"æŸ¥æ‰¾èšç±»ç›®å½•æ—¶å‡ºé”™: {e}")
         return []


    montages_dir = latest_cluster_dir / "montages"
    if not montages_dir.exists():
        st.warning(f"æœªæ‰¾åˆ°è’™å¤ªå¥‡å›¾ç‰‡ç›®å½•: {montages_dir}")
        # We can still proceed by comparing against individual cluster images if montages are missing
        # return []

    # Get all person cluster directories within the latest results
    try:
        person_dirs = [d for d in latest_cluster_dir.iterdir()
                       if d.is_dir() and d.name.startswith("person_")]
        if not person_dirs:
            st.warning(f"åœ¨ {latest_cluster_dir} ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆçš„äººè„¸èšç±»ç»„ (person_*)")
            return []
    except Exception as e:
         st.error(f"æŸ¥æ‰¾personç›®å½•æ—¶å‡ºé”™: {e}")
         return []

    # Results list for each detected face
    face_best_matches = []
    model_name = "ArcFace" # Use consistent model

    # Process each detected face (which are numpy arrays)
    for face_idx, face_array in enumerate(detected_faces_arrays):
        if face_array is None or face_array.size == 0:
            continue

        try:
            # Get embedding for the current detected face
            current_face_embedding_obj = DeepFace.represent(
                img_path=face_array, # Pass numpy array directly
                model_name=model_name,
                enforce_detection=False
            )
            if not current_face_embedding_obj or not isinstance(current_face_embedding_obj, list):
                 print(f"æ— æ³•ä¸ºæ£€æµ‹åˆ°çš„äººè„¸ #{face_idx} è·å–embedding")
                 continue
            current_face_embedding = current_face_embedding_obj[0]['embedding']
            current_face_embedding = np.array(current_face_embedding) # Ensure numpy array

        except Exception as e:
            print(f"å¤„ç†æ£€æµ‹åˆ°çš„äººè„¸ #{face_idx} æ—¶è·å–embeddingå‡ºé”™: {e}")
            continue # Skip this face if embedding fails

        # Find the best matching cluster for this face
        best_match_info = None
        best_similarity_score = -1 # Initialize with -1 (cosine similarity is between -1 and 1)

        for person_dir_path in person_dirs:
            cluster_name = person_dir_path.name
            # Get image files within this person cluster directory
            try:
                 face_image_files = [f for f in person_dir_path.iterdir()
                                    if f.is_file() and f.suffix.lower() in ['.jpg', '.jpeg', '.png', '.bmp']]
            except Exception as e:
                 print(f"è¯»å–èšç±»ç›®å½• {cluster_name} æ—¶å‡ºé”™: {e}")
                 continue

            if not face_image_files:
                continue

            # Sample some images from the cluster for comparison
            sample_size = min(5, len(face_image_files))
            sampled_image_paths = random.sample(face_image_files, sample_size)

            cluster_embeddings = []
            # Get embeddings for the sampled images
            for img_path in sampled_image_paths:
                try:
                    # Use DeepFace.represent on the image path
                    # Enforce detection to find the main face in the sample image
                    embedding_obj = DeepFace.represent(
                        img_path=str(img_path),
                        model_name=model_name,
                        enforce_detection=True # Find face in sample image
                    )
                    if embedding_obj and isinstance(embedding_obj, list):
                        cluster_embeddings.append(embedding_obj[0]['embedding'])
                except ValueError: # Handles case where DeepFace finds no face
                    # print(f"No face found by DeepFace in cluster sample: {img_path.name}")
                    pass
                except Exception as e:
                    print(f"è·å–èšç±»æ ·æœ¬ {img_path.name} çš„embeddingæ—¶å‡ºé”™: {e}")

            if not cluster_embeddings:
                continue # No valid embeddings from this cluster's sample

            # Calculate cosine similarity between the detected face and cluster samples
            cluster_embeddings_array = np.array(cluster_embeddings)
            similarities = cosine_similarity(current_face_embedding.reshape(1, -1), cluster_embeddings_array)[0]

            # Find the max similarity within this cluster
            if similarities.size > 0:
                max_similarity_in_cluster = np.max(similarities)

                # Compare with overall best similarity found so far
                # Similarity threshold check: higher similarity is better
                # Convert distance tolerance to similarity threshold: similarity >= 1 - tolerance
                similarity_threshold = 1.0 - tolerance
                if max_similarity_in_cluster >= similarity_threshold and max_similarity_in_cluster > best_similarity_score:
                    best_similarity_score = max_similarity_in_cluster

                    # Find the corresponding montage path (if it exists)
                    montage_path = montages_dir / f"{cluster_name}_montage.jpg"

                    best_match_info = {
                        "path": str(montage_path) if montage_path.exists() else None, # Store path or None
                        "cluster": cluster_name,
                        "similarity": float(best_similarity_score),
                        "images_count": len(face_image_files), # Total images in this cluster
                        "face_idx": face_idx
                    }

        # Append the best match found for this face_idx (if any)
        if best_match_info:
            face_best_matches.append(best_match_info)

    return face_best_matches


def check_image_dimensions(image):
    try:
        width, height = image.size
        if height == 0: return None # Avoid division by zero

        # Standard Sizes (Approximate aspect ratios)
        standard_sizes_info = {
            (640, 480): "4:3", (800, 600): "4:3", (1024, 768): "4:3",
            (1280, 720): "16:9", (1920, 1080): "16:9", (2560, 1440): "16:9", (3840, 2160): "16:9",
            (1280, 960): "4:3", (2048, 1536): "4:3", # More 4:3
            (1280, 800): "16:10", (1440, 900): "16:10", (1680, 1050): "16:10", (1920, 1200): "16:10", # 16:10
            (1080, 1920): "9:16 (Vertical)", # Vertical
            (750, 1334): "~9:16 (iPhone)", # Common Mobile
            (1125, 2436): "~9:19.5 (iPhone X)",
            (1080, 1080): "1:1 (Square)", (2048, 2048): "1:1 (Square)" # Square
        }

        tolerance = 15 # Pixel tolerance for matching standard sizes

        is_standard = False
        standard_ratio_name = "Non-standard"
        for (std_w, std_h), ratio_name in standard_sizes_info.items():
            if (abs(width - std_w) <= tolerance and abs(height - std_h) <= tolerance) or \
               (abs(width - std_h) <= tolerance and abs(height - std_w) <= tolerance): # Check rotated
                is_standard = True
                standard_ratio_name = ratio_name
                break

        # Calculate aspect ratio
        aspect_ratio = width / height

        # Determine common ratio name based on value
        ratio_thresholds = {
            (0.95, 1.05): "1:1 (Square)",
            (1.30, 1.40): "4:3",
            (1.45, 1.55): "3:2",
            (1.58, 1.68): "16:10", # Between 3:2 and 16:9
            (1.72, 1.82): "16:9",
            (0.70, 0.80): "3:4 (Vertical)", # Approx vertical 4:3
            (0.52, 0.62): "9:16 (Vertical)", # Approx vertical 16:9
        }

        calculated_ratio_name = "Other Ratio"
        for (low, high), name in ratio_thresholds.items():
            if low <= aspect_ratio <= high:
                calculated_ratio_name = name
                break

        # Refine name if it matched a standard size directly
        final_ratio_name = standard_ratio_name if is_standard else calculated_ratio_name

        return {
            "width": width,
            "height": height,
            "aspect_ratio": round(aspect_ratio, 4),
            "ratio_name": final_ratio_name,
            "is_standard_size": is_standard
        }
    except Exception as e:
        print(f"Error checking dimensions: {e}")
        return None

# ... existing code ...
def extract_text_from_analysis(analysis):
    # Use Gemini/OpenAI for extraction if key is available
    api_key = os.environ.get("GEMINI_API_KEY")
    if not api_key:
        # Fallback or simple extraction if no API key
        try:
            # Basic keyword search in the analysis text
            lines = analysis.split('\n')
            text_info_line = next((line for line in lines if "# æ–‡å­—ä¿¡æ¯ï¼š" in line), None)
            if text_info_line:
                extracted = text_info_line.split("# æ–‡å­—ä¿¡æ¯ï¼š", 1)[1].strip()
                if extracted and extracted not in ["æœªæåŠ", "æ— æ³•åˆ¤æ–­", "æ— "]:
                    return extracted
            return "æ— æ–‡å­—å†…å®¹" # Default if not found
        except Exception:
            return "æå–æ–‡å­—ä¿¡æ¯å¤±è´¥ (Fallback)" # Fallback error

    try:
        client = OpenAI(api_key=api_key, base_url="https://generativelanguage.googleapis.com/v1beta")
        response = client.chat.completions.create(
            model="models/gemini-1.5-flash-latest",
            messages=[
                {
                     "role": "user",
                     "parts": [
                         {
                              "text": f"""ä»ä»¥ä¸‹AIåˆ†æç»“æœä¸­ï¼Œä»…æå– '# æ–‡å­—ä¿¡æ¯ï¼š' éƒ¨åˆ†åé¢çš„å…·ä½“å†…å®¹ã€‚
                              å¦‚æœè¯¥éƒ¨åˆ†å†…å®¹æ˜¯ 'æœªæåŠ'ã€'æ— æ³•åˆ¤æ–­' æˆ– 'æ— 'ï¼Œåˆ™è¾“å‡º 'æ— æ–‡å­—å†…å®¹'ã€‚
                              ä¸è¦åŒ…å« '# æ–‡å­—ä¿¡æ¯ï¼š' æœ¬èº«ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€æ³¨é‡Šæˆ–æ ‡é¢˜ã€‚

                              AIåˆ†æç»“æœ:
                              ---
                              {analysis}
                              ---

                              æå–çš„æ–‡å­—å†…å®¹:"""
                         }
                    ]
                }
            ],
             # Add generation_config if needed
            # generation_config={"temperature": 0.1, "stop_sequences": ["\n"]} # Example config
        )
        # Access Gemini response
        if response.choices and response.choices[0].message and response.choices[0].message.content:
             result = response.choices[0].message.content.strip()
             # Final check if the model still included boilerplate
             if result.startswith("æå–çš„æ–‡å­—å†…å®¹:"):
                 result = result.replace("æå–çš„æ–‡å­—å†…å®¹:", "").strip()
             if not result or result in ["æœªæåŠ", "æ— æ³•åˆ¤æ–­", "æ— "]:
                 return "æ— æ–‡å­—å†…å®¹"
             return result
        elif response.candidates and response.candidates[0].content and response.candidates[0].content.parts:
            result = "".join(part.text for part in response.candidates[0].content.parts if hasattr(part, 'text')).strip()
            if result.startswith("æå–çš„æ–‡å­—å†…å®¹:"):
                 result = result.replace("æå–çš„æ–‡å­—å†…å®¹:", "").strip()
            if not result or result in ["æœªæåŠ", "æ— æ³•åˆ¤æ–­", "æ— "]:
                 return "æ— æ–‡å­—å†…å®¹"
            return result
        else:
            return "æå–æ–‡å­—ä¿¡æ¯å¤±è´¥ (Geminiæ— å“åº”)"
    except Exception as e:
        return f"æå–æ–‡å­—ä¿¡æ¯å¤±è´¥ (Gemini): {str(e)}"

def initialize_text_vectordb():
    text_vector_file = data_dir / "vectors" / "text_vectors.pkl"
    text_db = {}
    if os.path.exists(text_vector_file):
        try:
            with open(text_vector_file, "rb") as f:
                text_db = pickle.load(f)
            if not isinstance(text_db, dict):
                 st.warning("æ–‡æœ¬å‘é‡æ•°æ®åº“æ ¼å¼é”™è¯¯ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
                 text_db = {}
        except (pickle.UnpicklingError, EOFError, TypeError) as e:
            st.warning(f"è¯»å–æ–‡æœ¬å‘é‡æ•°æ®åº“å¤±è´¥ ({e})ï¼Œå°†é‡æ–°åˆ›å»ºã€‚")
            text_db = {}
    return text_db

def save_text_vector_db(text_db):
    text_vector_file = data_dir / "vectors" / "text_vectors.pkl"
    try:
        with open(text_vector_file, "wb") as f:
            pickle.dump(text_db, f)
    except Exception as e:
        st.error(f"ä¿å­˜æ–‡æœ¬å‘é‡æ•°æ®åº“å¤±è´¥: {e}")


def update_text_vector_db(image_id, text_content):
    if not text_content or text_content == "æ— æ–‡å­—å†…å®¹":
        print("è·³è¿‡æ–‡æœ¬å‘é‡æ›´æ–°ï¼šæ— æœ‰æ•ˆæ–‡æœ¬å†…å®¹ã€‚")
        return

    # åˆå§‹åŒ–æ–‡å­—å‘é‡æ•°æ®åº“
    text_db = initialize_text_vectordb()

    try:
        # Lazy load sentence transformer
        from sentence_transformers import SentenceTransformer

        @st.cache_resource
        def load_sentence_transformer():
            try:
                # Specify cache folder within the data directory if possible
                cache_folder = str(data_dir / "st_cache")
                os.makedirs(cache_folder, exist_ok=True)
                print(f"Loading Sentence Transformer model (cache: {cache_folder})...")
                # You might need to pre-download the model if running in restricted envs
                model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2", cache_folder=cache_folder)
                print("Sentence Transformer model loaded.")
                return model
            except Exception as e:
                st.error(f"åŠ è½½Sentence Transformeræ¨¡å‹å¤±è´¥: {e}")
                print(f"Error loading Sentence Transformer: {e}")
                return None

        model = load_sentence_transformer()
        if model is None:
             st.error("æ— æ³•åŠ è½½æ–‡æœ¬ç¼–ç æ¨¡å‹ï¼Œè·³è¿‡æ–‡æœ¬å‘é‡æ›´æ–°ã€‚")
             return

        # Check if text is too long - adjust if needed, or chunk
        # max_seq_length = model.max_seq_length # Get model's max sequence length
        # if len(text_content) > max_seq_length * 4: # Heuristic: truncate very long text
        #     print(f"æ–‡æœ¬å†…å®¹è¿‡é•¿ ({len(text_content)} chars)ï¼Œå°†æˆªæ–­ã€‚")
        #     text_content = text_content[:max_seq_length*4]

        print(f"ä¸ºå›¾ç‰‡ {image_id} ç”Ÿæˆæ–‡æœ¬å‘é‡...")
        text_vector = model.encode(text_content, show_progress_bar=False)
        print(f"æ–‡æœ¬å‘é‡ç”Ÿæˆå®Œæ¯•ï¼Œç»´åº¦: {text_vector.shape}")

        text_db[image_id] = {
            'vector': text_vector.astype(np.float32), # Store as float32 to save space
            'text': text_content
        }

        # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
        save_text_vector_db(text_db)
        print(f"æ–‡æœ¬å‘é‡æ•°æ®åº“å·²ä¸º {image_id} æ›´æ–°ã€‚")

    except ImportError:
        st.error("è¯·å®‰è£… 'sentence-transformers' åº“ä»¥å¯ç”¨æ–‡æœ¬ç›¸ä¼¼æ€§æœç´¢: pip install sentence-transformers")
        print("sentence-transformers not installed.")
    except Exception as e:
        st.error(f"æ›´æ–°æ–‡æœ¬å‘é‡æ•°æ®åº“æ—¶å‡ºé”™: {str(e)}")
        print(f"Error updating text vector DB: {str(e)}")


def search_similar_texts(image_id, top_k=3, threshold=0.5):
    text_db = initialize_text_vectordb()

    if not text_db or image_id not in text_db:
        print(f"æ–‡æœ¬æ•°æ®åº“ä¸ºç©ºæˆ–æŸ¥è¯¢ID {image_id} ä¸å­˜åœ¨ã€‚")
        return []

    query_entry = text_db.get(image_id)
    if not query_entry or 'vector' not in query_entry:
        print(f"æŸ¥è¯¢ID {image_id} çš„å‘é‡ä¸å­˜åœ¨ã€‚")
        return []

    query_vector = query_entry['vector']
    if query_vector is None: return []

    results = []
    query_vector_norm = np.linalg.norm(query_vector)
    if query_vector_norm == 0: return []

    print(f"æœç´¢ä¸å›¾ç‰‡ {image_id} æ–‡æœ¬ç›¸ä¼¼çš„å†…å®¹...")
    for text_id, data in text_db.items():
        # Skip comparing the item with itself
        if text_id == image_id:
            continue

        db_vector = data.get('vector')
        if db_vector is None or not isinstance(db_vector, np.ndarray):
            continue

        db_vector_norm = np.linalg.norm(db_vector)
        if db_vector_norm == 0: continue

        try:
            # Calculate Cosine Similarity
            similarity = np.dot(query_vector, db_vector) / (query_vector_norm * db_vector_norm)
            similarity = float(similarity) # Ensure float

            if similarity >= threshold:
                results.append({
                    'id': text_id, # This ID links back to the image
                    'text': data['text'],
                    'similarity': similarity
                })
        except Exception as e:
            print(f"è®¡ç®—æ–‡æœ¬å‘é‡ç›¸ä¼¼åº¦æ—¶å‡ºé”™ (ID: {text_id}): {str(e)}")
            continue

    # Sort by similarity
    results.sort(key=lambda x: x['similarity'], reverse=True)
    print(f"æ‰¾åˆ° {len(results)} ä¸ªç›¸ä¼¼æ–‡æœ¬ (é˜ˆå€¼ > {threshold})ã€‚")

    return results[:top_k]


def display_similar_texts(similar_texts):
    if not similar_texts:
        st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„æ–‡æœ¬å†…å®¹ã€‚")
        return

    st.subheader(f"ç›¸ä¼¼æ–‡å­—å†…å®¹æ£€æŸ¥ç»“æœ (Top {len(similar_texts)})")

    # Display results
    cols = st.columns(min(len(similar_texts), 3))
    image_db, _ = initialize_vector_db() # Load image DB to find associated images

    for i, result in enumerate(similar_texts):
        with cols[i % 3]:
            text_id = result['id'] # This is the image_id associated with the similar text

            # Try to find the corresponding image
            image_info = image_db.get(text_id)
            if image_info and 'path' in image_info:
                image_path = Path(image_info['path'])
                try:
                    if image_path.exists():
                        img = Image.open(image_path)
                        st.image(img, caption=f"ç›¸ä¼¼åº¦: {result['similarity']:.2f}", use_container_width=True)

                        if 'timestamp' in image_info:
                            time_str = time.strftime("%Y-%m-%d %H:%M:%S",
                                                     time.localtime(image_info['timestamp']))
                            st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")

                        # Show the similar text in an expander
                        with st.expander("æŸ¥çœ‹ç›¸ä¼¼æ–‡å­—å†…å®¹"):
                            st.write(result['text'])
                    else:
                        st.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path.name}")
                        # Still show the text even if image is missing
                        with st.expander("æŸ¥çœ‹ç›¸ä¼¼æ–‡å­—å†…å®¹ (å›¾ç‰‡ä¸¢å¤±)"):
                             st.write(result['text'])
                             st.caption(f"(æ¥è‡ªå›¾ç‰‡ ID: {text_id})")

                except Exception as e:
                     st.error(f"æ˜¾ç¤ºç›¸ä¼¼æ–‡æœ¬å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                     # Still show text on error
                     with st.expander("æŸ¥çœ‹ç›¸ä¼¼æ–‡å­—å†…å®¹ (æ˜¾ç¤ºé”™è¯¯)"):
                         st.write(result['text'])
                         st.caption(f"(æ¥è‡ªå›¾ç‰‡ ID: {text_id})")
            else:
                 # If image record not found, just show the text
                 st.warning(f"æœªæ‰¾åˆ°IDä¸º {text_id} çš„å›¾ç‰‡è®°å½•")
                 with st.expander("æŸ¥çœ‹ç›¸ä¼¼æ–‡å­—å†…å®¹"):
                     st.write(result['text'])
                     st.caption(f"(æ¥è‡ªå›¾ç‰‡ ID: {text_id}, ç›¸ä¼¼åº¦: {result['similarity']:.2f})")


# æ·»åŠ ä¸€ä¸ªæ¸…é™¤å¤„ç†çŠ¶æ€çš„å‡½æ•°
def reset_processed_state():
    keys_to_reset = [
        'processed', 'detection_results', 'processed_img', 'detected_faces',
        'similar_images', 'similar_faces_results', 'advanced_face_search_done',
        'face_best_matches', 'similar_texts'
        # Keep 'last_upload_id', 'image', 'current_tolerance' maybe?
        # Resetting 'image' means the preview disappears on re-run after upload.
        # Let's keep 'image' and 'last_upload_id' if an upload happened.
    ]
    for key in keys_to_reset:
        if key in st.session_state:
             # Reset to default values
             if key in ['detected_faces', 'similar_images', 'similar_faces_results', 'face_best_matches', 'similar_texts']:
                 st.session_state[key] = []
             elif key == 'processed' or key == 'advanced_face_search_done':
                 st.session_state[key] = False
             else:
                 st.session_state[key] = None
    print("Processed state reset.")


# ä¿®æ”¹å›¾ç‰‡å‹ç¼©å‡½æ•°
def compress_image(image, max_size_kb=500, quality=65, max_dimension=1500): # Increased default quality slightly
    """
    Compresses a PIL Image object to be below a target size, also resizing if needed.

    Args:
        image (PIL.Image.Image): Input image.
        max_size_kb (int): Target maximum size in kilobytes.
        quality (int): Initial JPEG quality (1-95).
        max_dimension (int): Maximum width or height allowed.

    Returns:
        PIL.Image.Image: Compressed (and potentially resized) image.
    """
    try:
        img = image.copy() # Work on a copy

        # 1. Resize if dimensions exceed max_dimension
        width, height = img.size
        if max(width, height) > max_dimension:
            ratio = max_dimension / max(width, height)
            new_width = int(width * ratio)
            new_height = int(height * ratio)
            print(f"Resizing image from {width}x{height} to {new_width}x{new_height}")
            img = img.resize((new_width, new_height), Image.Resampling.LANCZOS) # Use LANCZOS

        # 2. Ensure RGB format (JPEG doesn't support alpha)
        if img.mode != 'RGB':
            img = img.convert('RGB')

        # 3. Iteratively reduce quality to meet size target
        buffer = BytesIO()
        current_quality = quality
        img.save(buffer, format='JPEG', quality=current_quality, optimize=True)
        current_size_kb = buffer.tell() / 1024

        while current_size_kb > max_size_kb and current_quality > 10:
            # Reduce quality more aggressively if far from target
            reduction = 10 if current_size_kb > max_size_kb * 1.5 else 5
            current_quality -= reduction
            print(f"Reducing quality to {current_quality}...")
            buffer = BytesIO() # Reset buffer
            img.save(buffer, format='JPEG', quality=current_quality, optimize=True)
            current_size_kb = buffer.tell() / 1024

        # 4. If still too large after quality reduction, resize further (optional, maybe warn instead)
        # This part might degrade quality significantly. Consider just returning the lowest quality version.
        if current_size_kb > max_size_kb and current_quality <= 10:
             print(f"Warning: Image still exceeds size limit ({current_size_kb:.1f}KB) even at lowest quality (10). Further resizing might occur.")
             # Optional: Add further resizing loop here if absolutely necessary
             # while current_size_kb > max_size_kb and max(img.size) > 300: # Example condition
             #     width, height = img.size
             #     img = img.resize((int(width * 0.9), int(height * 0.9)), Image.Resampling.LANCZOS)
             #     buffer = BytesIO()
             #     img.save(buffer, format='JPEG', quality=10, optimize=True)
             #     current_size_kb = buffer.tell() / 1024


        buffer.seek(0)
        print(f"Final compressed size: {current_size_kb:.1f} KB, Quality: {current_quality}, Dimensions: {img.size}")
        return Image.open(buffer)

    except Exception as e:
        st.error(f"å›¾ç‰‡å‹ç¼©å¤±è´¥: {e}")
        return image # Return original on failure


# æ·»åŠ æ‰¹é‡å¤„ç†å‡½æ•°
def process_batch_images(uploaded_files):
    """æ‰¹é‡å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡"""
    processed_count = 0
    failed_files = []

    with st.spinner(f"å‡†å¤‡å¤„ç† {len(uploaded_files)} å¼ å›¾ç‰‡..."):
        # --- Pre-load models ---
        try:
            model = load_model(selected_model)
        except Exception as e:
            st.error(f"æ— æ³•åŠ è½½YOLOæ¨¡å‹ {selected_model}ï¼Œæ‰¹é‡å¤„ç†ä¸­æ­¢ã€‚é”™è¯¯ï¼š{e}")
            return 0, []

        face_detector = None
        if detect_faces:
            try:
                face_detector = load_face_detector()
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹ï¼Œå°†è·³è¿‡äººè„¸æ£€æµ‹ã€‚é”™è¯¯ï¼š{e}")
                # Continue without face detection? Or stop? Let's continue for now.
                # return 0, [] # Uncomment to stop if face detector fails

        feature_extractor = None
        transform = None
        if enable_similarity_search or enable_face_similarity:
            try:
                feature_extractor = load_feature_extractor()
                transform = get_transform()
            except Exception as e:
                st.error(f"æ— æ³•åŠ è½½ç‰¹å¾æå–æ¨¡å‹ï¼Œå°†è·³è¿‡ç›¸ä¼¼æ€§æœç´¢ã€‚é”™è¯¯ï¼š{e}")
                # Continue without similarity? Let's continue.

        # --- Clear previous batch state ---
        st.session_state.batch_images = []
        st.session_state.batch_faces = [] # List of lists (one list of faces per image)
        st.session_state.batch_processed_images = []
        st.session_state.batch_image_vectors = {} # {image_id: {'vector': vec, 'index': i}}
        st.session_state.batch_face_vectors = {} # {face_id: {'vector': vec, 'image_index': i, 'face_index': j}}

        # Get selected object classes for detection
        selected_classes = []
        if detect_person: selected_classes.append("äºº")
        if detect_cup: selected_classes.append("æ¯å­/é…’æ¯")
        if detect_bottle: selected_classes.append("ç“¶å­")
        if detect_all: selected_classes.append("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“")

    # --- Process each image ---
    prog_bar = st.progress(0)
    status_text = st.empty()

    for i, uploaded_file in enumerate(uploaded_files):
        status_text.text(f"å¤„ç†å›¾ç‰‡ {i+1}/{len(uploaded_files)}: {uploaded_file.name}")
        try:
            # Read image
            image_pil = Image.open(uploaded_file)

            # Compress if large (adjust max size as needed)
            uploaded_file.seek(0) # Reset file pointer
            file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
            if file_size_mb > 1.5: # Compress images > 1.5 MB
                print(f"Compressing large image: {uploaded_file.name} ({file_size_mb:.2f} MB)")
                image_pil = compress_image(image_pil, max_size_kb=800, max_dimension=1920) # Target 800KB, max 1920px

            # Store original (potentially compressed) PIL image
            st.session_state.batch_images.append(image_pil)

            # --- Object Detection (YOLO) ---
            try:
                results = model.predict(image_pil, verbose=False) # verbose=False for cleaner logs
            except Exception as e:
                 print(f"YOLOé¢„æµ‹å¤±è´¥ for {uploaded_file.name}: {e}")
                 results = [None] # Handle prediction failure gracefully

            # --- Face Detection (MTCNN) ---
            detected_faces_list = [] # Faces for this specific image
            image_for_face_detection = image_pil # Use the (potentially compressed) PIL image
            if detect_faces and face_detector:
                try:
                    _, detected_faces_list = detect_face(image_for_face_detection, face_detector, face_confidence)
                except Exception as e:
                     print(f"äººè„¸æ£€æµ‹å¤±è´¥ for {uploaded_file.name}: {e}")
                     # detected_faces_list remains empty

            st.session_state.batch_faces.append(detected_faces_list) # Append list of faces for this image

            # --- Process and Draw Detections ---
            processed_img_array = np.array(image_pil.convert("RGB")) # Start with current image
            if results[0] is not None: # Check if YOLO prediction succeeded
                processed_img_array = process_prediction(image_pil, results, selected_classes, confidence)
            # Add face boxes onto the image that already has object boxes
            if detect_faces and face_detector and detected_faces_list: # Draw if faces were detected
                # Re-run detection on the original array to get boxes again (or store boxes from detect_face)
                img_rgb_for_drawing = np.array(image_for_face_detection.convert("RGB"))
                faces_data = face_detector.detect_faces(img_rgb_for_drawing)
                filtered_faces_data = [f for f in faces_data if f['confidence'] >= face_confidence]

                # Draw boxes on the 'processed_img_array' which has object detections
                img_bgr_for_drawing = cv2.cvtColor(processed_img_array, cv2.COLOR_RGB2BGR)
                hex_color = face_box_color.lstrip('#')
                face_bgr_color = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))
                for face in filtered_faces_data:
                     x, y, w, h = face['box']
                     x1, y1, x2, y2 = max(0, x), max(0, y), min(img_bgr_for_drawing.shape[1], x+w), min(img_bgr_for_drawing.shape[0], y+h)
                     cv2.rectangle(img_bgr_for_drawing, (x1, y1), (x2, y2), face_bgr_color, 2)
                     # Add label if needed
                     # label = f"Face: {face['confidence']:.2f}"
                     # cv2.putText(img_bgr_for_drawing, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, face_bgr_color, 2)
                processed_img_array = cv2.cvtColor(img_bgr_for_drawing, cv2.COLOR_BGR2RGB)


            st.session_state.batch_processed_images.append(processed_img_array) # Store final marked image array

            # --- Feature Extraction ---
            image_id = f"batch_image_{i}" # Unique ID for this batch image

            # Image Features (ResNet50)
            if enable_similarity_search and feature_extractor and transform:
                img_features = extract_image_features(image_pil, feature_extractor, transform)
                if img_features is not None:
                    st.session_state.batch_image_vectors[image_id] = {
                        'vector': img_features,
                        'index': i
                    }

            # Face Features (DeepFace)
            if enable_face_similarity and detected_faces_list:
                for j, face_array in enumerate(detected_faces_list):
                    face_features = extract_face_features(face_array) # Pass numpy array
                    if face_features is not None:
                        face_id = f"batch_face_{i}_{j}"
                        st.session_state.batch_face_vectors[face_id] = {
                            'vector': face_features,
                            'image_index': i,
                            'face_index': j
                        }

            processed_count += 1

        except Exception as e:
            st.error(f"å¤„ç†å›¾ç‰‡ {uploaded_file.name} æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯: {str(e)}")
            failed_files.append(uploaded_file.name)
            # Add placeholders for failed images to keep lists aligned?
            st.session_state.batch_images.append(None) # Or a placeholder image
            st.session_state.batch_faces.append([])
            st.session_state.batch_processed_images.append(None) # Or a placeholder

        # Update progress bar
        prog_bar.progress((i + 1) / len(uploaded_files))

    status_text.text(f"æ‰¹é‡å¤„ç†å®Œæˆã€‚æˆåŠŸ: {processed_count}, å¤±è´¥: {len(failed_files)}.")
    if failed_files:
        st.warning(f"ä»¥ä¸‹æ–‡ä»¶å¤„ç†å¤±è´¥: {', '.join(failed_files)}")

    # --- Compute Similarities (after processing all images) ---
    with st.spinner("è®¡ç®—ç›¸ä¼¼åº¦..."):
        if enable_similarity_search and len(st.session_state.batch_image_vectors) > 1:
            compute_batch_image_similarity()

        if enable_face_similarity and len(st.session_state.batch_face_vectors) > 1:
            compute_batch_face_similarity()

    return processed_count, failed_files


# æ·»åŠ è®¡ç®—æ‰¹é‡å›¾ç‰‡ç›¸ä¼¼åº¦çš„å‡½æ•°
def compute_batch_image_similarity():
    """è®¡ç®—æ‰¹é‡ä¸Šä¼ å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦"""
    similarity_results = []
    image_vectors_dict = st.session_state.batch_image_vectors

    image_ids = list(image_vectors_dict.keys())
    vectors = [image_vectors_dict[id]['vector'] for id in image_ids]
    indices = [image_vectors_dict[id]['index'] for id in image_ids]

    if not vectors: return # No vectors to compare

    # Calculate cosine similarity matrix
    try:
        # Ensure vectors are valid numpy arrays
        valid_vectors = [v for v in vectors if isinstance(v, np.ndarray)]
        if len(valid_vectors) < 2: return

        vectors_array = np.array(valid_vectors)
        # Normalize vectors to avoid issues with zero vectors if any slipped through
        norms = np.linalg.norm(vectors_array, axis=1, keepdims=True)
        # Prevent division by zero
        norms[norms == 0] = 1e-6
        normalized_vectors = vectors_array / norms

        # Compute similarity matrix
        similarity_matrix = cosine_similarity(normalized_vectors)

        # Extract pairs above threshold
        num_images = len(image_ids)
        for i in range(num_images):
            for j in range(i + 1, num_images):
                similarity = similarity_matrix[i, j]
                if similarity >= similarity_threshold:
                    similarity_results.append({
                        'image1_id': image_ids[i],
                        'image2_id': image_ids[j],
                        'image1_index': indices[i],
                        'image2_index': indices[j],
                        'similarity': float(similarity)
                    })

    except Exception as e:
         st.error(f"è®¡ç®—æ‰¹é‡å›¾ç‰‡ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
         st.session_state.batch_similarity_results = []
         return


    # Sort by similarity
    similarity_results.sort(key=lambda x: x['similarity'], reverse=True)
    st.session_state.batch_similarity_results = similarity_results


# æ·»åŠ è®¡ç®—æ‰¹é‡äººè„¸ç›¸ä¼¼åº¦çš„å‡½æ•°
def compute_batch_face_similarity():
    """è®¡ç®—æ‰¹é‡ä¸Šä¼ å›¾ç‰‡ä¸­äººè„¸é—´çš„ç›¸ä¼¼åº¦"""
    face_similarity_results = []
    face_vectors_dict = st.session_state.batch_face_vectors

    face_ids = list(face_vectors_dict.keys())
    vectors = [face_vectors_dict[id]['vector'] for id in face_ids]
    image_indices = [face_vectors_dict[id]['image_index'] for id in face_ids]
    face_indices = [face_vectors_dict[id]['face_index'] for id in face_ids]

    if len(vectors) < 2: return # Need at least two faces

    # Calculate cosine similarity matrix for faces
    try:
         # Ensure vectors are valid numpy arrays
        valid_vectors = [v for v in vectors if isinstance(v, np.ndarray)]
        if len(valid_vectors) < 2: return

        vectors_array = np.array(valid_vectors)
        # Normalize vectors
        norms = np.linalg.norm(vectors_array, axis=1, keepdims=True)
        norms[norms == 0] = 1e-6
        normalized_vectors = vectors_array / norms

        # Compute similarity matrix
        similarity_matrix = cosine_similarity(normalized_vectors)

        # Extract pairs above threshold
        num_faces = len(face_ids)
        for i in range(num_faces):
            for j in range(i + 1, num_faces):
                 # Avoid comparing faces from the exact same original image? Optional.
                 # if image_indices[i] == image_indices[j]: continue

                 similarity = similarity_matrix[i, j]
                 if similarity >= similarity_threshold: # Use general similarity threshold for now
                     face_similarity_results.append({
                         'face1_id': face_ids[i],
                         'face2_id': face_ids[j],
                         'image1_index': image_indices[i],
                         'image2_index': image_indices[j],
                         'face1_index': face_indices[i],
                         'face2_index': face_indices[j],
                         'similarity': float(similarity)
                     })

    except Exception as e:
         st.error(f"è®¡ç®—æ‰¹é‡äººè„¸ç›¸ä¼¼åº¦æ—¶å‡ºé”™: {e}")
         st.session_state.batch_face_similarity_results = []
         return


    # Sort by similarity
    face_similarity_results.sort(key=lambda x: x['similarity'], reverse=True)
    st.session_state.batch_face_similarity_results = face_similarity_results

# æ·»åŠ æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ç›¸ä¼¼ç»“æœçš„å‡½æ•°
def display_batch_similarity_results():
    """æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ç»“æœ"""
    results = st.session_state.get('batch_similarity_results', [])
    images = st.session_state.get('batch_images', [])

    if not results:
        st.info("åœ¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡ä¸­æœªæ‰¾åˆ°ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜çš„å›¾ç‰‡å¯¹ã€‚")
        return

    st.subheader(f"æ‰¹é‡å†…å›¾ç‰‡ç›¸ä¼¼åº¦ç»“æœ (Top {min(10, len(results))}, ç›¸ä¼¼åº¦ > {similarity_threshold:.2f})")

    # Display top N results
    for i, result in enumerate(results[:10]):
        cols = st.columns([1, 0.5, 1]) # Image | Similarity | Image

        img1_index = result.get('image1_index')
        img2_index = result.get('image2_index')
        similarity = result.get('similarity', 0)

        # Display first image
        with cols[0]:
            if img1_index is not None and img1_index < len(images) and images[img1_index] is not None:
                st.image(images[img1_index], caption=f"å›¾ç‰‡ #{img1_index+1}", use_container_width=True)
            else:
                st.warning(f"å›¾ç‰‡ #{img1_index+1} æ— æ³•æ˜¾ç¤º")

        # Display similarity
        with cols[1]:
            st.markdown(f"<div style='text-align: center; margin-top: 50px;'><h3>ç›¸ä¼¼åº¦<br>{similarity:.3f}</h3></div>", unsafe_allow_html=True)


        # Display second image
        with cols[2]:
            if img2_index is not None and img2_index < len(images) and images[img2_index] is not None:
                st.image(images[img2_index], caption=f"å›¾ç‰‡ #{img2_index+1}", use_container_width=True)
            else:
                 st.warning(f"å›¾ç‰‡ #{img2_index+1} æ— æ³•æ˜¾ç¤º")

        st.markdown("---") # Separator


# æ·»åŠ æ˜¾ç¤ºæ‰¹é‡äººè„¸ç›¸ä¼¼ç»“æœçš„å‡½æ•°
def display_batch_face_similarity_results():
    """æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ä¸­äººè„¸ä¹‹é—´çš„ç›¸ä¼¼åº¦ç»“æœ"""
    results = st.session_state.get('batch_face_similarity_results', [])
    all_batch_faces = st.session_state.get('batch_faces', []) # List of lists

    if not results:
        st.info("åœ¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡ä¸­æœªæ‰¾åˆ°ç›¸ä¼¼åº¦è¶³å¤Ÿé«˜çš„äººè„¸å¯¹ã€‚")
        return

    st.subheader(f"æ‰¹é‡å†…äººè„¸ç›¸ä¼¼åº¦ç»“æœ (Top {min(10, len(results))}, ç›¸ä¼¼åº¦ > {similarity_threshold:.2f})")

    # Display top N results
    for i, result in enumerate(results[:10]):
        cols = st.columns([1, 0.5, 1]) # Face | Similarity | Face

        img1_idx = result.get('image1_index')
        face1_idx = result.get('face1_index')
        img2_idx = result.get('image2_index')
        face2_idx = result.get('face2_index')
        similarity = result.get('similarity', 0)

        # Display first face
        with cols[0]:
            face1_array = None
            caption1 = f"å›¾ç‰‡ #{img1_idx+1}, äººè„¸ #{face1_idx+1}"
            try:
                if img1_idx < len(all_batch_faces) and face1_idx < len(all_batch_faces[img1_idx]):
                    face1_array = all_batch_faces[img1_idx][face1_idx]
                    if isinstance(face1_array, np.ndarray) and face1_array.size > 0:
                        st.image(face1_array, caption=caption1, use_container_width=True)
                    else: raise ValueError("æ— æ•ˆçš„äººè„¸æ•°æ®")
                else: raise IndexError("ç´¢å¼•è¶Šç•Œ")
            except Exception as e:
                st.warning(f"äººè„¸æ— æ³•æ˜¾ç¤º ({caption1}): {e}")


        # Display similarity
        with cols[1]:
             st.markdown(f"<div style='text-align: center; margin-top: 50px;'><h3>ç›¸ä¼¼åº¦<br>{similarity:.3f}</h3></div>", unsafe_allow_html=True)


        # Display second face
        with cols[2]:
            face2_array = None
            caption2 = f"å›¾ç‰‡ #{img2_idx+1}, äººè„¸ #{face2_idx+1}"
            try:
                if img2_idx < len(all_batch_faces) and face2_idx < len(all_batch_faces[img2_idx]):
                     face2_array = all_batch_faces[img2_idx][face2_idx]
                     if isinstance(face2_array, np.ndarray) and face2_array.size > 0:
                         st.image(face2_array, caption=caption2, use_container_width=True)
                     else: raise ValueError("æ— æ•ˆçš„äººè„¸æ•°æ®")
                else: raise IndexError("ç´¢å¼•è¶Šç•Œ")
            except Exception as e:
                st.warning(f"äººè„¸æ— æ³•æ˜¾ç¤º ({caption2}): {e}")

        st.markdown("---") # Separator


# ... existing code ...
def summarize_batch_images(similarity_threshold=0.5):
    """
    æ ¹æ®æ‰¹é‡ä¸Šä¼ å›¾ç‰‡çš„ç›¸ä¼¼æ€§æ€»ç»“åœºæ™¯æ•°é‡ (using image embeddings)
    """
    try:
        if 'batch_images' not in st.session_state or not st.session_state.batch_images:
            return "æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„å›¾ç‰‡"

        # Get image feature vectors from batch processing results
        image_vectors_dict = st.session_state.get('batch_image_vectors', {})
        if not image_vectors_dict:
            return "æœªèƒ½æå–å›¾ç‰‡ç‰¹å¾ï¼Œæ— æ³•è¿›è¡Œåœºæ™¯æ€»ç»“ã€‚"

        image_features = [data['vector'] for data in image_vectors_dict.values() if data.get('vector') is not None]

        if len(image_features) < 2:
            return f"ä»…æœ‰ {len(image_features)} å¼ å›¾ç‰‡å…·æœ‰æœ‰æ•ˆç‰¹å¾ï¼Œæ— æ³•è¿›è¡Œåœºæ™¯èšç±»ã€‚"

        # Use Agglomerative Clustering based on Cosine Distance
        features_array = np.array(image_features)

        # Calculate Cosine Distance matrix (1 - similarity)
        # Handle potential normalization issues
        norms = np.linalg.norm(features_array, axis=1, keepdims=True)
        norms[norms == 0] = 1e-6 # Avoid division by zero
        normalized_features = features_array / norms
        similarity_matrix = cosine_similarity(normalized_features)
        # Ensure distance is non-negative and handle precision issues
        distance_matrix = np.clip(1.0 - similarity_matrix, 0.0, 2.0)


        # Perform Agglomerative Clustering
        # distance_threshold is based on the distance metric (cosine distance here)
        # A threshold of (1 - similarity_threshold) groups items with similarity >= similarity_threshold
        clustering = AgglomerativeClustering(
            n_clusters=None,
            distance_threshold= (1.0 - similarity_threshold),
            metric='precomputed', # Use the precomputed distance matrix
            linkage='average' # Or 'complete', 'single'
        )
        clusters = clustering.fit_predict(distance_matrix)

        # Calculate the number of distinct scenes (clusters)
        unique_clusters = len(set(clusters))
        # DBSCAN might produce -1 for noise, Agglomerative usually doesn't unless distance > threshold for all
        # unique_clusters = len(set(clusters)) - (1 if -1 in clusters else 0) # More relevant for DBSCAN

        return f"åˆ†æç»“æœï¼šåŸºäºå›¾ç‰‡å†…å®¹ç›¸ä¼¼åº¦ï¼ˆé˜ˆå€¼={similarity_threshold:.2f}ï¼‰ï¼Œä¸Šä¼ çš„ {len(image_features)} å¼ æœ‰æ•ˆå›¾ç‰‡ä¸­ï¼Œå¤§çº¦åŒ…å« **{unique_clusters}** ä¸ªä¸åŒåœºæ™¯ã€‚"

    except Exception as e:
        st.error(f"åœºæ™¯æ€»ç»“å‡ºé”™: {str(e)}")
        # import traceback
        # print(traceback.format_exc()) # Log traceback for debugging
        return f"åœºæ™¯æ€»ç»“å‡ºé”™: {str(e)}"

def summarize_batch_faces(face_dist_tolerance=0.4):
    """
    æ ¹æ®æ‰¹é‡ä¸Šä¼ å›¾ç‰‡ä¸­çš„äººè„¸æ€»ç»“ä¸åŒäººç‰©æ•°é‡ (using face clustering results)
    """
    try:
        # Check if face processing happened and if there are faces
        if 'batch_faces' not in st.session_state:
            return "äººè„¸æ•°æ®æœªå¤„ç†ã€‚"

        all_faces_list = [face for sublist in st.session_state.batch_faces if sublist for face in sublist]
        total_detected_faces = len(all_faces_list)

        if total_detected_faces == 0:
            return "æœªåœ¨ä¸Šä¼ çš„å›¾ç‰‡ä¸­æ£€æµ‹åˆ°äººè„¸ã€‚"

        # Create temporary directories for clustering input and output
        temp_base = Path(tempfile.gettempdir()) / f"streamlit_face_cluster_{uuid.uuid4().hex[:8]}"
        temp_input_dir = temp_base / "input"
        temp_output_dir = temp_base / "output"

        # Cleanup previous temp dirs if they exist (robustness)
        if temp_base.exists(): shutil.rmtree(temp_base)
        temp_input_dir.mkdir(parents=True, exist_ok=True)

        # Save all detected faces (numpy arrays) to the temp input directory
        face_count_saved = 0
        for i, faces_in_image in enumerate(st.session_state.batch_faces):
            if isinstance(faces_in_image, list):
                for j, face_np in enumerate(faces_in_image):
                    if isinstance(face_np, np.ndarray) and face_np.size > 0:
                        try:
                            face_img = Image.fromarray(face_np).convert("RGB") # Ensure RGB
                            face_path = temp_input_dir / f"face_{i}_{j}.jpg"
                            face_img.save(face_path, format="JPEG")
                            face_count_saved += 1
                        except Exception as e:
                            print(f"Error saving temp face image face_{i}_{j}: {e}")


        if face_count_saved == 0:
            # Cleanup if no faces were saved
            if temp_base.exists(): shutil.rmtree(temp_base)
            return "æ— æ³•ä¿å­˜æ£€æµ‹åˆ°çš„äººè„¸å›¾åƒè¿›è¡Œèšç±»ã€‚"

        print(f"å‡†å¤‡å¯¹ {face_count_saved} ä¸ªä¿å­˜çš„äººè„¸è¿›è¡Œèšç±»...")

        # Run the face clustering function (which now uses MTCNN+DeepFace)
        # Use the tolerance provided (cosine distance)
        # Set min_cluster_size=2 to identify pairs as clusters, or 1 to count every face initially
        effective_min_cluster_size = 2
        face_clustering(str(temp_input_dir), str(temp_output_dir),
                        tolerance=face_dist_tolerance,
                        min_cluster_size=effective_min_cluster_size)

        # Count the number of resulting clusters (people)
        cluster_dirs = [d for d in temp_output_dir.iterdir()
                       if d.is_dir() and d.name.startswith("person_")]
        num_clusters = len(cluster_dirs)

        # Count unclustered faces (optional info)
        unclustered_dir = temp_output_dir / "unclustered_faces"
        num_unclustered = 0
        if unclustered_dir.exists():
            num_unclustered = len([f for f in unclustered_dir.iterdir() if f.is_file()])


        # Prepare summary message
        summary = f"åˆ†æç»“æœï¼šåŸºäºäººè„¸ç›¸ä¼¼åº¦ï¼ˆè·ç¦»é˜ˆå€¼={face_dist_tolerance:.2f}ï¼‰ï¼Œä¸Šä¼ å›¾ç‰‡ä¸­å…±æ£€æµ‹åˆ° **{total_detected_faces}** ä¸ªæœ‰æ•ˆäººè„¸ï¼Œè¯†åˆ«å‡ºå¤§çº¦ **{num_clusters}** ä¸ªä¸åŒçš„äººç‰©èšç±»ã€‚"
        if num_unclustered > 0:
            summary += f" å¦æœ‰ {num_unclustered} ä¸ªäººè„¸æœªå½’å…¥ä»»ä½•èšç±»ï¼ˆå™ªå£°ç‚¹ï¼‰ã€‚"

        # Return summary and the path to the output directory for montage display
        return summary, str(temp_output_dir)

    except Exception as e:
        st.error(f"äººè„¸æ€»ç»“åŠèšç±»å‡ºé”™: {str(e)}")
        # import traceback
        # print(traceback.format_exc()) # Log traceback for debugging
        # Cleanup temp dir on error
        if 'temp_base' in locals() and temp_base.exists(): shutil.rmtree(temp_base)
        return f"äººè„¸æ€»ç»“å‡ºé”™: {str(e)}", None


def display_batch_analysis_summary():
    """
    Displays the summary of batch image analysis (scenes and faces).
    """
    if 'batch_images' not in st.session_state or not st.session_state.batch_images:
        st.info("è¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ‰¹é‡å›¾ç‰‡ã€‚")
        return

    st.subheader("æ‰¹é‡åˆ†ææ€»ç»“")

    # Sliders for tuning summarization thresholds
    col1, col2 = st.columns(2)
    with col1:
        scene_similarity_threshold = st.slider("åœºæ™¯èšç±»ç›¸ä¼¼åº¦é˜ˆå€¼", 0.1, 0.9, 0.6, 0.05, # Higher threshold = more scenes
                                            help="å€¼è¶Šé«˜ï¼Œéœ€è¦å›¾ç‰‡è¶Šç›¸ä¼¼æ‰ä¼šè¢«å½’ä¸ºåŒä¸€åœºæ™¯ï¼Œä»è€Œå¯èƒ½è¯†åˆ«å‡ºæ›´å¤šåœºæ™¯ã€‚")
    with col2:
        face_distance_threshold = st.slider("äººè„¸èšç±»è·ç¦»é˜ˆå€¼", 0.1, 1.0, 0.4, 0.05, # Lower threshold = more people
                                            help="å€¼è¶Šä½ï¼ŒåŒ¹é…è¶Šä¸¥æ ¼ï¼ˆéœ€è¦äººè„¸ç‰¹å¾è·ç¦»è¶Šå°ï¼‰ï¼Œä»è€Œå¯èƒ½è¯†åˆ«å‡ºæ›´å¤šä¸åŒçš„äººã€‚ä½¿ç”¨Cosineè·ç¦»ã€‚")

    # --- Scene Summary ---
    with st.spinner("æ€»ç»“åœºæ™¯æ•°é‡..."):
        scene_summary = summarize_batch_images(similarity_threshold=scene_similarity_threshold)
        st.info(scene_summary)

    # --- Face Summary and Montage Display ---
    temp_output_dir_path = None # To store path for montage creation
    with st.spinner("æ€»ç»“äººç‰©æ•°é‡å¹¶è¿›è¡Œèšç±»..."):
        face_summary, temp_output_dir_path = summarize_batch_faces(face_dist_tolerance=face_distance_threshold)
        st.info(face_summary)

    # Display face cluster montages if clustering was successful
    if temp_output_dir_path and os.path.exists(temp_output_dir_path):
        st.subheader("äººè„¸èšç±»ç»“æœå¯è§†åŒ–")
        with st.spinner("ç”Ÿæˆå¹¶åŠ è½½äººè„¸èšç±»è’™å¤ªå¥‡å›¾..."):
            try:
                # Generate montages from the temp output directory
                montages_info = create_montages(temp_output_dir_path, montage_size=(100, 100), images_per_row=6) # Smaller size for batch

                if montages_info:
                    montage_cols = st.columns(min(len(montages_info), 3)) # Display up to 3 montages side-by-side
                    for i, montage in enumerate(montages_info):
                        with montage_cols[i % 3]:
                            try:
                                st.image(montage["path"],
                                        caption=f"{montage['cluster']} ({montage['images_count']} faces)",
                                        use_container_width=True)
                            except Exception as img_e:
                                st.error(f"æ— æ³•æ˜¾ç¤ºè’™å¤ªå¥‡å›¾ {montage['cluster']}: {img_e}")
                elif os.path.exists(temp_output_dir_path): # Check if clustering ran but produced no montage-able clusters
                     # Check for unclustered or no-face images
                     unclustered_dir = Path(temp_output_dir_path) / "unclustered_faces"
                     no_faces_dir = Path(temp_output_dir_path) / "no_faces_detected"
                     msg = "æœªç”Ÿæˆäººè„¸èšç±»è’™å¤ªå¥‡å›¾ã€‚"
                     if unclustered_dir.exists() and any(unclustered_dir.iterdir()):
                         msg += " (å¯èƒ½æ‰€æœ‰èšç±»çš„äººæ•°éƒ½å°‘äºé˜ˆå€¼ï¼Œæˆ–äººè„¸æœªå½¢æˆèšç±»)"
                     elif no_faces_dir.exists() and any(no_faces_dir.iterdir()):
                         msg += " (éƒ¨åˆ†å›¾ç‰‡æœªæ£€æµ‹åˆ°äººè„¸)"
                     st.info(msg)


            except Exception as e:
                st.error(f"æ˜¾ç¤ºäººè„¸èšç±»è’™å¤ªå¥‡å›¾æ—¶å‡ºé”™: {str(e)}")
                # import traceback
                # print(traceback.format_exc())

        # --- Cleanup Temporary Directory ---
        # Perform cleanup AFTER potentially displaying montages
        try:
            temp_base_path = Path(temp_output_dir_path).parent # Get the base temp folder path
            if temp_base_path.exists() and temp_base_path.name.startswith("streamlit_face_cluster_"):
                print(f"Cleaning up temporary clustering directory: {temp_base_path}")
                shutil.rmtree(temp_base_path)
        except Exception as e:
            st.warning(f"æ¸…ç†ä¸´æ—¶èšç±»ç›®å½•æ—¶å‡ºé”™: {e}")
            # Don't let cleanup errors stop the app
    elif face_summary and "å‡ºé”™" not in face_summary: # Check if face summary indicated an error earlier
         st.info("äººè„¸èšç±»æœªäº§ç”Ÿæœ‰æ•ˆè¾“å‡ºç›®å½•ï¼Œæ— æ³•æ˜¾ç¤ºè’™å¤ªå¥‡å›¾ã€‚")


# ... existing code ...

# Main program logic wrapped in try-except
try:
    # Get selected model name and load it
    selected_model = model_mapping[model_option]
    model = load_model(selected_model) # Handles model loading/downloading

    # Load face detector (MTCNN) if enabled
    face_detector = None
    if detect_faces:
        face_detector = load_face_detector() # Handles MTCNN loading

    # Load feature extractor models if needed
    feature_extractor = None
    transform = None
    if enable_similarity_search or enable_face_similarity:
        feature_extractor = load_feature_extractor() # Loads ResNet50
        transform = get_transform()
        # DeepFace models are loaded on demand by DeepFace functions

    st.info(f"å½“å‰ä½¿ç”¨æ¨¡å‹: {model_option}")

    # --- Batch Processing Mode ---
    if st.session_state.batch_mode:
        st.header("æ‰¹é‡å¤„ç†æ¨¡å¼")
        uploaded_files = st.file_uploader("ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡å¤„ç†", type=["jpg", "jpeg", "png", "bmp", "tiff"], accept_multiple_files=True)

        if uploaded_files:
            st.write(f"å·²é€‰æ‹© {len(uploaded_files)} å¼ å›¾ç‰‡")

            if st.button(f"å¼€å§‹å¤„ç† {len(uploaded_files)} å¼ å›¾ç‰‡"):
                # Execute batch processing
                processed_count, failed_files = process_batch_images(uploaded_files)
                st.success(f"æ‰¹é‡å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {processed_count} å¼ å›¾ç‰‡ã€‚")
                if failed_files:
                     st.warning(f"å¤„ç†å¤±è´¥çš„æ–‡ä»¶: {', '.join(failed_files)}")

                # --- Display Batch Results ---
                if processed_count > 0:
                    st.subheader("æ‰¹é‡å¤„ç†ç»“æœæ¦‚è§ˆ")

                    # Create a summary DataFrame
                    batch_summary_data = []
                    valid_images = [img for img in st.session_state.batch_images if img is not None]
                    valid_processed = [img for img in st.session_state.batch_processed_images if img is not None]
                    valid_faces = [f for f in st.session_state.batch_faces if f is not None] # List of lists or Nones

                    for i in range(len(valid_images)): # Iterate based on successful image loads
                         image = valid_images[i]
                         face_list = valid_faces[i] if i < len(valid_faces) else []
                         face_count = len(face_list) if isinstance(face_list, list) else 0

                         batch_summary_data.append({
                             "å›¾ç‰‡åºå·": i + 1,
                             "åŸå§‹å°ºå¯¸": f"{image.width}x{image.height}",
                             "æ£€æµ‹åˆ°çš„äººè„¸æ•°": face_count
                         })
                    if batch_summary_data:
                        st.dataframe(batch_summary_data)
                    else:
                        st.info("æ²¡æœ‰æˆåŠŸå¤„ç†çš„å›¾ç‰‡å¯ä¾›æ¦‚è§ˆã€‚")


                    # Tabs for detailed results
                    tab_titles = ["å¤„ç†åå›¾ç‰‡", "æ£€æµ‹åˆ°äººè„¸"]
                    if enable_similarity_search: tab_titles.append("å›¾ç‰‡ç›¸ä¼¼åº¦")
                    if enable_face_similarity: tab_titles.append("äººè„¸ç›¸ä¼¼åº¦")
                    tab_titles.append("æ‰¹é‡åˆ†ææ€»ç»“")

                    tabs = st.tabs(tab_titles)
                    tab_idx = 0

                    # Tab 1: Processed Images
                    with tabs[tab_idx]:
                        st.subheader("å¤„ç†åçš„å›¾ç‰‡")
                        cols = st.columns(3)
                        img_display_count = 0
                        for i, processed_img_array in enumerate(valid_processed):
                             if processed_img_array is not None:
                                 with cols[img_display_count % 3]:
                                     st.image(processed_img_array, caption=f"å›¾ç‰‡ #{i+1}", use_container_width=True)
                                     img_display_count += 1
                        if img_display_count == 0: st.info("æ— å¤„ç†åçš„å›¾ç‰‡å¯æ˜¾ç¤ºã€‚")
                    tab_idx += 1

                    # Tab 2: Detected Faces
                    with tabs[tab_idx]:
                        st.subheader("æ£€æµ‹åˆ°çš„äººè„¸")
                        face_display_count = 0
                        for i, faces_list in enumerate(valid_faces):
                             if isinstance(faces_list, list) and faces_list:
                                 st.write(f"--- å›¾ç‰‡ #{i+1} ({len(faces_list)} äººè„¸) ---")
                                 cols = st.columns(min(len(faces_list), 5)) # Show up to 5 faces per row
                                 for j, face_array in enumerate(faces_list):
                                     if isinstance(face_array, np.ndarray) and face_array.size > 0:
                                          with cols[j % 5]:
                                             st.image(face_array, caption=f"äººè„¸ #{j+1}", use_container_width=True)
                                             face_display_count +=1
                        if face_display_count == 0: st.info("æœªæ£€æµ‹åˆ°ä»»ä½•äººè„¸ã€‚")
                    tab_idx += 1


                    # Tab 3: Image Similarity (Conditional)
                    if enable_similarity_search:
                         with tabs[tab_idx]:
                             display_batch_similarity_results()
                         tab_idx += 1

                    # Tab 4: Face Similarity (Conditional)
                    if enable_face_similarity:
                         with tabs[tab_idx]:
                             display_batch_face_similarity_results()
                         tab_idx += 1

                    # Tab 5: Batch Analysis Summary
                    with tabs[tab_idx]:
                        display_batch_analysis_summary()
                    tab_idx += 1

                else: # If processed_count is 0
                    st.warning("æœªèƒ½æˆåŠŸå¤„ç†ä»»ä½•å›¾ç‰‡ï¼Œæ— æ³•æ˜¾ç¤ºç»“æœã€‚")

    # --- Single Image Processing Mode ---
    else:
        st.header("å•å¼ å›¾ç‰‡å¤„ç†æ¨¡å¼")
        # Use on_change to clear state BEFORE processing the new file
        uploaded_file = st.file_uploader("ä¸Šä¼ å•å¼ å›¾ç‰‡", type=["jpg", "jpeg", "png", "bmp", "tiff"], on_change=reset_processed_state)

        if uploaded_file is not None:
            current_upload_id = f"{uploaded_file.name}-{uploaded_file.size}" # More robust ID

            # Check if it's a new file or if processing hasn't happened yet
            if current_upload_id != st.session_state.get('last_upload_id') or not st.session_state.get('processed'):
                st.session_state.processed = False # Ensure reset before processing starts
                st.session_state.image = None
                st.session_state.processed_img = None
                st.session_state.detected_faces = []
                st.session_state.similar_images = []
                st.session_state.similar_faces_results = []
                st.session_state.face_best_matches = []
                st.session_state.similar_texts = []
                st.session_state.advanced_face_search_done = False


                processing_placeholder = st.empty() # Placeholder for status messages/spinners
                col1, col2 = st.columns(2) # Setup columns for display later

                with processing_placeholder:
                    with st.spinner("è¯»å–å’Œé¢„å¤„ç†å›¾ç‰‡..."):
                        try:
                            # Read image
                            image = Image.open(uploaded_file)

                            # Get original size and file size
                            original_width, original_height = image.size
                            uploaded_file.seek(0)
                            file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)

                            # Compress if large
                            if file_size_mb > 1.5:
                                st.warning(f"å›¾ç‰‡è¾ƒå¤§ ({file_size_mb:.2f}MB)ï¼Œå°†è¿›è¡Œå‹ç¼©...")
                                image = compress_image(image, max_size_kb=800, max_dimension=1920)
                                compressed_size_kb = len(image.tobytes()) / 1024 # Approximation after compression
                                st.success(f"å‹ç¼©å®Œæˆï¼ æ–°å°ºå¯¸: {image.width}x{image.height} (~{compressed_size_kb:.1f}KB)")

                            st.session_state.image = image # Store potentially compressed image
                            st.session_state.last_upload_id = current_upload_id

                            with col1:
                                st.subheader("åŸå§‹å›¾ç‰‡ (å¤„ç†å)")
                                st.image(image, use_container_width=True)

                        except Exception as e:
                            st.error(f"è¯»å–æˆ–å‹ç¼©å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                            st.stop() # Stop execution if image loading fails

                # --- Object Detection ---
                with processing_placeholder, st.spinner(f"ç‰©ä½“æ£€æµ‹ ({model_option})..."):
                    try:
                        results = model.predict(st.session_state.image, verbose=False)
                        st.session_state.detection_results = results
                        # Get selected classes
                        selected_classes = []
                        if detect_person: selected_classes.append("äºº")
                        if detect_cup: selected_classes.append("æ¯å­/é…’æ¯")
                        if detect_bottle: selected_classes.append("ç“¶å­")
                        if detect_all: selected_classes.append("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“")

                    except Exception as e:
                         st.error(f"ç‰©ä½“æ£€æµ‹å¤±è´¥: {e}")
                         results = [None] # Set results to None on failure
                         st.session_state.detection_results = results


                # --- Face Detection ---
                detected_faces = [] # List of numpy arrays for faces
                if detect_faces and face_detector:
                     with processing_placeholder, st.spinner("äººè„¸æ£€æµ‹ (MTCNN)..."):
                         try:
                             # Use the potentially compressed image for detection
                             _, detected_faces = detect_face(st.session_state.image, face_detector, face_confidence)
                             st.session_state.detected_faces = detected_faces # Store numpy arrays
                             print(f"æ£€æµ‹åˆ° {len(detected_faces)} ä¸ªäººè„¸ (MTCNN, conf>{face_confidence})")
                         except Exception as e:
                             st.error(f"äººè„¸æ£€æµ‹å¤±è´¥: {e}")


                # --- Draw Detections and Faces ---
                with processing_placeholder, st.spinner("ç»˜åˆ¶æ£€æµ‹ç»“æœ..."):
                    # Start with the base image
                    processed_img_base = st.session_state.image.copy().convert("RGB")
                    processed_img_array = np.array(processed_img_base)

                    # Draw object boxes if detection was successful
                    if st.session_state.detection_results and st.session_state.detection_results[0] is not None:
                         processed_img_array = process_prediction(processed_img_base, st.session_state.detection_results, selected_classes, confidence)

                    # Draw face boxes on top of the image (which might already have object boxes)
                    if detected_faces: # Draw if faces were found
                        # Re-detect face bounding boxes on the original image array for drawing accuracy
                        img_rgb_for_drawing = np.array(st.session_state.image.convert("RGB"))
                        faces_data_for_drawing = face_detector.detect_faces(img_rgb_for_drawing)
                        filtered_faces_data = [f for f in faces_data_for_drawing if f['confidence'] >= face_confidence]

                        # Convert current processed array (with object boxes) to BGR for cv2 drawing
                        img_bgr_for_drawing = cv2.cvtColor(processed_img_array, cv2.COLOR_RGB2BGR)
                        try:
                            hex_color = face_box_color.lstrip('#')
                            face_bgr_color = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))
                        except ValueError: face_bgr_color = (0, 255, 0)

                        for face in filtered_faces_data:
                            x, y, w, h = face['box']
                            x1, y1 = max(0, x), max(0, y)
                            x2, y2 = min(img_bgr_for_drawing.shape[1], x + w), min(img_bgr_for_drawing.shape[0], y + h)
                            cv2.rectangle(img_bgr_for_drawing, (x1, y1), (x2, y2), face_bgr_color, 2)
                            # Add label if needed
                            # label = f"Face: {face['confidence']:.2f}"
                            # cv2.putText(img_bgr_for_drawing, label, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255), 1)

                        # Convert back to RGB
                        processed_img_array = cv2.cvtColor(img_bgr_for_drawing, cv2.COLOR_BGR2RGB)

                    # Store the final image with all markings
                    st.session_state.processed_img = processed_img_array


                # --- Update DB and Similarity Search ---
                image_id = None # Initialize image_id
                if enable_similarity_search or enable_face_similarity:
                    with processing_placeholder, st.spinner("æ›´æ–°å‘é‡åº“å¹¶æ‰§è¡Œç›¸ä¼¼æœç´¢..."):
                        try:
                            # Update DB returns the new image_id and list of face_ids
                            image_id, face_ids = update_vector_db(
                                st.session_state.image, # The potentially compressed PIL image
                                st.session_state.detected_faces, # List of face numpy arrays
                                feature_extractor,
                                transform
                            )

                            if image_id:
                                # Reload DBs to get the latest data including the current item
                                image_db, face_db = initialize_vector_db()

                                # Image Similarity Search
                                if enable_similarity_search and image_id in image_db:
                                    current_image_vector = image_db[image_id].get('vector')
                                    if current_image_vector is not None:
                                        # Exclude current image from search results
                                        search_image_db = {k: v for k, v in image_db.items() if k != image_id}
                                        similar_images = search_similar_images(
                                            current_image_vector, search_image_db, top_k=top_k, threshold=similarity_threshold
                                        )
                                        st.session_state.similar_images = similar_images
                                    else: st.session_state.similar_images = []

                                # Face Similarity Search
                                if enable_face_similarity and face_ids:
                                    current_face_vectors = [face_db[fid]['vector'] for fid in face_ids if fid in face_db and face_db[fid].get('vector') is not None]
                                    if current_face_vectors:
                                        # Exclude current image's faces from search
                                        search_face_db = {k: v for k, v in face_db.items() if k not in face_ids}
                                        similar_faces_results = search_similar_faces(
                                            current_face_vectors, search_face_db, top_k=top_k, threshold=similarity_threshold # Use general threshold
                                        )
                                        st.session_state.similar_faces_results = similar_faces_results
                                    else: st.session_state.similar_faces_results = []
                            else:
                                 st.warning("æ— æ³•æ›´æ–°æ•°æ®åº“ï¼Œè·³è¿‡ç›¸ä¼¼æ€§æœç´¢ã€‚")
                                 st.session_state.similar_images = []
                                 st.session_state.similar_faces_results = []

                        except Exception as e:
                            st.error(f"ç›¸ä¼¼æ€§æœç´¢æˆ–æ•°æ®åº“æ›´æ–°å¤±è´¥: {e}")
                            st.session_state.similar_images = []
                            st.session_state.similar_faces_results = []


                # --- Mark processing as done ---
                st.session_state.processed = True
                processing_placeholder.empty() # Clear the spinner/status message


            # --- Display Results (if processed) ---
            if st.session_state.get('processed'):
                 # Display potentially compressed original image
                 if 'image' in st.session_state and st.session_state.image:
                     with col1:
                         st.subheader("åŸå§‹å›¾ç‰‡ (å¤„ç†å)")
                         st.image(st.session_state.image, use_container_width=True)

                 # Display processed image with detections
                 if 'processed_img' in st.session_state and st.session_state.processed_img is not None:
                    with col2:
                        st.subheader("æ£€æµ‹ç»“æœ")
                        st.image(st.session_state.processed_img, use_container_width=True)
                        # Add download button for the processed image
                        try:
                            buf = io.BytesIO()
                            pil_img = Image.fromarray(st.session_state.processed_img)
                            pil_img.save(buf, format="PNG")
                            st.download_button(
                                label="ä¸‹è½½æ£€æµ‹ç»“æœå›¾",
                                data=buf.getvalue(),
                                file_name=f"detected_{Path(uploaded_file.name).stem}.png",
                                mime="image/png",
                            )
                        except Exception as e:
                            st.error(f"åˆ›å»ºä¸‹è½½æ–‡ä»¶å¤±è´¥: {e}")
                 else:
                     with col2:
                         st.warning("æ— æ³•æ˜¾ç¤ºå¤„ç†åçš„å›¾ç‰‡ã€‚")


                 # --- Display Sections: Detections, Faces, Similarity, AI Analysis ---
                 st.markdown("---") # Separator

                 # Column layout for results
                 res_col1, res_col2 = st.columns(2)

                 with res_col1:
                    # Display Detection Statistics
                    st.subheader("ç‰©ä½“æ£€æµ‹ç»Ÿè®¡")
                    if st.session_state.get('detection_results') and st.session_state.detection_results[0] is not None and hasattr(st.session_state.detection_results[0], 'boxes') and st.session_state.detection_results[0].boxes is not None:
                        results = st.session_state.detection_results
                        boxes = results[0].boxes
                        class_names = results[0].names if hasattr(results[0], 'names') else {}
                        class_counts = {}

                        # Recalculate selected classes based on sidebar state *now*
                        selected_classes_now = []
                        if detect_person: selected_classes_now.append("äºº")
                        if detect_cup: selected_classes_now.append("æ¯å­/é…’æ¯")
                        if detect_bottle: selected_classes_now.append("ç“¶å­")
                        if detect_all: selected_classes_now.append("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“")
                        # Map names to IDs
                        class_mapping_now = {"äºº": 0, "æ¯å­/é…’æ¯": 41, "ç“¶å­": 39}
                        selected_class_ids_now = list(class_names.keys()) if detect_all else [class_mapping_now[cls] for cls in selected_classes_now if cls in class_mapping_now]

                        for box in boxes:
                            cls_id = int(box.cls.item())
                            conf = box.conf.item()
                            if cls_id in selected_class_ids_now and conf >= confidence:
                                cls_name = class_names.get(cls_id, f'ID:{cls_id}')
                                class_counts[cls_name] = class_counts.get(cls_name, 0) + 1

                        if class_counts:
                             for cls_name, count in class_counts.items():
                                 st.write(f"- **{cls_name}**: {count} ä¸ª")
                        else:
                            st.write("æœªæ£€æµ‹åˆ°é€‰å®šç±»åˆ«çš„ç‰©ä½“ã€‚")
                    else:
                        st.write("æ— ç‰©ä½“æ£€æµ‹ç»“æœã€‚")

                    # Display Detected Faces (if any)
                    if detect_faces:
                        st.subheader(f"äººè„¸æ£€æµ‹ä¸æƒ…ç»ªåˆ†æ ({len(st.session_state.get('detected_faces', []))})")
                        detected_faces_list = st.session_state.get('detected_faces', [])
                        if detected_faces_list:
                            face_cols = st.columns(min(len(detected_faces_list), 4))
                            emotion_emojis = {"angry": "ğŸ˜ ","disgust": "ğŸ¤¢","fear": "ğŸ˜¨","happy": "ğŸ˜Š","sad": "ğŸ˜¢","surprise": "ğŸ˜²","neutral": "ğŸ˜"}

                            for i, face_array in enumerate(detected_faces_list):
                                with face_cols[i % 4]:
                                    st.image(face_array, caption=f"äººè„¸ #{i+1}", use_container_width=True)
                                    try:
                                        with st.spinner("åˆ†ææƒ…ç»ª..."):
                                             # DeepFace emotion analysis
                                             # Important: DeepFace expects BGR sometimes, but MTCNN gives RGB crop.
                                             # Let's try passing the RGB array. If it fails, try converting.
                                             try:
                                                  result = DeepFace.analyze(face_array, actions=['emotion'], enforce_detection=False, detector_backend='skip')
                                             except ValueError as ve: # If it fails, try BGR
                                                  if "BGR" in str(ve):
                                                       print("DeepFace analyze failed with RGB, trying BGR...")
                                                       face_bgr = cv2.cvtColor(face_array, cv2.COLOR_RGB2BGR)
                                                       result = DeepFace.analyze(face_bgr, actions=['emotion'], enforce_detection=False, detector_backend='skip')
                                                  else: raise ve # Re-raise other ValueErrors

                                             # Check result format
                                             if isinstance(result, list) and len(result) > 0 and isinstance(result[0], dict):
                                                  emotion = result[0].get('dominant_emotion', 'æœªçŸ¥')
                                                  emotion_emoji = emotion_emojis.get(emotion, "")
                                                  st.write(f"æƒ…ç»ª: **{emotion}** {emotion_emoji}")
                                             else:
                                                  st.write("æƒ…ç»ª: åˆ†æå¤±è´¥")

                                    except Exception as e:
                                        st.write(f"æƒ…ç»ª: åˆ†æå‡ºé”™ ({type(e).__name__})")
                                        # print(f"DeepFace emotion error face {i}: {e}") # Log error
                        else:
                            st.info("æœªæ£€æµ‹åˆ°äººè„¸ã€‚")

                 with res_col2:
                    # Display AI Analysis (Groq + Gemini/OpenAI)
                    st.subheader("AI ç»¼åˆåˆ†æ")
                    with st.spinner("æ‰§è¡Œ AI åˆ†æå’Œåˆè§„æ€»ç»“..."):
                        groq_api_key = os.environ.get("GROQ_API_KEY")
                        openai_api_key = os.environ.get("GEMINI_API_KEY") # Using Gemini Key

                        if groq_api_key and openai_api_key:
                            image_to_analyze = st.session_state.get('image')
                            if image_to_analyze:
                                # 1. Get image dimensions
                                image_dimensions = check_image_dimensions(image_to_analyze)

                                # 2. Groq Analysis
                                groq_analysis = analyze_with_groq(image_to_analyze, groq_api_key)
                                # st.text_area("Groq è¯¦ç»†åˆ†æ:", groq_analysis, height=200) # Optional: show raw Groq output

                                # 3. Prepare CV Summary for OpenAI/Gemini
                                cv_summary_dict = {
                                    "detected_objects": class_counts if 'class_counts' in locals() else {},
                                    "face_count": len(st.session_state.get('detected_faces', []))
                                }

                                # 4. OpenAI/Gemini Summary
                                final_summary = summarize_with_openai(cv_summary_dict, groq_analysis, openai_api_key, image_dimensions)
                                st.markdown("##### åˆè§„æ€§åˆ†ææŠ¥å‘Š:")
                                st.markdown(final_summary) # Display the summary table


                                # 5. Text Extraction and Similarity (after Groq)
                                if image_id: # Ensure image_id was set from DB update
                                    text_content = extract_text_from_analysis(groq_analysis)
                                    print(f"æå–çš„æ–‡æœ¬å†…å®¹: '{text_content}'") # Debugging
                                    if text_content and text_content.strip().lower() != 'æ— æ–‡å­—å†…å®¹':
                                        update_text_vector_db(image_id, text_content)
                                        print(f"Searching similar text for {image_id}...")
                                        similar_texts = search_similar_texts(image_id, top_k=top_k, threshold=0.85) # Higher threshold for text
                                        st.session_state.similar_texts = similar_texts
                                        if similar_texts:
                                             display_similar_texts(similar_texts)
                                        else:
                                             print("No similar text found above threshold.")
                                    else:
                                        print("æ— æœ‰æ•ˆæ–‡å­—å†…å®¹ï¼Œè·³è¿‡æ–‡æœ¬ç›¸ä¼¼æ€§æœç´¢ã€‚")
                                        st.session_state.similar_texts = []

                                    with st.expander("å›¾ç‰‡ä¸­æå–çš„æ–‡å­—å†…å®¹ (æ¥è‡ª AI åˆ†æ)"):
                                        st.write(text_content if text_content else "æ— æ–‡å­—å†…å®¹")
                                else:
                                     st.warning("æ— æ³•æå–æˆ–æ¯”è¾ƒæ–‡æœ¬ï¼Œå› ä¸ºå›¾ç‰‡æœªèƒ½æ·»åŠ åˆ°æ•°æ®åº“ã€‚")


                            else:
                                st.warning("æ— æ³•æ‰§è¡Œ AI åˆ†æï¼Œå› ä¸ºå¤„ç†åçš„å›¾ç‰‡ä¸å¯ç”¨ã€‚")
                        else:
                            st.warning("è¯·è®¾ç½® GROQ_API_KEY å’Œ GEMINI_API_KEY ç¯å¢ƒå˜é‡ä»¥å¯ç”¨å®Œæ•´çš„ AI åˆ†æåŠŸèƒ½ã€‚")


                 st.markdown("---") # Separator

                 # --- Similarity Search Results ---
                 results_exist = st.session_state.get('similar_images') or st.session_state.get('similar_faces_results')
                 if results_exist:
                     st.subheader("ç›¸ä¼¼æ€§æœç´¢ç»“æœ")
                     sim_col1, sim_col2 = st.columns(2)
                     with sim_col1:
                         if enable_similarity_search and st.session_state.get('similar_images'):
                             display_similar_images(st.session_state.similar_images)
                         elif enable_similarity_search:
                             st.info("æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡ã€‚")

                     with sim_col2:
                         if enable_face_similarity and st.session_state.get('similar_faces_results'):
                             display_similar_faces(st.session_state.similar_faces_results, st.session_state.get('detected_faces', []))
                         elif enable_face_similarity:
                             st.info("æœªæ‰¾åˆ°ç›¸ä¼¼äººè„¸ã€‚")


                 st.markdown("---") # Separator

                 # --- Advanced Face Search (Offline Clustering Comparison) ---
                 if enable_advanced_face_search and st.session_state.get('detected_faces'):
                     st.subheader("åŠ å¼ºäººè„¸æ£€ç´¢ (ä¸ç¦»çº¿èšç±»æ¯”è¾ƒ)")
                     detected_faces_list = st.session_state.get('detected_faces', [])

                     if len(detected_faces_list) > 10: # Limit for performance
                          st.warning(f"æ£€æµ‹åˆ°çš„äººè„¸æ•°é‡ ({len(detected_faces_list)}) è¾ƒå¤šï¼Œä¸ºæé«˜æ€§èƒ½å·²è·³è¿‡åŠ å¼ºäººè„¸æ£€ç´¢ã€‚")
                     else:
                         # Check if tolerance changed or search hasn't been done
                         tolerance_changed = st.session_state.get('current_tolerance') != face_cluster_tolerance
                         search_needed = not st.session_state.get('advanced_face_search_done') or tolerance_changed

                         if search_needed:
                             with st.spinner("ä¸ç¦»çº¿èšç±»æ¯”è¾ƒäººè„¸..."):
                                 st.session_state.current_tolerance = face_cluster_tolerance # Update stored tolerance
                                 # Pass the list of numpy arrays
                                 face_best_matches = process_face_for_advanced_search(
                                     detected_faces_list,
                                     tolerance=face_cluster_tolerance # Pass current tolerance (distance)
                                 )
                                 st.session_state.face_best_matches = face_best_matches
                                 st.session_state.advanced_face_search_done = True
                         else:
                             face_best_matches = st.session_state.get('face_best_matches', [])

                         # Display results
                         if face_best_matches:
                             st.write(f"ä¸º {len(face_best_matches)} ä¸ªæ£€æµ‹åˆ°çš„äººè„¸åœ¨ç¦»çº¿èšç±»ä¸­æ‰¾åˆ°äº†æœ€ä½³åŒ¹é…ï¼š")
                             match_cols = st.columns(min(len(face_best_matches), 3))
                             for i, match in enumerate(face_best_matches):
                                 with match_cols[i % 3]:
                                     face_idx = match['face_idx']
                                     st.write(f"**æ£€æµ‹äººè„¸ #{face_idx + 1}**")
                                     # Display the detected face
                                     if face_idx < len(detected_faces_list):
                                          st.image(detected_faces_list[face_idx], width=100)
                                     # Display the matched cluster montage (if path exists)
                                     if match.get('path') and os.path.exists(match['path']):
                                         st.image(match["path"],
                                                  caption=f"åŒ¹é…èšç±»: {match['cluster']} (ç›¸ä¼¼åº¦: {match['similarity']:.3f})",
                                                  use_container_width=True)
                                     elif match.get('cluster'): # Show cluster name even if montage missing
                                          st.info(f"åŒ¹é…èšç±»: {match['cluster']} (ç›¸ä¼¼åº¦: {match['similarity']:.3f}, è’™å¤ªå¥‡å›¾ä¸¢å¤±)")
                                     else: # Should not happen if match exists
                                          st.warning("åŒ¹é…ä¿¡æ¯ä¸å®Œæ•´ã€‚")
                         else:
                              st.info("æœªåœ¨ç¦»çº¿èšç±»ä¸­æ‰¾åˆ°ä¸å½“å‰å›¾ç‰‡äººè„¸è¶³å¤Ÿç›¸ä¼¼çš„èšç±»ã€‚")

        # End of single image processing block
        # Display usage instructions if no file is uploaded yet
        elif not uploaded_file:
             st.info("è¯·åœ¨ä¸Šæ–¹ä¸Šä¼ å•å¼ å›¾ç‰‡å¼€å§‹å¤„ç†ï¼Œæˆ–åœ¨ä¾§è¾¹æ åˆ‡æ¢åˆ°æ‰¹é‡å¤„ç†æ¨¡å¼ã€‚")


except Exception as e:
    st.error(f"åº”ç”¨å‘ç”Ÿä¸¥é‡é”™è¯¯: {type(e).__name__} - {str(e)}")
    st.error("è¯·æ£€æŸ¥æ§åˆ¶å°æ—¥å¿—è·å–è¯¦ç»†ä¿¡æ¯ã€‚å¦‚æœé—®é¢˜æŒç»­ï¼Œè¯·å°è¯•åˆ·æ–°é¡µé¢æˆ–é‡å¯åº”ç”¨ã€‚")
    # Optional: Log the full traceback for debugging
    import traceback
    st.code(traceback.format_exc())
    # Attempt to reset state partially on major error
    reset_processed_state()


# Add usage instructions at the end
st.markdown("---")
with st.expander("ä½¿ç”¨è¯´æ˜å’Œæ¨¡å‹ä¿¡æ¯", expanded=False):
    st.markdown("""
    ### ä½¿ç”¨æ–¹æ³•ï¼š
    1.  **æ¨¡å¼é€‰æ‹©**: åœ¨ä¾§è¾¹æ åº•éƒ¨é€‰æ‹©â€œå•å¼ å›¾ç‰‡å¤„ç†â€æˆ–å¯ç”¨â€œæ‰¹é‡å¤„ç†æ¨¡å¼â€ã€‚
    2.  **ä¸Šä¼ **:
        *   å•å¼ æ¨¡å¼ï¼šä½¿ç”¨ä¸»ç•Œé¢çš„ä¸Šä¼ æ¡†ä¸Šä¼ ä¸€å¼ å›¾ç‰‡ã€‚
        *   æ‰¹é‡æ¨¡å¼ï¼šä½¿ç”¨ä¸»ç•Œé¢çš„ä¸Šä¼ æ¡†ä¸Šä¼ å¤šå¼ å›¾ç‰‡ã€‚
    3.  **è®¾ç½® (ä¾§è¾¹æ )**:
        *   **æ¨¡å‹é€‰æ‹©**: é€‰æ‹©åˆé€‚çš„YOLOæ¨¡å‹ï¼ˆç²¾åº¦ vs. é€Ÿåº¦ï¼‰ã€‚
        *   **ç½®ä¿¡åº¦é˜ˆå€¼**: è°ƒæ•´ç‰©ä½“æ£€æµ‹çš„ç½®ä¿¡åº¦ï¼ˆè¶Šé«˜è¶Šä¸¥æ ¼ï¼‰ã€‚
        *   **æ£€æµ‹ç±»åˆ«**: é€‰æ‹©å¸Œæœ›æ£€æµ‹çš„ç‰©ä½“ã€‚
        *   **äººè„¸æ£€æµ‹**: å¯ç”¨/ç¦ç”¨MTCNNäººè„¸æ£€æµ‹åŠç½®ä¿¡åº¦ã€‚
        *   **ç›¸ä¼¼æœç´¢**: å¯ç”¨/ç¦ç”¨å›¾ç‰‡åŠäººè„¸ç›¸ä¼¼æ€§æœç´¢ï¼Œè®¾ç½®é˜ˆå€¼å’Œè¿”å›æ•°é‡ã€‚
        *   **æ ‡è®°é¢œè‰²**: è‡ªå®šä¹‰ç‰©ä½“å’Œäººè„¸è¾¹ç•Œæ¡†é¢œè‰²ã€‚
        *   **åŠ å¼ºäººè„¸æ£€ç´¢**: å¯ç”¨åï¼Œä¼šå°†æ£€æµ‹åˆ°çš„äººè„¸ä¸é¢„å…ˆç”Ÿæˆçš„ç¦»çº¿äººè„¸èšç±»è¿›è¡Œæ¯”è¾ƒã€‚è°ƒæ•´â€œä¸¥æ ¼åº¦â€ï¼ˆè·ç¦»é˜ˆå€¼ï¼Œè¶Šä½è¶Šä¸¥ï¼‰å’Œâ€œæœ€å°èšç±»æ•°é‡â€ã€‚**æ³¨æ„**: æ­¤åŠŸèƒ½éœ€è¦é¢„å…ˆè¿è¡Œ `face_clustering` è„šæœ¬ç”Ÿæˆ `clustered_faces_*` ç›®å½•ã€‚
    4.  **å¤„ç†**:
        *   å•å¼ æ¨¡å¼ï¼šå›¾ç‰‡ä¸Šä¼ åè‡ªåŠ¨å¤„ç†ã€‚
        *   æ‰¹é‡æ¨¡å¼ï¼šä¸Šä¼ å›¾ç‰‡åï¼Œç‚¹å‡»â€œå¼€å§‹å¤„ç†â€æŒ‰é’®ã€‚
    5.  **æŸ¥çœ‹ç»“æœ**:
        *   å•å¼ æ¨¡å¼ï¼šç»“æœç›´æ¥æ˜¾ç¤ºåœ¨ä¸»ç•Œé¢ã€‚
        *   æ‰¹é‡æ¨¡å¼ï¼šç»“æœæ˜¾ç¤ºåœ¨ä¸åŒçš„æ ‡ç­¾é¡µä¸­ã€‚
        *   **AIç»¼åˆåˆ†æ**: ï¼ˆéœ€è¦API Keyï¼‰æä¾›Groqçš„è¯¦ç»†åˆ†æå’ŒGemini/OpenAIçš„åˆè§„æ€§æ€»ç»“ã€‚
        *   **ç›¸ä¼¼æ–‡æœ¬æ£€æŸ¥**: ï¼ˆéœ€è¦API Keyå’Œ`sentence-transformers`ï¼‰å¦‚æœAIåˆ†ææå–åˆ°æ–‡å­—ï¼Œä¼šä¸å†å²å›¾ç‰‡ä¸­çš„æ–‡å­—è¿›è¡Œç›¸ä¼¼åº¦æ¯”è¾ƒã€‚

    ### æ¨¡å‹å’ŒæŠ€æœ¯:
    *   **ç‰©ä½“æ£€æµ‹**: YOLOv8 / YOLOv11 (Ultralytics) - åŸºäºCOCOæ•°æ®é›†é¢„è®­ç»ƒã€‚
    *   **äººè„¸æ£€æµ‹**: MTCNN (Multi-task Cascaded Convolutional Networks)ã€‚
    *   **äººè„¸ç‰¹å¾æå–/æ¯”è¾ƒ**: DeepFace (æ”¯æŒVGG-Face, Facenet, ArcFaceç­‰æ¨¡å‹ï¼Œé»˜è®¤ä½¿ç”¨ArcFaceè¿›è¡Œç‰¹å¾æå–å’Œç›¸ä¼¼åº¦è®¡ç®—)ã€‚
    *   **å›¾ç‰‡ç‰¹å¾æå–**: ResNet50 (PyTorch).
    *   **ç›¸ä¼¼åº¦è®¡ç®—**: Cosine Similarity.
    *   **AIåˆ†æ**: Groq (Llama 3 Vision) / OpenAI (Gemini Pro/Flash)ã€‚
    *   **æ–‡æœ¬ç‰¹å¾æå–**: Sentence Transformers (`paraphrase-multilingual-MiniLM-L12-v2`).
    *   **äººè„¸èšç±»**: DBSCAN / Agglomerative Clustering (Scikit-learn) ç»“åˆ DeepFace ç‰¹å¾ã€‚
    """)

# Footer
st.markdown("---")
st.markdown("å·¥å…·ç‰ˆæœ¬ v1.2 | æŠ€æœ¯æ ˆ: Streamlit, YOLO, MTCNN, DeepFace, Groq, Gemini, Scikit-learn")
