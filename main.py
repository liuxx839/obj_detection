import streamlit as st
from ultralytics import YOLO
import cv2
import numpy as np
from PIL import Image
import io
import os
import pickle
import uuid
import time
from sklearn.metrics.pairwise import cosine_similarity
from pathlib import Path
import torch
from torchvision import models, transforms
from groq import Groq
from openai import OpenAI
import base64
from io import BytesIO
from mtcnn import MTCNN
from deepface import DeepFace
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from imutils import build_montages
import shutil
from datetime import datetime
import hashlib
import random
import tempfile


# è®¾ç½®é¡µé¢æ ‡é¢˜
st.set_page_config(page_title="ç‰©ä½“æ£€æµ‹ä¸ç›¸ä¼¼æœç´¢å·¥å…·", layout="wide")

# åˆå§‹åŒ–session_stateä»¥è·Ÿè¸ªå›¾ç‰‡å¤„ç†çŠ¶æ€
if 'processed' not in st.session_state:
    st.session_state.processed = False
if 'image' not in st.session_state:
    st.session_state.image = None
if 'last_upload_id' not in st.session_state:
    st.session_state.last_upload_id = None
if 'detection_results' not in st.session_state:
    st.session_state.detection_results = None
if 'processed_img' not in st.session_state:
    st.session_state.processed_img = None
if 'detected_faces' not in st.session_state:
    st.session_state.detected_faces = []
if 'similar_images' not in st.session_state:
    st.session_state.similar_images = []
if 'similar_faces_results' not in st.session_state:
    st.session_state.similar_faces_results = []
if 'advanced_face_search_done' not in st.session_state:
    st.session_state.advanced_face_search_done = False
if 'face_best_matches' not in st.session_state:
    st.session_state.face_best_matches = []
if 'current_tolerance' not in st.session_state:
    st.session_state.current_tolerance = 0.6
if 'similar_texts' not in st.session_state:
    st.session_state.similar_texts = []
if 'batch_mode' not in st.session_state:
    st.session_state.batch_mode = False
if 'batch_images' not in st.session_state:
    st.session_state.batch_images = []  # å­˜å‚¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡
if 'batch_faces' not in st.session_state:
    st.session_state.batch_faces = []  # å­˜å‚¨æ‰¹é‡ä¸Šä¼ çš„å›¾ç‰‡ä¸­æ£€æµ‹åˆ°çš„äººè„¸
if 'batch_processed_images' not in st.session_state:
    st.session_state.batch_processed_images = []  # å­˜å‚¨æ‰¹é‡å¤„ç†åçš„å›¾ç‰‡
if 'batch_image_vectors' not in st.session_state:
    st.session_state.batch_image_vectors = {}  # å­˜å‚¨æ‰¹é‡å›¾ç‰‡çš„ç‰¹å¾å‘é‡
if 'batch_face_vectors' not in st.session_state:
    st.session_state.batch_face_vectors = {}  # å­˜å‚¨æ‰¹é‡äººè„¸çš„ç‰¹å¾å‘é‡
if 'batch_similarity_results' not in st.session_state:
    st.session_state.batch_similarity_results = []  # å­˜å‚¨æ‰¹é‡å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦ç»“æœ
if 'batch_face_similarity_results' not in st.session_state:
    st.session_state.batch_face_similarity_results = []  # å­˜å‚¨æ‰¹é‡äººè„¸é—´çš„ç›¸ä¼¼åº¦ç»“æœ

# æ ‡é¢˜
st.title("ğŸ“· é«˜ç²¾åº¦ç‰©ä½“æ£€æµ‹ä¸ç›¸ä¼¼æœç´¢å·¥å…·")
st.write("ä¸Šä¼ å›¾ç‰‡ï¼Œæ£€æµ‹äººã€é…’æ¯ç­‰ç‰©ä½“ï¼Œå¹¶åœ¨å†å²å›¾ç‰‡ä¸­å¯»æ‰¾ç›¸ä¼¼å›¾ç‰‡å’Œäººè„¸")

# åˆ›å»ºå­˜å‚¨ç›®å½•
def create_directories():
    dirs = ["data", "data/images", "data/faces", "data/vectors"]
    for dir_path in dirs:
        os.makedirs(dir_path, exist_ok=True)
    return Path("data")

data_dir = create_directories()

# ä¾§è¾¹æ è®¾ç½®
with st.sidebar:
    st.header("è®¾ç½®")
    
    # æ¨¡å‹é€‰æ‹©
    model_option = st.selectbox(
        "é€‰æ‹©æ¨¡å‹",
        ["YOLOv11x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)", 
         "YOLOv11l (é«˜ç²¾åº¦)", 
         "YOLOv11m (ä¸­ç­‰ç²¾åº¦)", 
         "YOLOv11s (è¾ƒå¿«)", 
         "YOLOv11n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)",
         "YOLOv8x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)", 
         "YOLOv8l (é«˜ç²¾åº¦)", 
         "YOLOv8m (ä¸­ç­‰ç²¾åº¦)", 
         "YOLOv8s (è¾ƒå¿«)", 
         "YOLOv8n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)"],
        index=4  # é»˜è®¤é€‰æ‹©YOLOv11x
    )
    
    # æ¨¡å‹æ˜ å°„
    model_mapping = {
        "YOLOv11x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)": "yolo11x.pt",
        "YOLOv11l (é«˜ç²¾åº¦)": "yolo11l.pt",
        "YOLOv11m (ä¸­ç­‰ç²¾åº¦)": "yolo11m.pt",
        "YOLOv11s (è¾ƒå¿«)": "yolo11s.pt",
        "YOLOv11n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)": "yolo11n.pt",
        "YOLOv8x (æœ€é«˜ç²¾åº¦ï¼Œè¾ƒæ…¢)": "yolov8x.pt",
        "YOLOv8l (é«˜ç²¾åº¦)": "yolov8l.pt",
        "YOLOv8m (ä¸­ç­‰ç²¾åº¦)": "yolov8m.pt",
        "YOLOv8s (è¾ƒå¿«)": "yolov8s.pt",
        "YOLOv8n (æœ€å¿«ï¼Œç²¾åº¦è¾ƒä½)": "yolov8n.pt"
    }
    
    confidence = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.1, 1.0, 0.5, 0.1)
    
    # é€‰æ‹©è¦æ£€æµ‹çš„ç±»åˆ«
    st.subheader("é€‰æ‹©è¦æ£€æµ‹çš„ç±»åˆ«")
    detect_person = st.checkbox("äºº", value=True)
    detect_cup = st.checkbox("æ¯å­/é…’æ¯", value=True)
    detect_bottle = st.checkbox("ç“¶å­", value=True)
    detect_all = st.checkbox("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“", value=True)
    
    # äººè„¸æ£€æµ‹é€‰é¡¹
    st.subheader("äººè„¸æ£€æµ‹")
    detect_faces = st.checkbox("å¯ç”¨äººè„¸æ£€æµ‹", value=True)
    face_confidence = st.slider("äººè„¸æ£€æµ‹ç½®ä¿¡åº¦", 0.1, 1.0, 0.8, 0.1)
    
    # ç›¸ä¼¼æœç´¢é€‰é¡¹
    st.subheader("ç›¸ä¼¼æœç´¢")
    enable_similarity_search = st.checkbox("å¯ç”¨ç›¸ä¼¼å›¾ç‰‡æœç´¢", value=True)
    enable_face_similarity = st.checkbox("å¯ç”¨äººè„¸ç›¸ä¼¼æœç´¢", value=True)
    similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.0, 1.0, 0.5, 0.05)
    top_k = st.slider("è¿”å›ç›¸ä¼¼ç»“æœæ•°é‡", 1, 10, 3)
    
    # è·å–é¢œè‰²è®¾ç½®
    st.subheader("æ ‡è®°é¢œè‰²")
    box_color = st.color_picker("è¾¹æ¡†é¢œè‰²", "#FF0000")
    face_box_color = st.color_picker("äººè„¸è¾¹æ¡†é¢œè‰²", "#00FF00")

    # æ·»åŠ åŠ å¼ºäººè„¸æ£€ç´¢é€‰é¡¹
    st.subheader("åŠ å¼ºäººè„¸æ£€ç´¢")
    enable_advanced_face_search = st.checkbox("å¯ç”¨åŠ å¼ºäººè„¸æ£€ç´¢", value=True)
    if enable_advanced_face_search:
        face_cluster_tolerance = st.slider("äººè„¸åŒ¹é…ä¸¥æ ¼åº¦", 0.4, 0.8, 0.6, 0.05, help="æ•°å€¼è¶Šä½åŒ¹é…è¶Šä¸¥æ ¼")
        min_cluster_size = st.slider("æœ€å°èšç±»æ•°é‡", 1, 10, 3, 1, help="å½¢æˆèšç±»æ‰€éœ€çš„æœ€å°å›¾ç‰‡æ•°é‡")

    # åœ¨ä¾§è¾¹æ æ·»åŠ æ‰¹é‡æ¨¡å¼é€‰é¡¹
    st.subheader("æ‰¹é‡å¤„ç†")
    batch_mode = st.checkbox("å¯ç”¨æ‰¹é‡å¤„ç†æ¨¡å¼", value=False)
    if batch_mode:
        st.session_state.batch_mode = True
        st.info("æ‰¹é‡æ¨¡å¼å·²å¯ç”¨ï¼Œæ‚¨å¯ä»¥ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œå¤„ç†")
        if st.button("æ¸…é™¤æ‰¹é‡å¤„ç†æ•°æ®"):
            st.session_state.batch_images = []
            st.session_state.batch_faces = []
            st.session_state.batch_processed_images = []
            st.session_state.batch_image_vectors = {}
            st.session_state.batch_face_vectors = {}
            st.session_state.batch_similarity_results = []
            st.session_state.batch_face_similarity_results = []
            st.success("æ‰¹é‡å¤„ç†æ•°æ®å·²æ¸…é™¤")
    else:
        st.session_state.batch_mode = False

# åŠ è½½YOLOæ¨¡å‹
@st.cache_resource
def load_model(model_name):
    return YOLO(model_name)

# åŠ è½½äººè„¸æ£€æµ‹æ¨¡å‹
@st.cache_resource
def load_face_detector():
    # ä½¿ç”¨MTCNNäººè„¸æ£€æµ‹å™¨
    detector = MTCNN()
    return detector


# åŠ è½½ç‰¹å¾æå–æ¨¡å‹
@st.cache_resource
def load_feature_extractor():
    # ä½¿ç”¨é¢„è®­ç»ƒçš„ResNet50
    model = models.resnet50(weights='DEFAULT')
    # ç§»é™¤æœ€åçš„å…¨è¿æ¥å±‚
    model = torch.nn.Sequential(*(list(model.children())[:-1]))
    model.eval()
    return model

# å›¾åƒé¢„å¤„ç†è½¬æ¢
@st.cache_resource
def get_transform():
    return transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])

# æå–å›¾åƒç‰¹å¾
def extract_image_features(image, model, transform):
    # é¢„å¤„ç†å›¾åƒ
    img_t = transform(image).unsqueeze(0)
    
    # æå–ç‰¹å¾
    with torch.no_grad():
        features = model(img_t)
    
    # è¿”å›ç‰¹å¾å‘é‡
    return features.squeeze().cpu().numpy()

# æå–äººè„¸ç‰¹å¾
def extract_face_features(face_image, model, transform):
    # å¦‚æœäººè„¸å›¾åƒå¤ªå°ï¼Œå¯èƒ½éœ€è¦è°ƒæ•´å¤§å°
    if face_image.size[0] < 30 or face_image.size[1] < 30:
        return None
    
    # é¢„å¤„ç†äººè„¸å›¾åƒ
    face_t = transform(face_image).unsqueeze(0)
    
    # æå–ç‰¹å¾
    with torch.no_grad():
        features = model(face_t)
    
    # è¿”å›ç‰¹å¾å‘é‡
    return features.squeeze().cpu().numpy()

# åˆå§‹åŒ–å‘é‡åº“
def initialize_vector_db():
    vector_file = data_dir / "vectors" / "image_vectors.pkl"
    face_vector_file = data_dir / "vectors" / "face_vectors.pkl"
    
    # å›¾åƒå‘é‡æ•°æ®åº“
    if os.path.exists(vector_file):
        with open(vector_file, 'rb') as f:
            image_db = pickle.load(f)
    else:
        # åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
        # ä½¿ç”¨å­—å…¸å­˜å‚¨: {image_id: {'vector': feature_vector, 'path': image_path}}
        image_db = {}
    
    # äººè„¸å‘é‡æ•°æ®åº“
    if os.path.exists(face_vector_file):
        with open(face_vector_file, 'rb') as f:
            face_db = pickle.load(f)
    else:
        # åˆ›å»ºæ–°çš„äººè„¸å‘é‡æ•°æ®åº“
        # ä½¿ç”¨å­—å…¸å­˜å‚¨: {face_id: {'vector': feature_vector, 'path': face_path, 'image_id': parent_image_id}}
        face_db = {}
    
    return image_db, face_db

# ä¿å­˜å‘é‡æ•°æ®åº“
def save_vector_db(image_db, face_db):
    vector_file = data_dir / "vectors" / "image_vectors.pkl"
    face_vector_file = data_dir / "vectors" / "face_vectors.pkl"
    
    with open(vector_file, 'wb') as f:
        pickle.dump(image_db, f)
    
    with open(face_vector_file, 'wb') as f:
        pickle.dump(face_db, f)

# æ›´æ–°å‘é‡æ•°æ®åº“
def update_vector_db(image, faces, feature_extractor, transform):
    # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
    image_db, face_db = initialize_vector_db()
    
    # ç”Ÿæˆå”¯ä¸€ID
    image_id = str(uuid.uuid4())
    timestamp = int(time.time())
    
    # ä¿å­˜å›¾åƒ
    image_filename = f"{image_id}.jpg"
    image_path = data_dir / "images" / image_filename
    # ç¡®ä¿å›¾åƒä¸ºRGBæ¨¡å¼
    if image.mode == 'RGBA':
        image = image.convert('RGB')
    image.save(image_path)
    
    # æå–å›¾åƒç‰¹å¾
    image_features = extract_image_features(image, feature_extractor, transform)
    
    # æ›´æ–°å›¾åƒå‘é‡æ•°æ®åº“
    image_db[image_id] = {
        'vector': image_features,
        'path': str(image_path),
        'timestamp': timestamp
    }
    
    # å¤„ç†äººè„¸
    face_ids = []
    for i, face in enumerate(faces):
        face_id = f"{image_id}_face_{i}"
        face_filename = f"{face_id}.jpg"
        face_path = data_dir / "faces" / face_filename
        
        # å°†PIL Imageè½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶ä¿å­˜
        face_pil = Image.fromarray(face)
        # ç¡®ä¿äººè„¸å›¾åƒä¸ºRGBæ¨¡å¼
        if face_pil.mode == 'RGBA':
            face_pil = face_pil.convert('RGB')
        face_pil.save(face_path)
        
        # æå–äººè„¸ç‰¹å¾
        face_features = extract_face_features(face_pil, feature_extractor, transform)
        
        if face_features is not None:
            # æ›´æ–°äººè„¸å‘é‡æ•°æ®åº“
            face_db[face_id] = {
                'vector': face_features,
                'path': str(face_path),
                'image_id': image_id,
                'timestamp': timestamp
            }
            face_ids.append(face_id)
    
    # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
    save_vector_db(image_db, face_db)
    
    return image_id, face_ids

# æ‰§è¡Œç›¸ä¼¼æœç´¢
def search_similar_images(query_vector, image_db, top_k=3, threshold=0.6):
    if not image_db:
        return []
    
    results = []
    
    # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰å‘é‡çš„ç›¸ä¼¼åº¦
    for image_id, data in image_db.items():
        db_vector = data['vector']
        similarity = cosine_similarity([query_vector], [db_vector])[0][0]
        
        if similarity >= threshold:
            results.append({
                'id': image_id,
                'path': data['path'],
                'similarity': similarity,
                'timestamp': data.get('timestamp', 0)
            })
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    results.sort(key=lambda x: x['similarity'], reverse=True)
    
    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

# æ‰§è¡Œäººè„¸ç›¸ä¼¼æœç´¢
def search_similar_faces(query_vectors, face_db, top_k=3, threshold=0.6):
    if not face_db or not query_vectors:
        return []
    
    all_results = []
    
    # å¯¹æ¯ä¸ªæŸ¥è¯¢äººè„¸è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
    for i, query_vector in enumerate(query_vectors):
        results = []
        
        # è®¡ç®—æŸ¥è¯¢å‘é‡ä¸æ•°æ®åº“ä¸­æ‰€æœ‰äººè„¸å‘é‡çš„ç›¸ä¼¼åº¦
        for face_id, data in face_db.items():
            db_vector = data['vector']
            similarity = cosine_similarity([query_vector], [db_vector])[0][0]
            
            if similarity >= threshold:
                results.append({
                    'id': face_id,
                    'path': data['path'],
                    'image_id': data['image_id'],
                    'similarity': similarity,
                    'timestamp': data.get('timestamp', 0)
                })
        
        # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
        results.sort(key=lambda x: x['similarity'], reverse=True)
        
        # ä¸ºå½“å‰æŸ¥è¯¢äººè„¸ä¿å­˜å‰Kä¸ªç»“æœ
        all_results.append({
            'query_face_index': i,
            'matches': results[:top_k]
        })
    
    return all_results

# å¤„ç†é¢„æµ‹ç»“æœå¹¶ç»˜åˆ¶è¾¹æ¡†
def process_prediction(image, results, selected_classes, conf_threshold):
    # è½¬æ¢ä¸ºOpenCVæ ¼å¼ç”¨äºç»˜å›¾
    img = np.array(image)
    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)
    
    # COCOæ•°æ®é›†ç±»åˆ«
    class_names = results[0].names
    
    # å¦‚æœé€‰æ‹©äº†"æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“"ï¼Œå°†æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«
    if "æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“" in selected_classes:
        selected_class_ids = list(class_names.keys())
    else:
        # åˆ›å»ºç±»åˆ«åç§°åˆ°IDçš„æ˜ å°„
        class_mapping = {
            "äºº": 0,  # person
            "æ¯å­/é…’æ¯": 41,  # cup
            "ç“¶å­": 39,  # bottle
        }
        # è·å–é€‰ä¸­ç±»åˆ«çš„ID
        selected_class_ids = [class_mapping[cls] for cls in selected_classes if cls in class_mapping]
    
    # è§£æåå…­è¿›åˆ¶é¢œè‰²ä¸ºBGR
    hex_color = box_color.lstrip('#')
    box_bgr = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))  # è½¬æ¢ä¸ºBGR
    
    # å¦‚æœæœ‰æ£€æµ‹ç»“æœ
    if results[0].boxes is not None:
        boxes = results[0].boxes
        
        for box in boxes:
            # è·å–ç±»åˆ«IDå’Œç½®ä¿¡åº¦
            cls_id = int(box.cls.item())
            conf = box.conf.item()
            
            # å¦‚æœç±»åˆ«åœ¨é€‰ä¸­çš„ç±»åˆ«ä¸­ä¸”ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼
            if cls_id in selected_class_ids and conf >= conf_threshold:
                # è·å–è¾¹ç•Œæ¡†åæ ‡
                x1, y1, x2, y2 = box.xyxy[0].tolist()
                x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
                
                # ç»˜åˆ¶è¾¹ç•Œæ¡†
                cv2.rectangle(img, (x1, y1), (x2, y2), box_bgr, 2)
                
                # æ·»åŠ æ ‡ç­¾
                label = f"{class_names[cls_id]} {conf:.2f}"
                text_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]
                cv2.rectangle(img, (x1, y1 - text_size[1] - 5), (x1 + text_size[0], y1), box_bgr, -1)
                cv2.putText(img, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
    
    # è½¬æ¢å›RGBç”¨äºStreamlitæ˜¾ç¤º
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    return img

# äººè„¸æ£€æµ‹å‡½æ•°
def detect_face(image, face_detector, conf_threshold):
    # è½¬æ¢ä¸ºnumpyæ•°ç»„å¹¶ç¡®ä¿RGBæ ¼å¼
    img = np.array(image)
    if img.shape[-1] == 4:  # æ£€æŸ¥æ˜¯å¦ä¸ºRGBA
        img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)  # è½¬æ¢ä¸ºRGB
    img_rgb = img.copy()
    
    # ä½¿ç”¨MTCNNæ£€æµ‹äººè„¸
    faces = face_detector.detect_faces(img_rgb)
    
    # ç­›é€‰ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„äººè„¸
    faces = [face for face in faces if face['confidence'] >= conf_threshold]
    
    # è§£æäººè„¸è¾¹æ¡†é¢œè‰²
    hex_color = face_box_color.lstrip('#')
    face_bgr = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))  # è½¬æ¢ä¸ºBGR
    
    # æå–çš„äººè„¸å›¾åƒåˆ—è¡¨
    face_images = []
    
    # åœ¨åŸå›¾ä¸Šæ ‡è®°äººè„¸
    img_cv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2BGR)
    for face in faces:
        # è·å–è¾¹ç•Œæ¡†åæ ‡
        x, y, w, h = face['box']
        
        # ç»˜åˆ¶äººè„¸è¾¹æ¡†
        cv2.rectangle(img_cv, (x, y), (x+w, y+h), face_bgr, 2)
        
        # æ·»åŠ æ ‡ç­¾å’Œç½®ä¿¡åº¦
        label = f"Face: {face['confidence']:.2f}"
        cv2.putText(img_cv, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, face_bgr, 2)
        
        # å¯é€‰ï¼šç»˜åˆ¶å…³é”®ç‚¹
        keypoints = face['keypoints']
        for point in keypoints.values():
            cv2.circle(img_cv, point, 2, face_bgr, 2)
        
        # æå–äººè„¸åŒºåŸŸ
        face_crop = img_rgb[y:y+h, x:x+w]
        face_images.append(face_crop)
    
    # è½¬æ¢å›RGB
    result_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB)
    
    return result_img, face_images

# æ˜¾ç¤ºç›¸ä¼¼å›¾ç‰‡ç»“æœ
def display_similar_images(similar_images):
    if not similar_images:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
        return
    
    st.subheader(f"ç›¸ä¼¼å›¾ç‰‡æ£€ç´¢ç»“æœ (Top {len(similar_images)})")
    
    # æ˜¾ç¤ºç»“æœ
    cols = st.columns(min(len(similar_images), 3))
    for i, result in enumerate(similar_images):
        with cols[i % 3]:
            image_path = Path(result['path'])
            if image_path.exists():
                img = Image.open(image_path)
                st.image(img, caption=f"ç›¸ä¼¼åº¦: {result['similarity']:.2f}", use_container_width=True)
                
                # æ˜¾ç¤ºæ—¶é—´æˆ³ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
                if 'timestamp' in result:
                    time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(result['timestamp']))
                    st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")

# æ˜¾ç¤ºç›¸ä¼¼äººè„¸ç»“æœ
def display_similar_faces(similar_faces_results, detected_faces):
    if not similar_faces_results:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼äººè„¸")
        return
    
    st.subheader("äººè„¸ç›¸ä¼¼åº¦æ£€ç´¢ç»“æœ")
    
    # ä¸ºæ¯ä¸ªæŸ¥è¯¢äººè„¸æ˜¾ç¤ºå…¶åŒ¹é…ç»“æœ
    for result in similar_faces_results:
        query_face_idx = result['query_face_index']
        matches = result['matches']
        
        if matches:
            st.write(f"##### æŸ¥è¯¢äººè„¸ #{query_face_idx + 1} çš„åŒ¹é…ç»“æœ:")
            
            # æ˜¾ç¤ºæŸ¥è¯¢äººè„¸
            query_face = detected_faces[query_face_idx]
            query_face_pil = Image.fromarray(query_face)
            
            # åˆ›å»ºåˆ—æ¥æ˜¾ç¤ºæŸ¥è¯¢äººè„¸å’ŒåŒ¹é…ç»“æœ
            cols = st.columns(1 + min(len(matches), 3))
            
            # æ˜¾ç¤ºæŸ¥è¯¢äººè„¸
            with cols[0]:
                st.image(query_face_pil, caption="æŸ¥è¯¢äººè„¸", use_container_width=True)
            
            # æ˜¾ç¤ºåŒ¹é…ç»“æœ
            for i, match in enumerate(matches):
                if i < len(cols) - 1:  # ç¡®ä¿ä¸è¶…å‡ºåˆ—æ•°
                    with cols[i + 1]:
                        face_path = Path(match['path'])
                        if face_path.exists():
                            match_face = Image.open(face_path)
                            st.image(match_face, caption=f"ç›¸ä¼¼åº¦: {match['similarity']:.2f}", use_container_width=True)
                            
                            # æ˜¾ç¤ºæ—¶é—´æˆ³ï¼ˆè½¬æ¢ä¸ºå¯è¯»æ ¼å¼ï¼‰
                            if 'timestamp' in match:
                                time_str = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(match['timestamp']))
                                st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")

# ä¿®æ”¹å›¾ç‰‡è½¬base64å‡½æ•°
def image_to_base64(image):
    # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ¨¡å¼
    if image.mode in ('RGBA', 'P'):
        image = image.convert('RGB')
    
    buffered = BytesIO()
    image.save(buffered, format="JPEG")
    return base64.b64encode(buffered.getvalue()).decode()

# ä¿®æ”¹analyze_with_groqå‡½æ•°
def analyze_with_groq(image, api_key):
    client_vision = Groq()
    client_vision.api_key = api_key
    
    try:
        # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ¨¡å¼
        if image.mode in ('RGBA', 'P'):
            image = image.convert('RGB')
            
        base64_image = image_to_base64(image)
        
        response = client_vision.chat.completions.create(
            model="meta-llama/llama-4-scout-17b-16e-instruct",
            messages=[
                {
                    "role": "system",
                    "content": '''è¯·æŒ‰ä»¥ä¸‹è§„åˆ™åˆ†æå›¾ç‰‡å†…å®¹ï¼š
                    é¦–å…ˆåˆ¤æ–­å›¾ç‰‡ä¸»è¦å†…å®¹æ˜¯å¦ä¸ºæ–‡å­—å†…å®¹ï¼ˆå³å›¾ç‰‡ä¸­è¶…è¿‡ 50% çš„åŒºåŸŸä¸ºå¯è¯†åˆ«çš„æ–‡å­—ï¼Œå¦‚æ–‡æ¡£ã€æµ·æŠ¥ã€æ ‡è¯­ç­‰ï¼‰ï¼š
                    è‹¥æ˜¯ï¼šç›´æ¥æå–å›¾ç‰‡ä¸­çš„æ‰€æœ‰å¯è¯†åˆ«æ–‡å­—ï¼ˆæ— éœ€é¢å¤–æè¿°ï¼‰ã€‚
                    è‹¥å¦ï¼šè¯¦ç»†æè¿°å›¾ç‰‡å†…å®¹ï¼Œéœ€åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼ˆæ— ç›¸å…³ä¿¡æ¯åˆ™æ ‡æ³¨ "æœªæåŠ" æˆ– "æ— æ³•åˆ¤æ–­"ï¼‰ï¼š
                    # ä¸»ä½“å†…å®¹ï¼šæ¸…æ™°æè¿°ç”»é¢ä¸­çš„ç‰©ä½“ã€åœºæ™¯ã€åŠ¨ä½œç­‰ç»†èŠ‚ï¼ˆå¦‚ "å®¤å†…ä¼šè®®åœºæ™¯ï¼Œ3 äººå›´åè®¨è®ºï¼Œæ¡Œä¸Šæ‘†æ”¾ç¬”è®°æœ¬ç”µè„‘å’Œæ–‡ä»¶"ï¼‰ã€‚
                    # æ–‡å­—ä¿¡æ¯ï¼šå°½å¯èƒ½æå–å›¾ç‰‡é‡Œçš„æ–‡å­—ä¿¡æ¯
                    # äººç‰©ä¿¡æ¯ï¼ˆè‹¥åŒ…å«äººï¼‰ï¼š
                    ãƒ»äººå¤´æ•°ï¼šX äººï¼ˆç²¾ç¡®è®¡æ•°ï¼‰ã€‚
                    ãƒ»æ€§åˆ«æ¯”ä¾‹ï¼šç”·æ€§ X äººï¼Œå¥³æ€§ Y äººï¼Œæ€§åˆ«ä¸æ˜ Z äººï¼ˆæ— äººç‰©åˆ™å¡« 0ï¼‰ã€‚
                    ãƒ»ç§æ—æ¯”ä¾‹ï¼šé»‘äººï¼Œç™½äººï¼Œäºšæ´²äººç­‰
                    ãƒ»å¹´é¾„æ®µï¼šæŒ‰å„¿ç«¥ï¼ˆï¼œ12 å²ï¼‰ã€é’å¹´ï¼ˆ12-35 å²ï¼‰ã€ä¸­å¹´ï¼ˆ36-60 å²ï¼‰ã€è€å¹´ï¼ˆï¼60 å²ï¼‰åˆ†ç±»ï¼Œæ ‡æ³¨å„å¹´é¾„æ®µäººæ•°ï¼ˆå¦‚ "é’å¹´ 2 äººï¼Œä¸­å¹´ 1 äºº"ï¼‰ã€‚
                    # æ‰€åœ¨åœºæ™¯ï¼šå…·ä½“æè¿°ç¯å¢ƒï¼ˆå¦‚ "åŒ»é™¢å€™è¯ŠåŒº""æˆ·å¤–å…¬å›­""å®éªŒå®¤" ç­‰ï¼‰ï¼Œéœ€åŒ…å«å®¤å†… / å®¤å¤–ã€åœ°ç‚¹ç‰¹å¾ç­‰ç»†èŠ‚ã€‚
                    # Sanofi ç›¸å…³æ€§ï¼šæ˜ç¡®è¯´æ˜æ˜¯å¦å‡ºç° Sanofi æ ‡å¿—ï¼ˆå¦‚ LOGOã€å“ç‰Œåç§°ï¼‰ã€äº§å“ï¼ˆå¦‚è¯å“åŒ…è£…ã€å®£ä¼ ææ–™ï¼‰ã€æ–‡å­—æåŠï¼ˆå¦‚ "Sanofi""èµ›è¯ºè²" å­—æ ·ï¼‰æˆ–ç›¸å…³åœºæ™¯ï¼ˆå¦‚ Sanofi æ´»åŠ¨ã€åˆä½œé¡¹ç›®ç­‰ï¼‰ï¼Œè‹¥æ— åˆ™æ ‡æ³¨ "æ— ç›¸å…³å…ƒç´ "ã€‚
                    ä½¿ç”¨è¯´æ˜ï¼š
                    è‹¥å›¾ç‰‡ä¸ºçº¯æ–‡å­—ï¼ˆå¦‚åˆåŒã€è¯´æ˜ä¹¦ï¼‰ï¼Œè¾“å‡ºä»…åŒ…å«æå–çš„æ–‡å­—å†…å®¹ã€‚
                    è‹¥å›¾ç‰‡ä¸ºéæ–‡å­—å†…å®¹ï¼ˆå¦‚ç…§ç‰‡ã€æ’ç”»ï¼‰ï¼ŒæŒ‰ä¸Šè¿°æ ¼å¼åˆ†ç‚¹è¯¦ç»†æè¿°ï¼Œäººç‰©ä¿¡æ¯éœ€ä¸¥æ ¼æŒ‰è¦æ±‚åˆ†ç±»ç»Ÿè®¡ã€‚
                    ç¡®ä¿è¯­è¨€ç®€æ´å‡†ç¡®ï¼Œé¿å…ä¸»è§‚æ¨æ–­ï¼Œä»…åŸºäºå›¾ç‰‡å¯è§å†…å®¹è¾“å‡ºã€‚
                    æ–°å¢éªŒè¯é¡¹
                    é’ˆå¯¹ä»¥ä¸‹åœºæ™¯ï¼Œæ˜ç¡®æ ‡æ³¨å·¥å…·æ˜¯å¦æ­£ç¡®è¯†åˆ«ï¼ˆéœ€åŸºäºå›¾ç‰‡å¯è§å†…å®¹åˆ¤æ–­ï¼Œè€Œéä¸»è§‚æ¨æµ‹ï¼‰ï¼š
                    # å›¾ç‰‡é£æ ¼ï¼š æ˜¯å¦ä¸ºçœŸå®ç…§ç‰‡ï¼Œè¿˜æ˜¯æŸç§é£æ ¼å›¾åƒ, æ˜¯å¦ä¸ºè§†é¢‘æˆªå›¾æˆ–è€…ç¿»æ‹ï¼Œè¿˜æ˜¯è‡ªæ‹
                    # ç©ºåœºæ™¯è¯†åˆ«ï¼šå›¾ç‰‡æ˜¯å¦ä¸ºæ— äººã€æ— æ˜æ˜¾ç‰©ä½“çš„ç©ºåœºæ™¯ï¼ˆå¦‚ç©ºç™½å¢™é¢ã€çº¯è‰²èƒŒæ™¯ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ / å¦ / æ— æ³•åˆ¤æ–­ï¼ˆè‹¥æœ‰ç‰©ä½“æˆ–äººç‰©ï¼Œæ ‡æ³¨ "éç©ºåœºæ™¯"ï¼‰ã€‚
                    # ä¼šè®®æ— å…³åœºæ™¯è¯†åˆ«ï¼šåœºæ™¯æ˜¯å¦ä¸ä¼šè®®æ— ç›´æ¥å…³è”ï¼ˆå¦‚çº¯è‡ªç„¶é£æ™¯ã€å•ç‹¬é™ç‰©ã€å¨±ä¹åœºæ‰€ç­‰ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆä¼šè®®æ— å…³ï¼‰/ å¦ï¼ˆå¯èƒ½ä¸ä¼šè®®ç›¸å…³ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # äººæ•°æ•æ‰ï¼šå·¥å…·æ˜¯å¦å‡†ç¡®ç»Ÿè®¡å›¾ç‰‡ä¸­çš„äººå¤´æ•°ï¼Ÿï¼ˆè‹¥æœ‰äººç‰©ï¼Œéœ€ä¸å®é™…è®¡æ•°ä¸€è‡´ï¼‰
                    # å®é™…äººå¤´æ•°ï¼šX äººï¼›å·¥å…·è¯†åˆ«ç»“æœï¼šX äººï¼ˆä¸€è‡´ / ä¸ä¸€è‡´ï¼‰ã€‚
                    # ä¼šè®®è®¾å¤‡è¯†åˆ«ï¼šæ˜¯å¦æ­£ç¡®è¯†åˆ«ä¼šè®®å¿…è¦è®¾å¤‡ï¼ˆå¦‚æŠ•å½±ä»ªã€ç”µè„‘ã€éº¦å…‹é£ã€ç™½æ¿ã€ä¼šè®®æ¡Œç­‰ï¼‰ï¼Ÿ
                    # è¯†åˆ«è®¾å¤‡ï¼šè‹¥æœ‰ï¼Œåˆ—å‡ºå…·ä½“è®¾å¤‡åç§°ï¼ˆå¦‚ "æŠ•å½±ä»ªã€ç¬”è®°æœ¬ç”µè„‘"ï¼‰ï¼›è‹¥æ— ï¼Œæ ‡æ³¨ "æœªè¯†åˆ«åˆ°ä¼šè®®è®¾å¤‡"ã€‚
                    # ä¼šè®®ç…§ç‰‡ç¿»æ‹è¯†åˆ«ï¼šå›¾ç‰‡æ˜¯å¦ä¸ºä¼šè®®ç…§ç‰‡çš„ç¿»æ‹ï¼ˆå¦‚å¯¹å±å¹•ã€çº¸è´¨ç…§ç‰‡çš„äºŒæ¬¡æ‹æ‘„ï¼Œå¯èƒ½å­˜åœ¨åå…‰ã€å˜å½¢ç­‰ç‰¹å¾ï¼‰ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆç¿»æ‹ç…§ç‰‡ï¼‰/ å¦ï¼ˆéç¿»æ‹ç…§ç‰‡ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # éæ­£å¸¸ä¼šè®®åœºæ‰€è¯†åˆ«ï¼šåœºæ™¯æ˜¯å¦ä¸ºéæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå¦‚å¨±ä¹åœºæ‰€ã€å¼€æ”¾åŒºåŸŸã€å®¶åº­ç¯å¢ƒç­‰ï¼‰ï¼Ÿ
                    # åœºæ‰€ç±»å‹ï¼šæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå¦‚ä¼šè®®å®¤ï¼‰/ éæ­£å¸¸ä¼šè®®åœºæ‰€ï¼ˆå…·ä½“æè¿°ï¼Œå¦‚ "KTV åŒ…å¢""å…¬å›­è‰åª"ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    # å‚ä¼šäººéœ²è„¸è¯†åˆ«ï¼šæ˜¯å¦æ­£ç¡®è¯†åˆ«äººç‰©éœ²è„¸æƒ…å†µï¼ˆå…¨éƒ¨éœ²è„¸ / éƒ¨åˆ†æœªéœ²è„¸ / å…¨éƒ¨æœªéœ²è„¸ï¼‰ï¼Ÿ
                    # éœ²è„¸æƒ…å†µï¼šè‹¥æœ‰äººç‰©ï¼Œæ ‡æ³¨ "å…¨éƒ¨éœ²è„¸" æˆ– "X äººæœªéœ²è„¸"ï¼›æ— äººç‰©åˆ™æ ‡æ³¨ "æ— äººç‰©"ã€‚
                    # éåˆè§„ç‰©ä»¶è¯†åˆ«ï¼šæ˜¯å¦å‡ºç°ä¸ä¼šè®®æ— å…³çš„éåˆè§„ç‰©ä»¶ï¼ˆå¦‚é…’ç“¶ã€éº»å°†æ¡Œã€é…’æ¯ï¼Œæ¸¸æˆæœºç­‰ï¼‰ï¼Œéœ€è¦ç€é‡æ£€æŸ¥ï¼Ÿ
                    # è¯†åˆ«ç»“æœï¼šæ˜¯ï¼ˆå…·ä½“ç‰©ä»¶ï¼šXXXï¼‰/ å¦ï¼ˆæ— éåˆè§„ç‰©ä»¶ï¼‰/ æ— æ³•åˆ¤æ–­ã€‚
                    '''
                },
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "image_url",
                            "image_url": {
                                "url": f"data:image/jpeg;base64,{base64_image}"
                            }
                        },
                        {
                            "type": "text",
                            "text": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡"
                        }
                    ]
                }
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"Groqåˆ†æå‡ºé”™: {str(e)}"

# æ·»åŠ OpenAIå®¡æŸ¥æ€»ç»“å‡½æ•°
def summarize_with_openai(cv_results, groq_analysis, api_key, image_dimensions=None):
    client = OpenAI(api_key=api_key,base_url="https://generativelanguage.googleapis.com/v1beta/")
    try:
        # å°†å›¾ç‰‡å°ºå¯¸ä¿¡æ¯æ·»åŠ åˆ°CVç»“æœä¸­
        if image_dimensions:
            cv_results["image_dimensions"] = image_dimensions
            
        response = client.chat.completions.create(
            model="gemini-2.0-flash",
            messages=[
                {
                    "role": "system",
                    "content": """ä½ æ˜¯ä¸€ä½ä¸¥è°¨çš„è¯å‚å®¡æŸ¥å…³å‘˜ã€‚è¯·æ ¹æ®CVæ£€æµ‹ç»“æœå’ŒAIåˆ†æç»“æœï¼Œç”Ÿæˆä¸€ä»½ç®€æ´çš„åˆè§„æ€§åˆ†ææŠ¥å‘Šã€‚
                    é‡ç‚¹å…³æ³¨ï¼š
                    0. æ˜¯å¦æœ‰sanofiçš„logoï¼Œæˆ–è€…æ˜ç¡®å’Œsanofiç›¸å…³çš„å› ç´ ï¼Œå¯¹äºæ˜ç¡®çš„sanofi logoï¼Œä¸å»ºè®®ç›´æ¥åˆ¤æ–­ä¸ºè¿è§„
                    1. åœºæ™¯åˆè§„æ€§ï¼ˆæ˜¯å¦ä¸ºæ­£å¼ä¼šè®®åœºæ‰€),æ ¹æ®ä¸»ä½“å†…å®¹ä¿¡æ¯åˆ¤æ–­, å®¤å¤–åœºæ™¯ä¸è¢«å…è®¸
                    2. äººå‘˜æƒ…å†µï¼ˆäººæ•°ã€ç€è£…ã€è¡Œä¸ºæ˜¯å¦å¾—ä½“ï¼‰ï¼Œè‹¥äººæ•°ä¸º0ï¼Œä¸è¢«å…è®¸
                    3. ç‰©å“åˆè§„æ€§ï¼ˆæ˜¯å¦æœ‰è¿è§„ç‰©å“å¦‚é…’ç²¾é¥®å“ï¼‰
                    4. å›¾ç‰‡é£æ ¼æ˜¯å¦ä¸ºçœŸå®ï¼Œä»…çœŸå®ç…§ç‰‡è¢«å…è®¸ã€‚è‹¥æ˜¯æˆªå›¾æˆ–è€…ç¿»æ‹ï¼Œä»¥åŠè‡ªæ‹ï¼Œä¸è¢«å…è®¸
                    5. æ–‡å­—ä¿¡æ¯åˆè§„æ€§:æ€»ç»“AIåˆ†æç»“æœå†…çš„æå–å›¾ç‰‡å†…å®¹ä¸­çš„æ–‡å­—ä¿¡æ¯ï¼Œå¦‚æœæœ‰çš„è¯ï¼Œåˆ¤æ–­æ˜¯å¦åˆé€‚
                    6. å›¾ç‰‡å°ºå¯¸: æ˜¯å¦å­˜åœ¨æ‹‰ä¼¸æˆ–è€…å‹ç¼©ï¼Œæ˜¯å¦ä¸ºéæ ‡å‡†ç…§ç‰‡å°ºå¯¸ï¼Œè‹¥éæ ‡å‡†å°ºå¯¸ï¼Œä»…æé†’å¯èƒ½è¢«è£å‰ªï¼Œä¸å»ºè®®ç›´æ¥åˆ¤æ–­ä¸ºè¿è§„ï¼Œå¸¸è§çš„å°ºå¯¸ä¸ºï¼š
                    1.0: "1:1 (æ­£æ–¹å½¢)",
                    1.33: "4:3 (å¸¸è§å±å¹•)",
                    1.5: "3:2 (ä¼ ç»Ÿç›¸æœº)",
                    1.78: "16:9 (å®½å±)",
                    1.85: "1.85:1 (ç”µå½±)",
                    2.35: "2.35:1 (ç”µå½±å®½é“¶å¹•)"
                    7. æ•´ä½“è¯„ä¼°ï¼ˆæ˜¯å¦å»ºè®®é€šè¿‡å®¡æ ¸ï¼‰
                    è¯·ä»¥è¡¨æ ¼å½¢å¼è¾“å‡ºï¼Œè¡¨æ ¼éœ€è¦åŒ…å«å®¡æ ¸é¡¹ç›®ï¼Œå®¡æ ¸ç»“æœï¼Œå®¡æ ¸åŸå› ä¸‰åˆ—ï¼Œå¯¹äºè¿è§„æƒ…å†µï¼ŒåŠ ç²—å­—ä½“ï¼Œä½¿ç”¨markdownæ ¼å¼ã€‚"""
                },
                {
                    "role": "user",
                    "content": f"CVæ£€æµ‹ç»“æœï¼š{cv_results}\n\nAIåˆ†æç»“æœï¼š{groq_analysis}"
                }
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"OpenAIæ€»ç»“å‡ºé”™: {str(e)}"

# æ·»åŠ å“ˆå¸Œè®¡ç®—å‡½æ•°
def get_image_hash(image_path):
    """
    è®¡ç®—å›¾ç‰‡çš„å“ˆå¸Œå€¼ç”¨äºå»é‡
    """
    try:
        with open(image_path, 'rb') as f:
            return hashlib.md5(f.read()).hexdigest()
    except Exception as e:
        print(f"[ERROR] Error hashing image {image_path}: {e}")
        return None

# æ·»åŠ äººè„¸èšç±»å‡½æ•°
def face_clustering(input_folder, output_folder, detection_method="mtcnn", tolerance=0.6, min_cluster_size=3,
                  dim_reduction=True, n_components=50):
    """
    Cluster similar faces from a collection of images and organize them into folders
    
    Parameters:
    - input_folder: Path to folder containing images
    - output_folder: Path to output the clustered images
    - detection_method: Ignored (always uses MTCNN)
    - tolerance: How strict the face matching should be (lower is stricter)
    - min_cluster_size: Minimum number of images needed to form a cluster
    - dim_reduction: Whether to apply dimensionality reduction (default: True)
    - n_components: Number of components for dimensionality reduction (default: 50)
    """
    # Create output directory if it doesn't exist
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    
    # Create a directory for images where no faces were detected
    no_faces_dir = os.path.join(output_folder, "no_faces_detected")
    if not os.path.exists(no_faces_dir):
        os.makedirs(no_faces_dir)
    
    # Create a directory for unclustered faces (faces that don't belong to any cluster)
    unclustered_dir = os.path.join(output_folder, "unclustered_faces")
    if not os.path.exists(unclustered_dir):
        os.makedirs(unclustered_dir)
    
    print(f"[INFO] Processing images from {input_folder}")
    
    # Get all image files from the input folder
    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
    image_paths = []
    for root, _, files in os.walk(input_folder):
        for file in files:
            if any(file.lower().endswith(ext) for ext in image_extensions):
                image_paths.append(os.path.join(root, file))
    
    print(f"[INFO] Found {len(image_paths)} images")
    
    # Initialize MTCNN detector
    face_detector = MTCNN()
    
    # Initialize lists to store data
    known_embeddings = []
    known_image_paths = []
    
    # Process each image
    for (i, image_path) in enumerate(image_paths):
        print(f"[INFO] Processing image {i+1}/{len(image_paths)}")
        
        try:
            # Load the image
            image = cv2.imread(image_path)
            if image is None:
                print(f"[WARNING] Could not read image {image_path}")
                continue
                
            # Convert BGR to RGB
            rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            
            # Detect faces using MTCNN
            faces = face_detector.detect_faces(rgb)
            
            # If no faces were found, move to the no_faces directory
            if len(faces) == 0:
                shutil.copy2(image_path, os.path.join(no_faces_dir, os.path.basename(image_path)))
                continue
            
            # Process each detected face
            for face in faces:
                if face['confidence'] < 0.8:  # Confidence threshold
                    continue
                
                # Extract face bounding box
                x, y, w, h = face['box']
                x = max(x, 0)
                y = max(y, 0)
                face_crop = rgb[y:y+h, x:x+w]
                
                # Ensure face crop is valid
                if face_crop.size == 0 or face_crop.shape[0] < 30 or face_crop.shape[1] < 30:
                    continue
                
                # Generate face embedding using DeepFace
                try:
                    embedding = DeepFace.represent(face_crop, model_name="Facenet", enforce_detection=False)[0]["embedding"]
                    known_embeddings.append(embedding)
                    known_image_paths.append(image_path)
                except Exception as e:
                    print(f"[ERROR] Error generating embedding for face in {image_path}: {e}")
                    continue
        
        except Exception as e:
            print(f"[ERROR] Error processing {image_path}: {e}")
    
    print(f"[INFO] Detected {len(known_embeddings)} faces in total")
    
    # Cluster the faces
    if len(known_embeddings) > 0:
        print("[INFO] Clustering faces...")
        
        # Convert embeddings to numpy array
        data = np.array(known_embeddings)
        
        # Apply dimensionality reduction if enabled
        if dim_reduction and len(data) > n_components:
            print(f"[INFO] Applying dimensionality reduction from {data.shape[1]} to {n_components} dimensions")
            from sklearn.decomposition import PCA
            
            # Initialize and fit PCA
            pca = PCA(n_components=n_components, random_state=42)
            data = pca.fit_transform(data)
            
            # Calculate and display explained variance
            explained_variance = sum(pca.explained_variance_ratio_) * 100
            print(f"[INFO] Explained variance with {n_components} components: {explained_variance:.2f}%")
        
        # Normalize the data
        print("[INFO] Normalizing feature vectors")
        from sklearn.preprocessing import StandardScaler
        scaler = StandardScaler()
        data = scaler.fit_transform(data)
        
        # Perform DBSCAN clustering
        clt = DBSCAN(metric="euclidean", n_jobs=-1, eps=tolerance, min_samples=min_cluster_size)
        clt.fit(data)
        
        # Get the number of unique clusters
        label_ids = np.unique(clt.labels_)
        num_unique_faces = len(np.where(label_ids > -1)[0])
        
        print(f"[INFO] Found {num_unique_faces} unique faces")
        
        # Create a folder for each cluster and copy the images there
        for label_id in label_ids:
            # If the cluster is -1, it's noise (unclustered)
            if label_id == -1:
                print("[INFO] Copying unclustered faces...")
                # Get indices of faces in this cluster
                indices = np.where(clt.labels_ == label_id)[0]
                
                # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„å›¾ç‰‡å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
                processed_hashes = set()
                
                # Copy each image to the unclustered directory
                for idx in indices:
                    image_path = known_image_paths[idx]
                    # è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
                    image_hash = get_image_hash(image_path)
                    
                    # å¦‚æœå›¾ç‰‡å·²ç»å¤„ç†è¿‡ï¼ˆé‡å¤ï¼‰ï¼Œåˆ™è·³è¿‡
                    if image_hash and image_hash in processed_hashes:
                        print(f"[INFO] Skipping duplicate image: {os.path.basename(image_path)}")
                        continue
                    
                    # æ·»åŠ å“ˆå¸Œå€¼åˆ°å·²å¤„ç†é›†åˆ
                    if image_hash:
                        processed_hashes.add(image_hash)
                        
                    shutil.copy2(image_path, os.path.join(unclustered_dir, os.path.basename(image_path)))
            else:
                # Create a directory for this cluster
                cluster_dir = os.path.join(output_folder, f"person_{label_id}")
                if not os.path.exists(cluster_dir):
                    os.makedirs(cluster_dir)
                
                # Get indices of faces in this cluster
                indices = np.where(clt.labels_ == label_id)[0]
                
                # ç”¨äºå­˜å‚¨å·²å¤„ç†çš„å›¾ç‰‡å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
                processed_hashes = set()
                # å­˜å‚¨å·²å¤„ç†ï¼ˆå»é‡åï¼‰çš„å›¾ç‰‡è·¯å¾„
                unique_image_paths = []
                
                # Copy each image to this cluster's directory
                for idx in indices:
                    image_path = known_image_paths[idx]
                    # è®¡ç®—å›¾ç‰‡å“ˆå¸Œå€¼
                    image_hash = get_image_hash(image_path)
                    
                    # å¦‚æœå›¾ç‰‡å·²ç»å¤„ç†è¿‡ï¼ˆé‡å¤ï¼‰ï¼Œåˆ™è·³è¿‡
                    if image_hash and image_hash in processed_hashes:
                        print(f"[INFO] Skipping duplicate image: {os.path.basename(image_path)}")
                        continue
                    
                    # æ·»åŠ å“ˆå¸Œå€¼åˆ°å·²å¤„ç†é›†åˆ
                    if image_hash:
                        processed_hashes.add(image_hash)
                        unique_image_paths.append(image_path)
                        
                    shutil.copy2(image_path, os.path.join(cluster_dir, os.path.basename(image_path)))
                
                print(f"[INFO] Cluster {label_id}: {len(indices)} faces, {len(unique_image_paths)} unique images after deduplication")
    
    print("[INFO] Face clustering completed!")
    return output_folder

# æ·»åŠ åˆ›å»ºè’™å¤ªå¥‡å›¾çš„å‡½æ•°
def create_montages(output_folder, montage_size=(150, 150), images_per_row=5):
    """
    Create montages for each cluster to visualize the results
    """
    montages_info = []
    
    # Get all subdirectories (clusters)
    cluster_dirs = [d for d in os.listdir(output_folder) 
                    if os.path.isdir(os.path.join(output_folder, d)) and d.startswith("person_")]
    
    montages_dir = os.path.join(output_folder, "montages")
    if not os.path.exists(montages_dir):
        os.makedirs(montages_dir)
    
    for cluster_dir in cluster_dirs:
        # Get all images in this cluster
        image_paths = [os.path.join(output_folder, cluster_dir, f) 
                      for f in os.listdir(os.path.join(output_folder, cluster_dir))
                      if os.path.isfile(os.path.join(output_folder, cluster_dir, f))]
        
        # Skip if no images in the cluster
        if len(image_paths) == 0:
            print(f"[INFO] No images found in cluster {cluster_dir}, skipping montage creation")
            continue
        
        # å»é‡ï¼šæ£€æŸ¥å›¾ç‰‡å†…å®¹æ˜¯å¦ç›¸åŒ
        unique_images = []
        image_hashes = set()
        
        for image_path in image_paths:
            image_hash = get_image_hash(image_path)
            
            # å¦‚æœè¿™ä¸ªå“ˆå¸Œå€¼æ²¡æœ‰è§è¿‡ï¼Œè¯´æ˜æ˜¯ä¸é‡å¤çš„å›¾ç‰‡
            if image_hash and image_hash not in image_hashes:
                image_hashes.add(image_hash)
                unique_images.append(image_path)
        
        print(f"[INFO] Cluster {cluster_dir}: {len(image_paths)} images, {len(unique_images)} unique for montage")
        
        # Load and resize images (åªä½¿ç”¨å»é‡åçš„å›¾ç‰‡)
        images = []
        for image_path in unique_images:
            image = cv2.imread(image_path)
            if image is not None:
                image = cv2.resize(image, montage_size)
                images.append(image)
        
        # Create montage
        if images:
            montage = build_montages(images, montage_size, (images_per_row, (len(images) // images_per_row) + 1))[0]
            
            # Save the montage
            montage_path = os.path.join(montages_dir, f"{cluster_dir}_montage.jpg")
            cv2.imwrite(montage_path, montage)
            
            print(f"[INFO] Created montage for {cluster_dir} with {len(images)} unique images")
            
            # æ·»åŠ è’™å¤ªå¥‡è·¯å¾„å’Œä¿¡æ¯åˆ°è¿”å›åˆ—è¡¨
            montages_info.append({
                "path": montage_path,
                "cluster": cluster_dir,
                "images_count": len(images)
            })
        else:
            print(f"[INFO] No valid images for montage in {cluster_dir}")
    
    print("[INFO] Montage creation completed!")
    return montages_info

# ä¿®æ”¹process_face_for_advanced_searchå‡½æ•°ï¼Œä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸æ‰¾åˆ°æœ€ç›¸ä¼¼çš„èšç±»
def process_face_for_advanced_search(detected_faces, tolerance=0.6):
    # æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„èšç±»ç»“æœç›®å½•
    current_dir = os.path.dirname(os.path.abspath(__file__))
    
    # æŸ¥æ‰¾æ‰€æœ‰ä»¥clustered_faces_å¼€å¤´çš„ç›®å½•
    cluster_dirs = [d for d in os.listdir(current_dir) 
                   if os.path.isdir(os.path.join(current_dir, d)) and d.startswith("clustered_faces_")]
    
    if not cluster_dirs:
        st.warning("æœªåœ¨å½“å‰ç›®å½•ä¸‹æ‰¾åˆ°äººè„¸èšç±»æ•°æ®(clustered_faces_*)ï¼Œè¯·å…ˆè¿è¡Œç¦»çº¿èšç±»å·¥å…·")
        return []
    
    # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œä½¿ç”¨æœ€æ–°çš„èšç±»ç»“æœ
    cluster_dirs.sort(reverse=True)
    latest_cluster_dir = os.path.join(current_dir, cluster_dirs[0])
    
    # è’™å¤ªå¥‡å›¾ç‰‡è·¯å¾„
    montages_dir = os.path.join(latest_cluster_dir, "montages")
    if not os.path.exists(montages_dir):
        st.warning(f"æœªæ‰¾åˆ°è’™å¤ªå¥‡å›¾ç‰‡ç›®å½•: {montages_dir}")
        return []
    
    # è·å–æ‰€æœ‰äººè„¸èšç±»ç›®å½•
    person_dirs = [d for d in os.listdir(latest_cluster_dir) 
                  if os.path.isdir(os.path.join(latest_cluster_dir, d)) and d.startswith("person_")]
    
    if not person_dirs:
        st.warning("æœªæ‰¾åˆ°æœ‰æ•ˆçš„äººè„¸èšç±»ç»„")
        return []
    
    # Initialize MTCNN detector
    face_detector = MTCNN()
    
    # ç»“æœåˆ—è¡¨ï¼Œä¸ºæ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸å­˜å‚¨æœ€ç›¸ä¼¼çš„èšç±»
    face_best_matches = []
    
    # å¤„ç†æ¯ä¸ªæ£€æµ‹åˆ°çš„äººè„¸
    for face_idx, face in enumerate(detected_faces):
        try:
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºRGBæ ¼å¼
            if len(face.shape) == 3 and face.shape[2] == 3:
                rgb_face = face
            else:
                # å¦‚æœä¸æ˜¯3é€šé“å½©è‰²å›¾åƒï¼Œè·³è¿‡
                continue
                
            # Generate embedding for the detected face using DeepFace
            current_face_embedding = DeepFace.represent(rgb_face, model_name="Facenet", enforce_detection=False)[0]["embedding"]
            
            # ä¸ºå½“å‰äººè„¸å¯»æ‰¾æœ€ç›¸ä¼¼çš„èšç±»
            best_match = None
            best_similarity = 0
            
            for person_dir in person_dirs:
                person_path = os.path.join(latest_cluster_dir, person_dir)
                # è·å–è¯¥èšç±»ä¸­çš„æ‰€æœ‰å›¾ç‰‡
                face_images = [f for f in os.listdir(person_path) 
                              if os.path.isfile(os.path.join(person_path, f)) and 
                              f.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp'))]
                
                if not face_images:
                    continue
                
                # ä»è¯¥èšç±»ä¸­éšæœºé€‰å–å‡ å¼ å›¾ç‰‡è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—
                sample_size = min(5, len(face_images))
                sample_images = random.sample(face_images, sample_size)
                
                cluster_embeddings = []
                for img_file in sample_images:
                    img_path = os.path.join(person_path, img_file)
                    try:
                        # è¯»å–å›¾ç‰‡
                        img = cv2.imread(img_path)
                        if img is None:
                            continue
                        
                        # è½¬æ¢ä¸ºRGB
                        rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                        
                        # æ£€æµ‹äººè„¸
                        faces = face_detector.detect_faces(rgb_img)
                        if faces:
                            # Use the first detected face
                            x, y, w, h = faces[0]['box']
                            x = max(x, 0)
                            y = max(y, 0)
                            face_crop = rgb_img[y:y+h, x:x+w]
                            
                            # Generate embedding
                            if face_crop.size > 0 and face_crop.shape[0] >= 30 and face_crop.shape[1] >= 30:
                                embedding = DeepFace.represent(face_crop, model_name="Facenet", enforce_detection=False)[0]["embedding"]
                                cluster_embeddings.append(embedding)
                    except Exception as e:
                        print(f"å¤„ç†å›¾ç‰‡ {img_path} æ—¶å‡ºé”™: {e}")
                
                if not cluster_embeddings:
                    continue
                
                # è®¡ç®—å½“å‰äººè„¸ä¸èšç±»ä¸­æ‰€æœ‰äººè„¸çš„ç›¸ä¼¼åº¦
                similarities = []
                for cluster_embedding in cluster_embeddings:
                    similarity = cosine_similarity([current_face_embedding], [cluster_embedding])[0][0]
                    similarities.append(similarity)
                
                # å–æœ€é«˜ç›¸ä¼¼åº¦
                if similarities:
                    max_similarity = max(similarities)
                    
                    # å¦‚æœç›¸ä¼¼åº¦é«˜äºé˜ˆå€¼ä¸”é«˜äºå½“å‰æœ€é«˜ç›¸ä¼¼åº¦
                    if max_similarity >= tolerance and max_similarity > best_similarity:
                        # æŸ¥æ‰¾å¯¹åº”çš„è’™å¤ªå¥‡å›¾
                        montage_path = os.path.join(montages_dir, f"{person_dir}_montage.jpg")
                        if os.path.exists(montage_path):
                            best_similarity = max_similarity
                            best_match = {
                                "path": montage_path,
                                "cluster": person_dir,
                                "similarity": max_similarity,
                                "images_count": len(face_images),
                                "face_idx": face_idx
                            }
            
            # å¦‚æœæ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼Œæ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            if best_match:
                face_best_matches.append(best_match)
                
        except Exception as e:
            print(f"å¤„ç†äººè„¸ #{face_idx} æ—¶å‡ºé”™: {e}")
    
    return face_best_matches

def check_image_dimensions(image):
    width, height = image.size

    # æ ‡å‡†çš„ç…§ç‰‡å°ºå¯¸åˆ—è¡¨ï¼ˆå»æ‰æ˜æ˜¾ä¸æ ‡å‡†çš„ï¼Œæ¯”å¦‚ 1920x1281ï¼‰
    standard_sizes = [
        (640, 480), (800, 600), (1024, 768), (1280, 720),
        (1280, 800), (1280, 1024), (1366, 768), (1440, 900),
        (1600, 900), (1600, 1200), (1920, 1080), (2048, 1536),
        (2560, 1440), (3840, 2160), (4096, 2160), (7680, 4320)
    ]

    # è®¾ç½®å®¹å·®èŒƒå›´ï¼ˆå•ä½ï¼šåƒç´ ï¼‰
    tolerance = 10

    def within_tolerance(w1, h1, w2, h2, tol):
        return abs(w1 - w2) <= tol and abs(h1 - h2) <= tol

    # æ£€æŸ¥æ˜¯å¦ä¸ºæ ‡å‡†å°ºå¯¸æˆ–ç¿»è½¬ç‰ˆæœ¬ï¼ŒåŠ å…¥å®¹å·®
    is_standard = any(
        within_tolerance(width, height, std_w, std_h, tolerance) or
        within_tolerance(height, width, std_w, std_h, tolerance)
        for std_w, std_h in standard_sizes
    )

    # è·å–å®½é«˜æ¯”
    aspect_ratio = width / height

    # å¸¸è§å®½é«˜æ¯”åŠåç§°
    common_ratios = {
        1.0: "1:1 (æ­£æ–¹å½¢)",
        4/3: "4:3 (å¸¸è§å±å¹•)",
        3/2: "3:2 (ä¼ ç»Ÿç›¸æœº)",
        16/9: "16:9 (å®½å±)",
        1.85: "1.85:1 (ç”µå½±)",
        2.35: "2.35:1 (ç”µå½±å®½é“¶å¹•)"
    }

    # åˆ¤æ–­æœ€æ¥è¿‘çš„å®½é«˜æ¯”
    closest_ratio = min(common_ratios, key=lambda x: abs(x - aspect_ratio))
    ratio_name = common_ratios[closest_ratio] if abs(closest_ratio - aspect_ratio) < 0.1 else "éæ ‡å‡†"

    return {
        "width": width,
        "height": height,
        "aspect_ratio": round(aspect_ratio, 4),
        "ratio_name": ratio_name,
        "is_standard_size": is_standard
    }

# ... existing code ...
def extract_text_from_analysis(analysis):
    client = OpenAI(api_key=os.getenv("GEMINI_API_KEY"),base_url="https://generativelanguage.googleapis.com/v1beta/")
    try: 
        response = client.chat.completions.create(
            model="gemini-2.0-flash",
            messages=[
                {
                    "role": "system",
                    "content": "ä½ çš„ä»»åŠ¡æ˜¯ä»AIåˆ†æç»“æœä¸­æå–å†…å®¹é‡Œçš„æ–‡å­—ä¿¡æ¯çš„éƒ¨åˆ†ï¼Œå¹¶æ˜ç¡®æ’é™¤ä¸éœ€è¦çš„ä¿¡æ¯ï¼Œç›´æ¥è¿”å›ç»“æœå³å¯ã€‚ ä»…è¾“å‡ºåˆ†æä¸­æåˆ°çš„å›¾ç‰‡é‡ŒåŒ…å«çš„æ–‡å­—å†…å®¹ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šæˆ–è¯„è®ºã€‚å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ–‡å­—ä¿¡æ¯ï¼Œè¯·å›å¤'æ— æ–‡å­—å†…å®¹'ã€‚"
                },
                {
                    "role": "user",
                    "content": f"è¯·ä»ä»¥ä¸‹AIåˆ†æä¸­æå–æ‰€æœ‰æ–‡å­—ä¿¡æ¯ï¼š\n\n{analysis}"
                }
            ]
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"æå–æ–‡å­—ä¿¡æ¯å¤±è´¥: {str(e)}"
    
def initialize_text_vectordb():
    text_vector_file = data_dir / "vectors" / "text_vectors.pkl"

    if os.path.exists(text_vector_file):
        with open(text_vector_file, "rb") as f:
            text_db = pickle.load(f)
    else:
   
   
        return {}
    
    return text_db
    
def save_text_vector_db(text_db):
    text_vector_file = data_dir / "vectors" / "text_vectors.pkl"

    with open(text_vector_file, "wb") as f:
        pickle.dump(text_db, f)

def update_text_vector_db(image_id, text_content):
    if not text_content or text_content == "æ— æ–‡å­—å†…å®¹":
        return
    
    # åˆå§‹åŒ–æ–‡å­—å‘é‡æ•°æ®åº“
    text_db = initialize_text_vectordb()

    try:

        from sentence_transformers import SentenceTransformer


        @st.cache_resource
        def load_sentence_transformer():
            return SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
        
        model = load_sentence_transformer()


        text_vector = model.encode(text_content)

        
        text_db[image_id] = {
            'vector': text_vector,
            'text': text_content
        }

        # ä¿å­˜æ›´æ–°åçš„æ•°æ®åº“
        save_text_vector_db(text_db)
    except Exception as e:
        st.error(f"ç”Ÿæˆæ–‡æœ¬å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
        print(f"ç”Ÿæˆæ–‡æœ¬å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
    
def search_similar_texts(image_id, top_k=3, threshold=0.5):
    text_db = initialize_text_vectordb()

    if not text_db or image_id not in text_db:
        return []
    
    query_vector = text_db[image_id]['vector']
    results = []


    for text_id, data in text_db.items():
        if text_id == image_id:
            continue
        
        db_vector = data['vector']

        try:


            similarity = np.dot(query_vector, db_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(db_vector))

            if similarity >= threshold:
                results.append({
                    'id': text_id,
                    'text': data['text'],
                    'similarity': float(similarity)
                })
        except Exception as e:
            print(f"è®¡ç®—æ–‡æœ¬å‘é‡ç›¸ä¼¼åº¦å¤±è´¥: {str(e)}")
            continue
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åº
    results.sort(key=lambda x: x['similarity'], reverse=True)


    return results[:top_k]

def display_similar_texts(similar_texts):
    if not similar_texts:
        st.info("æ²¡æœ‰æ‰¾åˆ°ç›¸ä¼¼çš„æ–‡æœ¬ã€‚")
        return
    
    st.subheader(f"ç›¸ä¼¼æ–‡å­—å†…å®¹æ£€æŸ¥ç»“æœ Top {len(similar_texts)}")

    #æ˜¾ç¤ºç»“æœ
    cols = st.columns(min(len(similar_texts), 3))
    for i,result in enumerate(similar_texts):
        with cols[i % 3]:
            text_id = result['id']


            image_db, _ = initialize_vector_db()

            if text_id in image_db:
                image_path = Path(image_db[text_id]['path'])
                if image_path.exists():
                    img = Image.open(image_path)
                    st.image(img, caption=f"ç›¸ä¼¼åº¦: {result['similarity']:.2f}", use_container_width=True)


                    if 'timestamp' in image_db[text_id]:
                        time_str = time.strftime("%Y-%m-%d %H:%M:%S", 
                                                 time.localtime(image_db[text_id]['timestamp']))
                        st.caption(f"ä¸Šä¼ æ—¶é—´: {time_str}")

                    
                    with st.expander("æŸ¥çœ‹æ–‡å­—å†…å®¹"):
                        st.write(result['text'])
                else:
                    st.error(f"å›¾ç‰‡æ–‡ä»¶ä¸å­˜åœ¨: {image_path}")
            else:
                st.warning(f"æœªæ‰¾åˆ°IDä¸º {text_id} çš„å›¾ç‰‡è®°å½•")
                st.write(result['text'])
                
    # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

# æ·»åŠ ä¸€ä¸ªæ¸…é™¤å¤„ç†çŠ¶æ€çš„å‡½æ•°
def reset_processed_state():
    st.session_state.processed = False
    st.session_state.detection_results = None
    st.session_state.processed_img = None
    st.session_state.detected_faces = []
    st.session_state.similar_images = []
    st.session_state.similar_faces_results = []
    st.session_state.advanced_face_search_done = False
    st.session_state.face_best_matches = []
    st.session_state.current_tolerance = 0.6
    st.session_state.similar_texts = []

# ä¿®æ”¹å›¾ç‰‡å‹ç¼©å‡½æ•°
def compress_image(image, max_size_kb=500, quality=50, max_dimension=1500):
    """
    å‹ç¼©å›¾ç‰‡ä½¿å…¶å°äºæŒ‡å®šå¤§å°
    
    å‚æ•°:
    - image: PIL Imageå¯¹è±¡
    - max_size_kb: æœ€å¤§æ–‡ä»¶å¤§å°(KB)
    - quality: åˆå§‹å‹ç¼©è´¨é‡
    - max_dimension: æœ€å¤§å°ºå¯¸é™åˆ¶
    
    è¿”å›:
    - å‹ç¼©åçš„PIL Imageå¯¹è±¡
    """
    # é¦–å…ˆé™åˆ¶å›¾ç‰‡å°ºå¯¸ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º
    width, height = image.size
    # å¦‚æœå›¾ç‰‡å¤ªå¤§ï¼Œå…ˆè°ƒæ•´å°ºå¯¸
    if max(width, height) > max_dimension:
        # è®¡ç®—è°ƒæ•´æ¯”ä¾‹
        ratio = max_dimension / max(width, height)
        new_width = int(width * ratio)
        new_height = int(height * ratio)
        # ä½¿ç”¨thumbnailèŠ‚çœå†…å­˜
        image.thumbnail((new_width, new_height), Image.LANCZOS)
    
    # ç¡®ä¿å›¾ç‰‡æ˜¯RGBæ¨¡å¼
    if image.mode != 'RGB':
        image = image.convert('RGB')
    
    # ç”¨äºå­˜å‚¨å‹ç¼©å›¾åƒçš„å­—èŠ‚æµ
    buffer = BytesIO()
    
    # ä¿å­˜å½“å‰è´¨é‡çš„å›¾åƒ
    image.save(buffer, format='JPEG', quality=quality, optimize=True)
    
    # æ£€æŸ¥å¤§å°
    current_size = buffer.tell() / 1024  # KB
    
    # å¦‚æœå¤§å°å·²ç»å°äºé™åˆ¶ï¼Œç›´æ¥è¿”å›
    if current_size <= max_size_kb:
        buffer.seek(0)
        return Image.open(buffer)
    
    # è®¡ç®—éœ€è¦çš„å‹ç¼©æ¯”
    compression_ratio = max_size_kb / current_size
    
    # å¦‚æœå›¾åƒè¿˜æ˜¯å¤ªå¤§ï¼Œè¿›ä¸€æ­¥é™ä½è´¨é‡
    new_quality = int(quality * compression_ratio)
    new_quality = max(10, min(new_quality, 60))  # é™åˆ¶è´¨é‡èŒƒå›´
    
    buffer = BytesIO()
    image.save(buffer, format='JPEG', quality=new_quality, optimize=True)
    current_size = buffer.tell() / 1024
    
    # å¦‚æœé™ä½è´¨é‡ä¸å¤Ÿï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å°ºå¯¸
    while current_size > max_size_kb and (width > 300 or height > 300):
        # æ¯æ¬¡ç¼©å°åˆ°80%
        width = int(width * 0.8)
        height = int(height * 0.8)
        
        # é‡æ–°è°ƒæ•´å›¾åƒå¤§å°
        image = image.resize((width, height), Image.LANCZOS)
        
        # ä¿å­˜å¹¶æ£€æŸ¥å¤§å°
        buffer = BytesIO()
        image.save(buffer, format='JPEG', quality=new_quality, optimize=True)
        current_size = buffer.tell() / 1024
    
    # å°†å­—èŠ‚æµè½¬æ¢å›PILå›¾åƒ
    buffer.seek(0)
    return Image.open(buffer)

# æ·»åŠ æ‰¹é‡å¤„ç†å‡½æ•°
def process_batch_images(uploaded_files):
    """æ‰¹é‡å¤„ç†ä¸Šä¼ çš„å›¾ç‰‡"""
    with st.spinner(f"æ­£åœ¨å¤„ç† {len(uploaded_files)} å¼ å›¾ç‰‡..."):
        # æ¸…ç©ºä¹‹å‰çš„æ‰¹é‡å¤„ç†æ•°æ®
        st.session_state.batch_images = []
        st.session_state.batch_faces = []
        st.session_state.batch_processed_images = []
        st.session_state.batch_image_vectors = {}
        st.session_state.batch_face_vectors = {}
        
        # åŠ è½½æ¨¡å‹
        model = load_model(selected_model)
        if detect_faces:
            face_detector = load_face_detector()
        if enable_similarity_search or enable_face_similarity:
            feature_extractor = load_feature_extractor()
            transform = get_transform()
        
        # å¤„ç†æ¯å¼ å›¾ç‰‡
        for i, uploaded_file in enumerate(uploaded_files):
            try:
                # è¯»å–å›¾ç‰‡
                image = Image.open(uploaded_file)
                
                # è·å–æ–‡ä»¶å¤§å°å¹¶å‹ç¼©å¤§å›¾ç‰‡
                file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)
                if file_size_mb > 2:
                    image = compress_image(image, max_size_kb=100)
                
                # æ·»åŠ åˆ°æ‰¹é‡å›¾ç‰‡åˆ—è¡¨
                st.session_state.batch_images.append(image)
                
                # ç‰©ä½“æ£€æµ‹
                results = model.predict(image)
                
                # è·å–é€‰æ‹©çš„ç±»åˆ«
                selected_classes = []
                if detect_person:
                    selected_classes.append("äºº")
                if detect_cup:
                    selected_classes.append("æ¯å­/é…’æ¯")
                if detect_bottle:
                    selected_classes.append("ç“¶å­")
                if detect_all:
                    selected_classes.append("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“")
                
                # äººè„¸æ£€æµ‹
                batch_faces_for_image = []
                if detect_faces:
                    original_image_for_face = np.array(image)
                    _, detected_faces = detect_face(original_image_for_face, face_detector, face_confidence)
                    batch_faces_for_image = detected_faces
                    st.session_state.batch_faces.append(detected_faces)
                
                # å¤„ç†æ£€æµ‹ç»“æœå¹¶æ ‡è®°
                processed_img = process_prediction(image, results, selected_classes, confidence)
                st.session_state.batch_processed_images.append(processed_img)
                
                # æå–å›¾ç‰‡ç‰¹å¾å‘é‡
                if enable_similarity_search:
                    image_features = extract_image_features(image, feature_extractor, transform)
                    image_id = f"batch_image_{i}"
                    st.session_state.batch_image_vectors[image_id] = {
                        'vector': image_features,
                        'index': i
                    }
                
                # æå–äººè„¸ç‰¹å¾å‘é‡
                if enable_face_similarity and batch_faces_for_image:
                    for j, face in enumerate(batch_faces_for_image):
                        face_pil = Image.fromarray(face)
                        face_features = extract_face_features(face_pil, feature_extractor, transform)
                        if face_features is not None:
                            face_id = f"batch_face_{i}_{j}"
                            st.session_state.batch_face_vectors[face_id] = {
                                'vector': face_features,
                                'image_index': i,
                                'face_index': j
                            }
            
            except Exception as e:
                st.error(f"å¤„ç†å›¾ç‰‡ #{i+1} æ—¶å‡ºé”™: {str(e)}")
        
        # è®¡ç®—æ‰¹é‡å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦
        if enable_similarity_search and len(st.session_state.batch_image_vectors) > 1:
            compute_batch_image_similarity()
        
        # è®¡ç®—æ‰¹é‡äººè„¸é—´çš„ç›¸ä¼¼åº¦
        if enable_face_similarity and len(st.session_state.batch_face_vectors) > 1:
            compute_batch_face_similarity()
        
        return len(uploaded_files)

# æ·»åŠ è®¡ç®—æ‰¹é‡å›¾ç‰‡ç›¸ä¼¼åº¦çš„å‡½æ•°
def compute_batch_image_similarity():
    """è®¡ç®—æ‰¹é‡ä¸Šä¼ å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦"""
    similarity_results = []
    
    # è·å–æ‰€æœ‰å›¾ç‰‡IDå’Œç‰¹å¾å‘é‡
    image_ids = list(st.session_state.batch_image_vectors.keys())
    
    # è®¡ç®—æ¯å¯¹å›¾ç‰‡é—´çš„ç›¸ä¼¼åº¦
    for i in range(len(image_ids)):
        for j in range(i+1, len(image_ids)):
            id1, id2 = image_ids[i], image_ids[j]
            vector1 = st.session_state.batch_image_vectors[id1]['vector']
            vector2 = st.session_state.batch_image_vectors[id2]['vector']
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity([vector1], [vector2])[0][0]
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            if similarity >= similarity_threshold:
                similarity_results.append({
                    'image1_id': id1,
                    'image2_id': id2,
                    'image1_index': st.session_state.batch_image_vectors[id1]['index'],
                    'image2_index': st.session_state.batch_image_vectors[id2]['index'],
                    'similarity': similarity
                })
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    similarity_results.sort(key=lambda x: x['similarity'], reverse=True)
    
    # å­˜å‚¨ç»“æœ
    st.session_state.batch_similarity_results = similarity_results

# æ·»åŠ è®¡ç®—æ‰¹é‡äººè„¸ç›¸ä¼¼åº¦çš„å‡½æ•°
def compute_batch_face_similarity():
    """è®¡ç®—æ‰¹é‡ä¸Šä¼ å›¾ç‰‡ä¸­äººè„¸é—´çš„ç›¸ä¼¼åº¦"""
    face_similarity_results = []
    
    # è·å–æ‰€æœ‰äººè„¸IDå’Œç‰¹å¾å‘é‡
    face_ids = list(st.session_state.batch_face_vectors.keys())
    
    # è®¡ç®—æ¯å¯¹äººè„¸é—´çš„ç›¸ä¼¼åº¦
    for i in range(len(face_ids)):
        for j in range(i+1, len(face_ids)):
            id1, id2 = face_ids[i], face_ids[j]
            vector1 = st.session_state.batch_face_vectors[id1]['vector']
            vector2 = st.session_state.batch_face_vectors[id2]['vector']
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = cosine_similarity([vector1], [vector2])[0][0]
            
            # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
            if similarity >= similarity_threshold:
                face_similarity_results.append({
                    'face1_id': id1,
                    'face2_id': id2,
                    'image1_index': st.session_state.batch_face_vectors[id1]['image_index'],
                    'image2_index': st.session_state.batch_face_vectors[id2]['image_index'],
                    'face1_index': st.session_state.batch_face_vectors[id1]['face_index'],
                    'face2_index': st.session_state.batch_face_vectors[id2]['face_index'],
                    'similarity': similarity
                })
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    face_similarity_results.sort(key=lambda x: x['similarity'], reverse=True)
    
    # å­˜å‚¨ç»“æœ
    st.session_state.batch_face_similarity_results = face_similarity_results

# æ·»åŠ æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ç›¸ä¼¼ç»“æœçš„å‡½æ•°
def display_batch_similarity_results():
    """æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ä¹‹é—´çš„ç›¸ä¼¼åº¦ç»“æœ"""
    if not st.session_state.batch_similarity_results:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼å›¾ç‰‡")
        return
    
    st.subheader(f"æ‰¹é‡å›¾ç‰‡ç›¸ä¼¼åº¦ç»“æœ (Top {min(5, len(st.session_state.batch_similarity_results))})")
    
    # æ˜¾ç¤ºå‰5ä¸ªç›¸ä¼¼ç»“æœ
    for i, result in enumerate(st.session_state.batch_similarity_results[:5]):
        cols = st.columns(3)
        
        # æ˜¾ç¤ºç¬¬ä¸€å¼ å›¾ç‰‡
        with cols[0]:
            img1_index = result['image1_index']
            st.image(st.session_state.batch_images[img1_index], caption=f"å›¾ç‰‡ #{img1_index+1}", use_container_width=True)
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦
        with cols[1]:
            st.write("")
            st.write("")
            st.write("")
            st.markdown(f"### ç›¸ä¼¼åº¦: {result['similarity']:.2f}")
            st.markdown("â†’")
        
        # æ˜¾ç¤ºç¬¬äºŒå¼ å›¾ç‰‡
        with cols[2]:
            img2_index = result['image2_index']
            st.image(st.session_state.batch_images[img2_index], caption=f"å›¾ç‰‡ #{img2_index+1}", use_container_width=True)
        
        st.write("---")

# æ·»åŠ æ˜¾ç¤ºæ‰¹é‡äººè„¸ç›¸ä¼¼ç»“æœçš„å‡½æ•°
def display_batch_face_similarity_results():
    """æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡ä¸­äººè„¸ä¹‹é—´çš„ç›¸ä¼¼åº¦ç»“æœ"""
    if not st.session_state.batch_face_similarity_results:
        st.info("æœªæ‰¾åˆ°ç›¸ä¼¼äººè„¸")
        return
    
    st.subheader(f"æ‰¹é‡äººè„¸ç›¸ä¼¼åº¦ç»“æœ (Top {min(5, len(st.session_state.batch_face_similarity_results))})")
    
    # æ˜¾ç¤ºå‰5ä¸ªç›¸ä¼¼ç»“æœ
    for i, result in enumerate(st.session_state.batch_face_similarity_results[:5]):
        cols = st.columns(3)
        
        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªäººè„¸
        with cols[0]:
            img1_index = result['image1_index']
            face1_index = result['face1_index']
            face1 = st.session_state.batch_faces[img1_index][face1_index]
            st.image(face1, caption=f"å›¾ç‰‡ #{img1_index+1} ä¸­çš„äººè„¸ #{face1_index+1}", use_container_width=True)
        
        # æ˜¾ç¤ºç›¸ä¼¼åº¦
        with cols[1]:
            st.write("")
            st.write("")
            st.write("")
            st.markdown(f"### ç›¸ä¼¼åº¦: {result['similarity']:.2f}")
            st.markdown("â†’")
        
        # æ˜¾ç¤ºç¬¬äºŒä¸ªäººè„¸
        with cols[2]:
            img2_index = result['image2_index']
            face2_index = result['face2_index']
            face2 = st.session_state.batch_faces[img2_index][face2_index]
            st.image(face2, caption=f"å›¾ç‰‡ #{img2_index+1} ä¸­çš„äººè„¸ #{face2_index+1}", use_container_width=True)
        
        st.write("---")


# ... existing code ...
def summarize_batch_images(similarity_threshold=0.5):
    """
    æ ¹æ®æ‰¹é‡ä¸Šä¼ å›¾ç‰‡çš„ç›¸ä¼¼æ€§æ€»ç»“åœºæ™¯æ•°é‡
    """
    try:
        if 'batch_images' not in st.session_state or len(st.session_state.batch_images) == 0:
            return "æœªæ£€æµ‹åˆ°ä¸Šä¼ çš„å›¾ç‰‡"
        
        # æ£€æŸ¥batch_image_vectorsæ˜¯å¦åŒ…å«ç‰¹å¾å‘é‡
        if 'batch_image_vectors' not in st.session_state or not st.session_state.batch_image_vectors:
            return "æœªèƒ½æå–å›¾ç‰‡ç‰¹å¾"
        
        # è·å–å›¾ç‰‡ç‰¹å¾å‘é‡
        image_features = []
        for image_id, data in st.session_state.batch_image_vectors.items():
            if 'vector' in data and data['vector'] is not None:
                image_features.append(data['vector'])
        
        if not image_features:
            return "æœªèƒ½æå–å›¾ç‰‡ç‰¹å¾"
        
        # ä½¿ç”¨å±‚æ¬¡èšç±»åˆ†æç›¸ä¼¼å›¾ç‰‡
        if len(image_features) > 1:
            features_array = np.vstack(image_features)
            #cats = np.concatenate([image_features], axis=1)
            
            # è®¡ç®—ä½™å¼¦è·ç¦»çŸ©é˜µ
            distance_matrix = 1 - cosine_similarity(features_array)
            
            # ä½¿ç”¨å±‚æ¬¡èšç±»
            clustering = AgglomerativeClustering(
                n_clusters=None, 
                distance_threshold=1 - similarity_threshold,  # è½¬æ¢ä¸ºè·ç¦»é˜ˆå€¼
                metric='precomputed',
                linkage='average'
            )
            clusters = clustering.fit_predict(distance_matrix)
            
            # è®¡ç®—åœºæ™¯æ•°é‡
            unique_clusters = len(set(clusters)) - (1 if -1 in clusters else 0)  # æ’é™¤å™ªå£°ç‚¹
            
            return f"åˆ†æç»“æœï¼šæ‚¨ä¸Šä¼ çš„{len(image_features)}å¼ å›¾ç‰‡ä¸­ï¼Œå¤§çº¦åŒ…å«{unique_clusters}ä¸ªä¸åŒåœºæ™¯ã€‚"
        else:
            return "å›¾ç‰‡æ•°é‡ä¸è¶³ï¼Œæ— æ³•è¿›è¡Œåœºæ™¯èšç±»"
    except Exception as e:
        return f"åœºæ™¯æ€»ç»“å‡ºé”™: {str(e)}"

def summarize_batch_faces(similarity_threshold=0.5):
    """
    æ ¹æ®æ‰¹é‡ä¸Šä¼ å›¾ç‰‡ä¸­çš„äººè„¸æ€»ç»“ä¸åŒäººç‰©æ•°é‡
    """
    try:
        if 'batch_faces' not in st.session_state or len(st.session_state.batch_faces) == 0:
            return "æœªæ£€æµ‹åˆ°äººè„¸"
        
        # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜å‚¨äººè„¸
        temp_input_dir = os.path.join(tempfile.gettempdir(), "face_clustering_input")
        temp_output_dir = os.path.join(tempfile.gettempdir(), "face_clustering_output")
        
        if os.path.exists(temp_input_dir):
            shutil.rmtree(temp_input_dir)
        if os.path.exists(temp_output_dir):
            shutil.rmtree(temp_output_dir)
            
        os.makedirs(temp_input_dir)
        
        # æå–å¹¶ä¿å­˜æ‰€æœ‰äººè„¸
        face_count = 0
        for i, faces in enumerate(st.session_state.batch_faces):
            if isinstance(faces, list):
                for j, face_np in enumerate(faces):
                    if isinstance(face_np, np.ndarray) and face_np.size > 0:
                        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºPIL Image
                        face_img = Image.fromarray(face_np)
                        face_path = os.path.join(temp_input_dir, f"face_{i}_{j}.jpg")
                        face_img.save(face_path)
                        face_count += 1
        
        if face_count == 0:
            return "æœªåœ¨ä¸Šä¼ çš„å›¾ç‰‡ä¸­æ£€æµ‹åˆ°äººè„¸"
        
        # ä½¿ç”¨face_clusteringå‡½æ•°è¿›è¡Œäººè„¸èšç±»
        face_clustering(temp_input_dir, temp_output_dir, 
                        detection_method="hog",
                        tolerance=23,
                        min_cluster_size=1,
                        dim_reduction=False, n_components=3)  # è®¾ç½®min_cluster_sizeä¸º1ä»¥åŒ…å«æ‰€æœ‰äººè„¸
        
        # è®¡ç®—èšç±»ç»“æœ
        cluster_dirs = [d for d in os.listdir(temp_output_dir) 
                       if os.path.isdir(os.path.join(temp_output_dir, d)) and d.startswith("person_")]
        
        # æ³¨æ„ï¼šæ­¤å¤„ä¸å†åˆ é™¤ä¸´æ—¶ç›®å½•ï¼Œä»¥ä¾¿åç»­ç”Ÿæˆè’™å¤ªå¥‡å›¾
        # è¿”å›ä¸´æ—¶ç›®å½•è·¯å¾„å’Œåˆ†æç»“æœ
        return f"åˆ†æç»“æœï¼šæ‚¨ä¸Šä¼ çš„å›¾ç‰‡ä¸­ï¼Œå…±æ£€æµ‹åˆ°{face_count}ä¸ªäººè„¸ï¼Œå¤§çº¦åŒ…å«{len(cluster_dirs)}ä¸ªä¸åŒäººç‰©ã€‚"
    except Exception as e:
        return f"äººè„¸æ€»ç»“å‡ºé”™: {str(e)}"

def display_batch_analysis_summary():
    """
    æ˜¾ç¤ºæ‰¹é‡å›¾ç‰‡åˆ†æçš„æ€»ç»“
    """
    if 'batch_images' in st.session_state and len(st.session_state.batch_images) > 0:
        st.subheader("æ‰¹é‡åˆ†ææ€»ç»“")
        
        col1, col2 = st.columns(2)
        with col1:
            # similarity_threshold = st.slider("ç›¸ä¼¼åº¦é˜ˆå€¼", 0.1, 0.9, 0.5, 0.1, 
            #                                 help="è¾ƒä½çš„å€¼ä¼šå°†æ›´å¤šå›¾ç‰‡å½’ä¸ºåŒä¸€åœºæ™¯/åŒä¸€äººç‰©")
            similarity_threshold = 0.5
        
        scene_summary = summarize_batch_images(similarity_threshold)
        face_summary = summarize_batch_faces(similarity_threshold)
        
        st.info(scene_summary)
        st.info(face_summary)

        # æ·»åŠ äººè„¸èšç±»è’™å¤ªå¥‡å›¾æ˜¾ç¤º
        st.subheader("äººè„¸èšç±»ç»“æœ")
        try:
            # ä½¿ç”¨ä¸summarize_batch_facesç›¸åŒçš„ä¸´æ—¶ç›®å½•
            temp_output_dir = os.path.join(tempfile.gettempdir(), "face_clustering_output")
            
            if os.path.exists(temp_output_dir):
                # ç”Ÿæˆè’™å¤ªå¥‡å›¾
                montages_info = create_montages(temp_output_dir, montage_size=(150, 150), images_per_row=5)
                
                if montages_info:
                    for montage in montages_info:
                        st.image(montage["path"], 
                                caption=f"èšç±» {montage['cluster']} ({montage['images_count']} å¼ å›¾ç‰‡)",
                                use_container_width=True)
                else:
                    st.info("æœªç”Ÿæˆäººè„¸èšç±»è’™å¤ªå¥‡å›¾")
            else:
                st.info("æœªæ‰¾åˆ°äººè„¸èšç±»ç»“æœ")
        except Exception as e:
            st.error(f"æ˜¾ç¤ºäººè„¸èšç±»è’™å¤ªå¥‡å›¾å‡ºé”™: {str(e)}")
            
        # åœ¨å®Œæˆæ˜¾ç¤ºåæ‰æ¸…ç†ä¸´æ—¶ç›®å½•
        try:
            temp_input_dir = os.path.join(tempfile.gettempdir(), "face_clustering_input")
            temp_output_dir = os.path.join(tempfile.gettempdir(), "face_clustering_output")
            
            if os.path.exists(temp_input_dir):
                shutil.rmtree(temp_input_dir)
            if os.path.exists(temp_output_dir):
                shutil.rmtree(temp_output_dir)
        except Exception as e:
            st.error(f"æ¸…ç†ä¸´æ—¶ç›®å½•å‡ºé”™: {str(e)}")
# ... existing code ...

# ä¸»ç¨‹åº
try:
    # è·å–é€‰æ‹©çš„æ¨¡å‹åç§°
    selected_model = model_mapping[model_option]
    model = load_model(selected_model)
    
    # åŠ è½½äººè„¸æ£€æµ‹å™¨
    if detect_faces:
        face_detector = load_face_detector()
    
    # åŠ è½½ç‰¹å¾æå–æ¨¡å‹
    if enable_similarity_search or enable_face_similarity:
        feature_extractor = load_feature_extractor()
        transform = get_transform()
    
    # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
    st.info(f"å½“å‰ä½¿ç”¨: {model_option}")
    
    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æ‰¹é‡æ¨¡å¼
    if st.session_state.batch_mode:
        # æ‰¹é‡ä¸Šä¼ å›¾ç‰‡
        uploaded_files = st.file_uploader("ä¸Šä¼ å¤šå¼ å›¾ç‰‡è¿›è¡Œæ‰¹é‡å¤„ç†", type=["jpg", "jpeg", "png"], accept_multiple_files=True)
        
        if uploaded_files:
            # æ˜¾ç¤ºä¸Šä¼ çš„å›¾ç‰‡æ•°é‡
            st.write(f"å·²ä¸Šä¼  {len(uploaded_files)} å¼ å›¾ç‰‡")
            
            # æ·»åŠ æ‰¹é‡å¤„ç†æŒ‰é’®
            if st.button(f"å¤„ç† {len(uploaded_files)} å¼ å›¾ç‰‡"):
                # æ‰§è¡Œæ‰¹é‡å¤„ç†
                processed_count = process_batch_images(uploaded_files)
                st.success(f"æˆåŠŸå¤„ç† {processed_count} å¼ å›¾ç‰‡")
                
                # æ˜¾ç¤ºå¤„ç†ç»“æœæ¦‚è§ˆ
                st.subheader("æ‰¹é‡å¤„ç†ç»“æœæ¦‚è§ˆ")
                
                # åˆ›å»ºä¸€ä¸ªè¡¨æ ¼æ˜¾ç¤ºæ¯å¼ å›¾ç‰‡çš„åŸºæœ¬ä¿¡æ¯
                data = []
                for i, (image, processed_img) in enumerate(zip(st.session_state.batch_images, st.session_state.batch_processed_images)):
                    # æ£€æŸ¥æ˜¯å¦æœ‰äººè„¸
                    face_count = len(st.session_state.batch_faces[i]) if i < len(st.session_state.batch_faces) else 0
                    
                    # æ·»åŠ è¡Œ
                    data.append({
                        "å›¾ç‰‡åºå·": i+1,
                        "å°ºå¯¸": f"{image.width}x{image.height}",
                        "æ£€æµ‹åˆ°çš„äººè„¸æ•°": face_count
                    })
                
                # æ˜¾ç¤ºè¡¨æ ¼
                st.dataframe(data)
                
                # åˆ›å»ºæ ‡ç­¾é¡µæ˜¾ç¤ºè¯¦ç»†ç»“æœ
                tab1, tab2, tab3, tab4, tab5 = st.tabs(["å¤„ç†åçš„å›¾ç‰‡", "æ£€æµ‹åˆ°çš„äººè„¸", "å›¾ç‰‡ç›¸ä¼¼åº¦", "äººè„¸ç›¸ä¼¼åº¦", "æ‰¹é‡åˆ†ææ€»ç»“"])
                
                with tab1:
                    # æ˜¾ç¤ºå¤„ç†åçš„å›¾ç‰‡
                    st.subheader("å¤„ç†åçš„å›¾ç‰‡")
                    cols = st.columns(3)
                    for i, processed_img in enumerate(st.session_state.batch_processed_images):
                        with cols[i % 3]:
                            st.image(processed_img, caption=f"å›¾ç‰‡ #{i+1}", use_container_width=True)
                
                with tab2:
                    # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„äººè„¸
                    st.subheader("æ£€æµ‹åˆ°çš„äººè„¸")
                    
                    # éå†æ¯å¼ å›¾ç‰‡
                    for i, faces in enumerate(st.session_state.batch_faces):
                        if faces:
                            st.write(f"#### å›¾ç‰‡ #{i+1} ä¸­çš„äººè„¸:")
                            cols = st.columns(min(len(faces), 4))
                            for j, face in enumerate(faces):
                                with cols[j % 4]:
                                    st.image(face, caption=f"äººè„¸ #{j+1}", use_container_width=True)
                        else:
                            st.write(f"å›¾ç‰‡ #{i+1} ä¸­æœªæ£€æµ‹åˆ°äººè„¸")
                
                with tab3:
                    # æ˜¾ç¤ºå›¾ç‰‡ç›¸ä¼¼åº¦ç»“æœ
                    display_batch_similarity_results()
                
                with tab4:
                    # æ˜¾ç¤ºäººè„¸ç›¸ä¼¼åº¦ç»“æœ
                    display_batch_face_similarity_results()

                with tab5:
                    # æ˜¾ç¤ºæ‰¹é‡åˆ†ææ€»ç»“
                    display_batch_analysis_summary()

    else:
        # åŸæœ‰çš„å•å¼ å›¾ç‰‡ä¸Šä¼ é€»è¾‘
        uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=["jpg", "jpeg", "png"], on_change=reset_processed_state)
        
        if uploaded_file is not None:
            # ä¸ºä¸Šä¼ æ–‡ä»¶ç”Ÿæˆå”¯ä¸€ID
            current_upload_id = id(uploaded_file)
            
            # æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
            col1, col2 = st.columns(2)
            
            # å¦‚æœæ˜¯æ–°ä¸Šä¼ çš„æ–‡ä»¶æˆ–è€…å°šæœªå¤„ç†
            if current_upload_id != st.session_state.last_upload_id or not st.session_state.processed:
                try:
                    # è·å–æ–‡ä»¶å¤§å°
                    uploaded_file.seek(0)
                    file_size_mb = len(uploaded_file.getvalue()) / (1024 * 1024)  # è½¬æ¢ä¸ºMB
                    
                    # è¯»å–ä¸Šä¼ çš„å›¾ç‰‡å¹¶ç«‹å³å‹ç¼©
                    image = Image.open(uploaded_file)
                    
                    # å§‹ç»ˆå‹ç¼©å›¾ç‰‡ä»¥é˜²æ­¢å†…å­˜é—®é¢˜ï¼Œå¯¹äºå¤§å›¾ç‰‡æ˜¾ç¤ºæç¤º
                    if file_size_mb > 2:
                        st.warning(f"ä¸Šä¼ çš„å›¾ç‰‡å¤§å°ä¸º {file_size_mb:.2f}MBï¼Œè¶…è¿‡2MBï¼Œå°†è¿›è¡Œå‹ç¼©ã€‚")
                    
                    # å‹ç¼©å›¾ç‰‡ï¼Œé™åˆ¶åœ¨500KBä»¥å†…
                    original_width, original_height = image.size
                    image = compress_image(image, max_size_kb=100)
                    
                    # è·å–å‹ç¼©åçš„å¤§å°å’Œå°ºå¯¸
                    buffer = BytesIO()
                    image.save(buffer, format='JPEG', optimize=True)
                    compressed_size_kb = buffer.tell() / 1024
                    current_width, current_height = image.size
                    
                    if file_size_mb > 2:
                        st.success(f"å‹ç¼©å®Œæˆï¼åŸå§‹å°ºå¯¸: {original_width}x{original_height}ï¼Œç°åœ¨: {current_width}x{current_height}ï¼Œå¤§å°: {compressed_size_kb:.2f}KB")
                    
                    st.session_state.image = image
                    st.session_state.last_upload_id = current_upload_id
                    
                    with col1:
                        st.subheader("åŸå§‹å›¾ç‰‡")
                        st.image(image, use_container_width=True)
                    
                    # è¿›è¡Œç‰©ä½“æ£€æµ‹
                    with st.spinner(f"æ­£åœ¨ä½¿ç”¨ {model_option} è¿›è¡Œç‰©ä½“æ£€æµ‹..."):
                        results = model.predict(image)
                        st.session_state.detection_results = results
                        
                        # è·å–é€‰æ‹©çš„ç±»åˆ«
                        selected_classes = []
                        if detect_person:
                            selected_classes.append("äºº")
                        if detect_cup:
                            selected_classes.append("æ¯å­/é…’æ¯")
                        if detect_bottle:
                            selected_classes.append("ç“¶å­")
                        if detect_all:
                            selected_classes.append("æ£€æµ‹æ‰€æœ‰æ”¯æŒçš„ç‰©ä½“")
                        
                        # å…ˆæ‰§è¡Œäººè„¸æ£€æµ‹ï¼ˆåœ¨åŸå§‹å›¾åƒä¸Šï¼‰
                        detected_faces = []
                        original_image_for_face = np.array(image)
                        if detect_faces:
                            with st.spinner("æ­£åœ¨è¿›è¡Œäººè„¸æ£€æµ‹..."):
                                _, detected_faces = detect_face(original_image_for_face, face_detector, face_confidence)
                                st.session_state.detected_faces = detected_faces
                        
                        # å¤„ç†ç»“æœå¹¶æ ‡è®°ï¼ˆå…ˆç‰©ä½“æ£€æµ‹ï¼‰
                        processed_img = process_prediction(image, results, selected_classes, confidence)
                        st.session_state.processed_img = processed_img
                        
                        # å¦‚æœå¯ç”¨äº†äººè„¸æ£€æµ‹ï¼Œåœ¨ç‰©ä½“æ£€æµ‹ç»“æœä¸Šæ·»åŠ äººè„¸æ ‡è®°
                        if detect_faces:
                            # è§£æäººè„¸è¾¹æ¡†é¢œè‰²
                            hex_color = face_box_color.lstrip('#')
                            face_bgr = tuple(int(hex_color[i:i+2], 16) for i in (4, 2, 0))  # è½¬æ¢ä¸ºBGR
                            
                            # å°†å¤„ç†è¿‡çš„å›¾åƒè½¬æ¢ä¸ºOpenCVæ ¼å¼
                            img_cv = cv2.cvtColor(np.array(processed_img), cv2.COLOR_RGB2BGR)
                            
                            # ä½¿ç”¨MTCNNæ£€æµ‹äººè„¸
                            faces = face_detector.detect_faces(original_image_for_face)
                            
                            # ç­›é€‰ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼çš„äººè„¸
                            faces = [face for face in faces if face['confidence'] >= face_confidence]
                            
                            # åœ¨å·²å¤„ç†çš„å›¾åƒä¸Šæ·»åŠ äººè„¸è¾¹æ¡†
                            for face in faces:
                                # è·å–è¾¹ç•Œæ¡†åæ ‡
                                x, y, w, h = face['box']
                                
                                # ç»˜åˆ¶äººè„¸è¾¹æ¡†
                                cv2.rectangle(img_cv, (x, y), (x+w, y+h), face_bgr, 2)
                                
                                # æ·»åŠ æ ‡ç­¾å’Œç½®ä¿¡åº¦
                                label = f"Face: {face['confidence']:.2f}"
                                cv2.putText(img_cv, label, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, face_bgr, 2)
                                
                                # å¯é€‰ï¼šç»˜åˆ¶å…³é”®ç‚¹
                                keypoints = face['keypoints']
                                for point in keypoints.values():
                                    cv2.circle(img_cv, point, 2, face_bgr, 2)
                            
                            # è½¬æ¢å›RGB
                            processed_img = cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB) 
                        
                        # å°†æœ€ç»ˆç»“æœä¿å­˜åˆ°session_state
                        st.session_state.processed_img = processed_img
                        
                        # æ‰§è¡Œç›¸ä¼¼å›¾ç‰‡æœç´¢
                        if enable_similarity_search or enable_face_similarity:
                            with st.spinner("æ­£åœ¨æ·»åŠ å›¾ç‰‡åˆ°å‘é‡åº“å¹¶æ‰§è¡Œç›¸ä¼¼æœç´¢..."):
                                # æå–ç‰¹å¾å¹¶æ›´æ–°å‘é‡åº“
                                image_id, face_ids = update_vector_db(image, detected_faces, feature_extractor, transform)
                                
                                # åˆå§‹åŒ–å‘é‡æ•°æ®åº“ï¼ˆè·å–æœ€æ–°çš„æ•°æ®ï¼‰
                                image_db, face_db = initialize_vector_db()
                                
                                # è·å–å½“å‰å›¾ç‰‡çš„ç‰¹å¾å‘é‡
                                current_image_vector = image_db[image_id]['vector']
                                
                                # æ‰§è¡Œç›¸ä¼¼å›¾ç‰‡æœç´¢
                                if enable_similarity_search:
                                    # æ’é™¤å½“å‰å›¾ç‰‡
                                    search_image_db = {k: v for k, v in image_db.items() if k != image_id}
                                    similar_images = search_similar_images(
                                        current_image_vector, 
                                        search_image_db, 
                                        top_k=top_k, 
                                        threshold=similarity_threshold
                                    )
                                    
                                    # ä¿å­˜æœç´¢ç»“æœåˆ°session_state
                                    st.session_state.similar_images = similar_images
                                
                                # æ‰§è¡Œäººè„¸ç›¸ä¼¼æœç´¢
                                if enable_face_similarity and detected_faces:
                                    # è·å–å½“å‰å›¾ç‰‡æ£€æµ‹åˆ°çš„äººè„¸ç‰¹å¾å‘é‡
                                    current_face_vectors = []
                                    for face_id in face_ids:
                                        if face_id in face_db:
                                            current_face_vectors.append(face_db[face_id]['vector'])
                                    
                                    # æ’é™¤å½“å‰å›¾ç‰‡çš„äººè„¸
                                    search_face_db = {k: v for k, v in face_db.items() if k not in face_ids}
                                    
                                    # å¯¹æ¯ä¸ªäººè„¸æ‰§è¡Œç›¸ä¼¼æœç´¢
                                    similar_faces_results = search_similar_faces(
                                        current_face_vectors, 
                                        search_face_db, 
                                        top_k=top_k, 
                                        threshold=similarity_threshold
                                    )
                                    
                                    # ä¿å­˜æœç´¢ç»“æœåˆ°session_state
                                    st.session_state.similar_faces_results = similar_faces_results
                        
                        # æ ‡è®°ä¸ºå·²å¤„ç†
                        st.session_state.processed = True
                        # é‡ç½®åŠ å¼ºäººè„¸æ£€ç´¢çŠ¶æ€
                        st.session_state.advanced_face_search_done = False
                except Exception as e:
                    st.error(f"å¤„ç†å›¾ç‰‡æ—¶å‡ºé”™: {str(e)}")
                    st.info("è¯·å°è¯•ä¸Šä¼ è¾ƒå°çš„å›¾ç‰‡æˆ–é™ä½å›¾ç‰‡åˆ†è¾¨ç‡")
            else:
                # ä½¿ç”¨ä¹‹å‰å¤„ç†è¿‡çš„ç»“æœ
                image = st.session_state.image
                with col1:
                    st.subheader("åŸå§‹å›¾ç‰‡")
                    st.image(image, use_container_width=True)
            
            # æ˜¾ç¤ºå¤„ç†åçš„å›¾ç‰‡
            with col2:
                st.subheader("æ£€æµ‹ç»“æœ")
                st.image(st.session_state.processed_img, use_container_width=True)
            
            # æ˜¾ç¤ºç›¸ä¼¼å›¾ç‰‡æœç´¢ç»“æœ
            if enable_similarity_search and st.session_state.similar_images:
                display_similar_images(st.session_state.similar_images)
            
            # æ˜¾ç¤ºç›¸ä¼¼äººè„¸ç»“æœ
            if enable_face_similarity and st.session_state.similar_faces_results:
                display_similar_faces(st.session_state.similar_faces_results, st.session_state.detected_faces)
            
            # æ˜¾ç¤ºæ£€æµ‹åˆ°çš„äººè„¸
            if detect_faces and st.session_state.detected_faces:
                st.subheader(f"æ£€æµ‹åˆ°çš„äººè„¸ä¸æƒ…ç»ªåˆ†æ ({len(st.session_state.detected_faces)})")
                face_cols = st.columns(min(len(st.session_state.detected_faces), 4))
                
                # æƒ…ç»ªå¯¹åº”çš„è¡¨æƒ…ç¬¦å·
                emotion_emojis = {
                    "angry": "ğŸ˜ ",
                    "disgust": "ğŸ¤¢",
                    "fear": "ğŸ˜¨",
                    "happy": "ğŸ˜Š",
                    "sad": "ğŸ˜¢",
                    "surprise": "ğŸ˜²",
                    "neutral": "ğŸ˜"
                }
                
                for i, face in enumerate(st.session_state.detected_faces):
                    with face_cols[i % 4]:
                        try:
                            # åˆ†æäººè„¸æƒ…ç»ª
                            result = DeepFace.analyze(face, actions=['emotion'], enforce_detection=False)
                            emotion = result[0]['dominant_emotion']
                            emotion_emoji = emotion_emojis.get(emotion, "")
                            
                            # æ˜¾ç¤ºäººè„¸å›¾åƒå’Œæƒ…ç»ª
                            st.image(face, caption=f"äººè„¸ #{i+1}")
                            st.write(f"ä¸»è¦æƒ…ç»ª: {emotion} {emotion_emoji}")
                            
                            # # æ˜¾ç¤ºæƒ…ç»ªåˆ†æ•°
                            # emotions = result[0]['emotion']
                            # # åˆ›å»ºæƒ…ç»ªåˆ†æ•°æ¡å½¢å›¾
                            # emotions_values = {k: v for k, v in emotions.items()}
                            # max_emotion = max(emotions_values, key=emotions_values.get)
                            # for emo, score in emotions_values.items():
                            #     emoji_icon = emotion_emojis.get(emo, "")
                            #     st.write(f"{emoji_icon} {emo}: {score:.2f}%")
                        except Exception as e:
                            st.error(f"åˆ†æäººè„¸ #{i+1} æ—¶å‡ºé”™: {str(e)}")
                            st.image(face, caption=f"äººè„¸ #{i+1} (æƒ…ç»ªåˆ†æå¤±è´¥)")
            elif detect_faces and not st.session_state.detected_faces:
                st.info("æœªæ£€æµ‹åˆ°äººè„¸")
            
            # æ·»åŠ ä¸‹è½½æŒ‰é’®
            buf = io.BytesIO()
            pil_img = Image.fromarray(st.session_state.processed_img)
            pil_img.save(buf, format="PNG")
            st.download_button(
                label="ä¸‹è½½æ£€æµ‹ç»“æœ",
                data=buf.getvalue(),
                file_name="detected_image.png",
                mime="image/png",
            )
            
            # æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
            if results[0].boxes is not None:
                # è·å–æ‰€æœ‰æ£€æµ‹ç»“æœçš„ç±»åˆ«
                all_classes = results[0].boxes.cls.cpu().numpy()
                all_confidences = results[0].boxes.conf.cpu().numpy()
                
                # ç»Ÿè®¡æ£€æµ‹åˆ°çš„ç‰©ä½“æ•°é‡
                st.subheader("æ£€æµ‹ç»Ÿè®¡")
                class_names = results[0].names
                
                # åˆ›å»ºä¸€ä¸ªå­—å…¸ï¼Œç”¨äºå­˜å‚¨å„ç±»åˆ«çš„è®¡æ•°
                class_counts = {}
                
                for cls, conf in zip(all_classes, all_confidences):
                    cls_id = int(cls)
                    # å¦‚æœç±»åˆ«åœ¨é€‰ä¸­çš„ç±»åˆ«ä¸­ä¸”ç½®ä¿¡åº¦é«˜äºé˜ˆå€¼
                    if ((cls_id == 0 and "äºº" in selected_classes) or
                        (cls_id == 41 and "æ¯å­/é…’æ¯" in selected_classes) or
                        (cls_id == 39 and "ç“¶å­" in selected_classes) or
                        detect_all) and conf >= confidence:
                        
                        cls_name = class_names[cls_id]
                        if cls_name in class_counts:
                            class_counts[cls_name] += 1
                        else:
                            class_counts[cls_name] = 1
                
                # æ˜¾ç¤ºç»Ÿè®¡ç»“æœ
                for cls_name, count in class_counts.items():
                    st.write(f"- æ£€æµ‹åˆ° {count} ä¸ª {cls_name}")
                
                # æ·»åŠ äººè„¸è®¡æ•°åˆ°ç»Ÿè®¡ç»“æœ
                if detect_faces and st.session_state.detected_faces:
                    st.write(f"- æ£€æµ‹åˆ° {len(st.session_state.detected_faces)} ä¸ªäººè„¸")
                
                if not class_counts and not (detect_faces and st.session_state.detected_faces):
                    st.write("æœªæ£€æµ‹åˆ°ä»»ä½•ç‰©ä½“")

                # æ·»åŠ AIåˆ†æéƒ¨åˆ†
                st.subheader("AIç»¼åˆåˆ†æ")
                
                with st.spinner("æ­£åœ¨è¿›è¡ŒAIåˆ†æ..."):
                    # è·å–API keys
                    groq_api_key = os.environ.get("GROQ_API_KEY")
                    openai_api_key = os.environ.get("GEMINI_API_KEY")
                    
                    if groq_api_key and openai_api_key:
                        # æ£€æŸ¥å›¾ç‰‡å°ºå¯¸
                        image_dimensions = check_image_dimensions(image)
                        
                        # æ‰§è¡ŒGroqåˆ†æ
                        groq_analysis = analyze_with_groq(image, groq_api_key)
                        print(groq_analysis)
                        print("--------------------------------")

                        # å‡†å¤‡CVæ£€æµ‹ç»“æœæ‘˜è¦
                        cv_summary = {
                            "detected_objects": class_counts,
                            "face_count": len(st.session_state.detected_faces) if detect_faces else 0
                        }
                        print(cv_summary)
                        
                        # æ‰§è¡ŒOpenAIæ€»ç»“ï¼ŒåŠ å…¥å›¾ç‰‡å°ºå¯¸ä¿¡æ¯
                        final_summary = summarize_with_openai(cv_summary, groq_analysis, openai_api_key, image_dimensions)
                        
                        # æ˜¾ç¤ºåˆ†æç»“æœ
                        st.markdown(final_summary)
                    else:
                        st.warning("è¯·è®¾ç½®GROQ_API_KEYå’ŒOPENAI_API_KEYç¯å¢ƒå˜é‡ä»¥å¯ç”¨AIåˆ†æåŠŸèƒ½")

                    
                    if groq_api_key:
                        with st.spinner("æ­£åœ¨æå–æ–‡å­—å†…å®¹å¹¶åˆ›å»ºembedding..."):
                            text_content = extract_text_from_analysis(groq_analysis)


                            print(f"æå–çš„æ–‡æœ¬å†…å®¹ï¼š'{text_content}'")
                            print(f"æ–‡æœ¬å†…å®¹é•¿åº¦ï¼š{len(text_content) if text_content else 0}")
                            print(f"æ˜¯å¦ç­‰äº'æ— æ–‡å­—å†…å®¹': {text_content == 'æ— æ–‡å­—å†…å®¹'}")


                            if text_content and text_content.strip() != 'æ— æ–‡å­—å†…å®¹':

                                update_text_vector_db(image_id, text_content)
                                print("æˆåŠŸæ›´æ–°æ–‡æœ¬å‘é‡æ•°æ®åº“")


                                print("å¼€å§‹æœç´¢ç›¸ä¼¼æ–‡æœ¬")
                                similar_texts = search_similar_texts(
                                    image_id,
                                    top_k=top_k,
                                    threshold=0.8
                                )

                                st.session_state.similar_texts = similar_texts
                                print(f"æ‰¾åˆ° {len(similar_texts)} ä¸ªç›¸ä¼¼æ–‡æœ¬")


                                if st.session_state.similar_texts and len(st.session_state.similar_texts) > 0:
                                    display_similar_texts(st.session_state.similar_texts)
                            else:

                                print("æ— æ–‡å­—å†…å®¹ï¼Œ è·³è¿‡æ–‡æœ¬å‘é‡æ›´æ–°å’Œç›¸ä¼¼æ–‡æœ¬æœç´¢")
                                st.session_state.similar_texts = []


                            with st.expander("å›¾ç‰‡ä¸­æå–çš„æ–‡å­—å†…å®¹"):
                                st.write(text_content)

            # åŠ å¼ºäººè„¸æ£€ç´¢éƒ¨åˆ†ï¼Œç‹¬ç«‹äºYOLOå’ŒGroqåˆ†æ
            if enable_advanced_face_search and st.session_state.detected_faces:
                st.subheader("åŠ å¼ºäººè„¸æ£€ç´¢ç»“æœ")


                if len(st.session_state.detected_faces) > 5:
                    st.warning("æ£€æµ‹åˆ°çš„äººè„¸æ•°é‡è¶…è¿‡5ä¸ªï¼Œä¸ºæé«˜æ€§èƒ½å·²è·³è¿‡åŠ å¼ºäººè„¸æœç´¢ï¼Œè¯·é€‰æ‹©åŒ…å«æ›´å°‘äººè„¸çš„å›¾ç‰‡ä»¥è·å¾—æ›´å¥½çš„æ£€ç´¢æ•ˆæœ")
                else:
                    # å½“toleranceå‚æ•°å˜åŒ–æˆ–å°šæœªæ‰§è¡Œè¿‡äººè„¸æ£€ç´¢æ—¶æ‰æ‰§è¡Œ
                    if not st.session_state.advanced_face_search_done or st.session_state.current_tolerance != face_cluster_tolerance:
                        with st.spinner("æ­£åœ¨æŸ¥æ‰¾ç¦»çº¿èšç±»ä¸­çš„ç›¸ä¼¼äººè„¸..."):
                            face_best_matches = process_face_for_advanced_search(
                                st.session_state.detected_faces,
                                tolerance=face_cluster_tolerance
                            )
                            
                            # æ›´æ–°session_state
                            st.session_state.face_best_matches = face_best_matches
                            st.session_state.advanced_face_search_done = True
                            st.session_state.current_tolerance = face_cluster_tolerance
                    else:
                        # ä½¿ç”¨å·²è®¡ç®—çš„ç»“æœ
                        face_best_matches = st.session_state.face_best_matches
                    
                    # æ˜¾ç¤ºäººè„¸åŒ¹é…ç»“æœ
                    if face_best_matches:
                        st.write(f"ä¸º {len(face_best_matches)} ä¸ªæ£€æµ‹åˆ°çš„äººè„¸æ‰¾åˆ°äº†ç›¸ä¼¼èšç±»")
                        
                        # æ˜¾ç¤ºæ¯ä¸ªäººè„¸åŠå…¶å¯¹åº”çš„æœ€ä½³åŒ¹é…
                        for i, match in enumerate(face_best_matches):
                            st.write(f"##### äººè„¸ #{match['face_idx'] + 1} çš„æœ€ä½³åŒ¹é…:")
                            
                            cols = st.columns(2)
                            with cols[0]:
                                # æ˜¾ç¤ºåŸå§‹æ£€æµ‹åˆ°çš„äººè„¸
                                face_img = Image.fromarray(st.session_state.detected_faces[match['face_idx']])
                                st.image(face_img, caption=f"æ£€æµ‹åˆ°çš„äººè„¸ #{match['face_idx'] + 1}", width=150)
                            
                            with cols[1]:
                                # æ˜¾ç¤ºåŒ¹é…çš„èšç±»è’™å¤ªå¥‡å›¾
                                st.image(match["path"], 
                                        caption=f"èšç±» {match['cluster']} (ç›¸ä¼¼åº¦: {match['similarity']:.2f}, {match['images_count']} å¼ å›¾ç‰‡)",
                                        use_container_width=True)
                            
                            st.write("---")  # æ·»åŠ åˆ†éš”çº¿
                    else:
                        st.info("æœªåœ¨å·²æœ‰èšç±»ä¸­æ‰¾åˆ°ç›¸ä¼¼äººè„¸")

except Exception as e:
    st.error(f"å‘ç”Ÿé”™è¯¯: {e}")
    if "æœªèƒ½ä¸‹è½½é¢„è®­ç»ƒæƒé‡" in str(e) or "Failed to download" in str(e):
        st.warning(f"é¦–æ¬¡è¿è¡Œæ—¶éœ€è¦ä¸‹è½½YOLOv8æ¨¡å‹ï¼Œè¯·ç¡®ä¿æ‚¨æœ‰ç¨³å®šçš„ç½‘ç»œè¿æ¥ã€‚è¾ƒå¤§çš„æ¨¡å‹ä¸‹è½½å¯èƒ½éœ€è¦æ›´é•¿æ—¶é—´ã€‚")
    
# æ·»åŠ ä½¿ç”¨è¯´æ˜
with st.expander("ä½¿ç”¨è¯´æ˜"):
    st.markdown("""
    ### ä½¿ç”¨æ–¹æ³•ï¼š
    1. ä¸Šä¼ ä¸€å¼ åŒ…å«è¦æ£€æµ‹ç‰©ä½“çš„å›¾ç‰‡
    2. åœ¨ä¾§è¾¹æ è°ƒæ•´æ£€æµ‹è®¾ç½®ï¼š
       - é€‰æ‹©æ¨¡å‹ï¼ˆç²¾åº¦ä¸é€Ÿåº¦çš„å¹³è¡¡ï¼‰
       - è®¾ç½®ç½®ä¿¡åº¦é˜ˆå€¼ï¼ˆå€¼è¶Šé«˜ï¼Œè¯¯æ£€æµ‹è¶Šå°‘ï¼Œä½†ä¹Ÿå¯èƒ½æ¼æ‰ä¸€äº›ç‰©ä½“ï¼‰
       - é€‰æ‹©è¦æ£€æµ‹çš„ç‰©ä½“ç±»åˆ«
       - å¯ç”¨/ç¦ç”¨äººè„¸æ£€æµ‹
       - è®¾ç½®ç›¸ä¼¼å›¾ç‰‡å’Œäººè„¸æœç´¢é€‰é¡¹
       - è‡ªå®šä¹‰æ ‡è®°é¢œè‰²
    3. æŸ¥çœ‹æ£€æµ‹ç»“æœã€ç›¸ä¼¼å›¾ç‰‡æœç´¢ç»“æœï¼Œå¹¶ä¸‹è½½æ ‡è®°åçš„å›¾ç‰‡
    
    ### æ¨¡å‹æ¯”è¾ƒï¼š
    - YOLOv8x: æœ€é«˜ç²¾åº¦ï¼Œä½†å¤„ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œé€‚åˆå¯¹ç²¾åº¦è¦æ±‚é«˜çš„åœºæ™¯
    - YOLOv8l: é«˜ç²¾åº¦ï¼Œé€Ÿåº¦é€‚ä¸­
    - YOLOv8m: å¹³è¡¡çš„ç²¾åº¦å’Œé€Ÿåº¦
    - YOLOv8s: è¾ƒå¿«çš„é€Ÿåº¦ï¼Œç²¾åº¦é€‚ä¸­
    - YOLOv8n: æœ€å¿«çš„é€Ÿåº¦ï¼Œä½†ç²¾åº¦è¾ƒä½
    
    ### ç›¸ä¼¼å›¾ç‰‡å’Œäººè„¸æœç´¢ï¼š
    - ç³»ç»Ÿä¼šå°†æ¯æ¬¡ä¸Šä¼ çš„å›¾ç‰‡æ·»åŠ åˆ°å‘é‡æ•°æ®åº“ä¸­
    - ç›¸ä¼¼å›¾ç‰‡æœç´¢ï¼šæŸ¥æ‰¾ä¸ä¸Šä¼ å›¾ç‰‡è§†è§‰ç‰¹å¾ç›¸ä¼¼çš„å†å²å›¾ç‰‡
    - äººè„¸ç›¸ä¼¼æœç´¢ï¼šå¯¹æ£€æµ‹åˆ°çš„æ¯ä¸ªäººè„¸ï¼ŒæŸ¥æ‰¾æ•°æ®åº“ä¸­ç›¸ä¼¼çš„äººè„¸
    - å¯è°ƒæ•´ç›¸ä¼¼åº¦é˜ˆå€¼å’Œè¿”å›ç»“æœæ•°é‡
    
    ### æ”¯æŒæ£€æµ‹çš„ç‰©ä½“ï¼š
    - äººç‰©
    - æ¯å­/é…’æ¯
    - ç“¶å­
    - äººè„¸ï¼ˆä½¿ç”¨OpenCVçš„äººè„¸æ£€æµ‹å™¨ï¼‰
    """)

# æ·»åŠ é¡µè„š
st.markdown("---")
st.markdown("ğŸ“¸ é«˜ç²¾åº¦ç‰©ä½“æ£€æµ‹å·¥å…· | åŸºäºYOLOv8å’ŒStreamlitæ„å»º")

